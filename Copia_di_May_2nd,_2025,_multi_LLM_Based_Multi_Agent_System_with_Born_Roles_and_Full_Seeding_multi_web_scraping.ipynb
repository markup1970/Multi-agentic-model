{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 11358599,
          "sourceType": "datasetVersion",
          "datasetId": 7108536
        }
      ],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f8b90d3bd3846449051bb8e537e8748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24c7fc3b37954e32a239a911bb136995",
              "IPY_MODEL_2d4e02d324c44db49349611a2fc93b5a",
              "IPY_MODEL_0b4b1de1dad84a588b3ac685b3234810"
            ],
            "layout": "IPY_MODEL_1d494419805b4bd08391bfe043081995"
          }
        },
        "24c7fc3b37954e32a239a911bb136995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c69a7eac9c43b4a64329da5161f27b",
            "placeholder": "​",
            "style": "IPY_MODEL_df7bc265f6f94b57bec7967768b5f6a8",
            "value": "modules.json: 100%"
          }
        },
        "2d4e02d324c44db49349611a2fc93b5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c97c4d3d213e4aeba33d50b53c5d5249",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_354648369d9a4e2f9fdf43fcfcdd3963",
            "value": 349
          }
        },
        "0b4b1de1dad84a588b3ac685b3234810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96b072db4587482daf88e670c790ffb2",
            "placeholder": "​",
            "style": "IPY_MODEL_d7c6b5ab180e4594bfa18938c9787c76",
            "value": " 349/349 [00:00&lt;00:00, 21.4kB/s]"
          }
        },
        "1d494419805b4bd08391bfe043081995": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1c69a7eac9c43b4a64329da5161f27b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df7bc265f6f94b57bec7967768b5f6a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c97c4d3d213e4aeba33d50b53c5d5249": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "354648369d9a4e2f9fdf43fcfcdd3963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96b072db4587482daf88e670c790ffb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7c6b5ab180e4594bfa18938c9787c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cef0c9f4959f438fb35ff263099c9307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f454c193e2d44a7a08ada987b9fb910",
              "IPY_MODEL_cf27768fdd3a48e09cfcf9ddbb57cdf3",
              "IPY_MODEL_9ed696e8838a4487bd95125b38ba62a6"
            ],
            "layout": "IPY_MODEL_cd394ccf8b1c487690714f2f5edecef3"
          }
        },
        "8f454c193e2d44a7a08ada987b9fb910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31a65de77a4d444b8eabd6f81565abfa",
            "placeholder": "​",
            "style": "IPY_MODEL_f4c8a16c3f3d40a4a2d17260a28cd9d9",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "cf27768fdd3a48e09cfcf9ddbb57cdf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d833c45520b04f0cb6d2c32b4e0f98c3",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26f1427c77be418aa8c485dfc55255d1",
            "value": 116
          }
        },
        "9ed696e8838a4487bd95125b38ba62a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05c040be3e954a548e1bdef0de92ebda",
            "placeholder": "​",
            "style": "IPY_MODEL_5e240f8536c0423f8b4e6822bd4438ef",
            "value": " 116/116 [00:00&lt;00:00, 8.04kB/s]"
          }
        },
        "cd394ccf8b1c487690714f2f5edecef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31a65de77a4d444b8eabd6f81565abfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4c8a16c3f3d40a4a2d17260a28cd9d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d833c45520b04f0cb6d2c32b4e0f98c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26f1427c77be418aa8c485dfc55255d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05c040be3e954a548e1bdef0de92ebda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e240f8536c0423f8b4e6822bd4438ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "958c6e1a116c4b6b9d5a97690f0c973f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fac0db0dc4e94596a597e2bea2da1ef1",
              "IPY_MODEL_c8a904629b8b4062a65a961aef949529",
              "IPY_MODEL_8659c7034eed4a79b53a87015f07c692"
            ],
            "layout": "IPY_MODEL_72e6342993454ea6b9ed284c6d889b84"
          }
        },
        "fac0db0dc4e94596a597e2bea2da1ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ce4943461284929a558c28488e9ccdc",
            "placeholder": "​",
            "style": "IPY_MODEL_f1a1197bc33d44fd9de3f9ff296cf0ab",
            "value": "README.md: 100%"
          }
        },
        "c8a904629b8b4062a65a961aef949529": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aeee6c978924886814b2180b7066062",
            "max": 10454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_686a5540c83342848b6cba96cd11acbd",
            "value": 10454
          }
        },
        "8659c7034eed4a79b53a87015f07c692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41bfbb0b37c34b2c852aa9be3a3f3898",
            "placeholder": "​",
            "style": "IPY_MODEL_a097a1fe9a184884b05e6dae42ceefd4",
            "value": " 10.5k/10.5k [00:00&lt;00:00, 564kB/s]"
          }
        },
        "72e6342993454ea6b9ed284c6d889b84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ce4943461284929a558c28488e9ccdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1a1197bc33d44fd9de3f9ff296cf0ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4aeee6c978924886814b2180b7066062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "686a5540c83342848b6cba96cd11acbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41bfbb0b37c34b2c852aa9be3a3f3898": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a097a1fe9a184884b05e6dae42ceefd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad278f5ef0e0484a9a412ee707ad60b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6b8bdb1850b48dcb51e271793b21dcc",
              "IPY_MODEL_5a1e59865b1a4ca7934413e64fe96848",
              "IPY_MODEL_134898d28392478fba958fffbefc3de6"
            ],
            "layout": "IPY_MODEL_3320b04c60c547a58f24283148498b45"
          }
        },
        "b6b8bdb1850b48dcb51e271793b21dcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8af635192d914bbd95d592bbd13a1c0d",
            "placeholder": "​",
            "style": "IPY_MODEL_827268ff92694c9f9c45b5e1a1f983c5",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "5a1e59865b1a4ca7934413e64fe96848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d51ec36464684ca49814db67e7659968",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f91866d05c644364b0d271341f32376d",
            "value": 53
          }
        },
        "134898d28392478fba958fffbefc3de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e18cf371416494cb38d4414703eab15",
            "placeholder": "​",
            "style": "IPY_MODEL_a2d48f41ac5a4cac825d0e065d32ba34",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.11kB/s]"
          }
        },
        "3320b04c60c547a58f24283148498b45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8af635192d914bbd95d592bbd13a1c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "827268ff92694c9f9c45b5e1a1f983c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d51ec36464684ca49814db67e7659968": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f91866d05c644364b0d271341f32376d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e18cf371416494cb38d4414703eab15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2d48f41ac5a4cac825d0e065d32ba34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6c4acb26cfc4d09835e1b1fc67ca144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b106c14dfbc143ac8dbff83a59ecc9c9",
              "IPY_MODEL_13e19d7124d1419c95f55013d205c2b5",
              "IPY_MODEL_08604234311a470c9b8e99e4fd610dd7"
            ],
            "layout": "IPY_MODEL_cac88a60663c465fb69574a7c8660d7a"
          }
        },
        "b106c14dfbc143ac8dbff83a59ecc9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46b82bce04a84438acb7372425b79aa3",
            "placeholder": "​",
            "style": "IPY_MODEL_6d8088f660a44364ae9ad8c9388d8ef8",
            "value": "config.json: 100%"
          }
        },
        "13e19d7124d1419c95f55013d205c2b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5443f7787e7646be9f3f3fb8e4132f41",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f316e2614e874db2b8fe6cd110b4479f",
            "value": 612
          }
        },
        "08604234311a470c9b8e99e4fd610dd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_530ce10b0ad9478fad2a72c8129160f6",
            "placeholder": "​",
            "style": "IPY_MODEL_602cea43dea94458865798dbf1df4fa6",
            "value": " 612/612 [00:00&lt;00:00, 52.9kB/s]"
          }
        },
        "cac88a60663c465fb69574a7c8660d7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46b82bce04a84438acb7372425b79aa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d8088f660a44364ae9ad8c9388d8ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5443f7787e7646be9f3f3fb8e4132f41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f316e2614e874db2b8fe6cd110b4479f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "530ce10b0ad9478fad2a72c8129160f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "602cea43dea94458865798dbf1df4fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d274525fa69c41538403dc5d51cfca60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f0931853ce044bea1082851a1aac9c7",
              "IPY_MODEL_774cc2dd661c4b14a13f5ee79d20faac",
              "IPY_MODEL_b4304a0c32fd475d9265f967c57c08d7"
            ],
            "layout": "IPY_MODEL_3bcacb3ca61d4d15845f1754f2856512"
          }
        },
        "2f0931853ce044bea1082851a1aac9c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7db9469f7b49481eb261e70399335baa",
            "placeholder": "​",
            "style": "IPY_MODEL_42980a2c508949dfb13e5dc7597e601f",
            "value": "model.safetensors: 100%"
          }
        },
        "774cc2dd661c4b14a13f5ee79d20faac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74ed4ac2136e4026b77280083c4b5e44",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d71f0db118f4f35aa1f7b3b292607c0",
            "value": 90868376
          }
        },
        "b4304a0c32fd475d9265f967c57c08d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7395876b9aec41738e23dacc07deb15f",
            "placeholder": "​",
            "style": "IPY_MODEL_18c2a9abc2c648dda091a0369f93f44f",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 116MB/s]"
          }
        },
        "3bcacb3ca61d4d15845f1754f2856512": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7db9469f7b49481eb261e70399335baa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42980a2c508949dfb13e5dc7597e601f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74ed4ac2136e4026b77280083c4b5e44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d71f0db118f4f35aa1f7b3b292607c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7395876b9aec41738e23dacc07deb15f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18c2a9abc2c648dda091a0369f93f44f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4ac8a4897bd4c03ab1f0b67c64e9449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f6803a11d58431a9c4fea1b69d1279f",
              "IPY_MODEL_ae147b9923754afd8c6febd5c894f0f6",
              "IPY_MODEL_20ff85a5f6c34f53879e88e4a453df6b"
            ],
            "layout": "IPY_MODEL_c7ee695c55e5473e8f805827578aa922"
          }
        },
        "7f6803a11d58431a9c4fea1b69d1279f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5cff4ceda674f90b23bc2d08df9830b",
            "placeholder": "​",
            "style": "IPY_MODEL_6093e09f50ba4eb795d2599e3e0aab3e",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "ae147b9923754afd8c6febd5c894f0f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_425272acbf8e416aba3a7163de6233df",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_607ae71576354cd19fabd8a5b6f94bcf",
            "value": 350
          }
        },
        "20ff85a5f6c34f53879e88e4a453df6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f50c497f969a4ee899b20b252087ac04",
            "placeholder": "​",
            "style": "IPY_MODEL_99f6f06a8c4447168b697592ac15abb6",
            "value": " 350/350 [00:00&lt;00:00, 25.2kB/s]"
          }
        },
        "c7ee695c55e5473e8f805827578aa922": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5cff4ceda674f90b23bc2d08df9830b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6093e09f50ba4eb795d2599e3e0aab3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "425272acbf8e416aba3a7163de6233df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "607ae71576354cd19fabd8a5b6f94bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f50c497f969a4ee899b20b252087ac04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99f6f06a8c4447168b697592ac15abb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83231728eaf74b84b59911b6711a16e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb7d27e79cb64634bc0eebecca7b0b9a",
              "IPY_MODEL_35d5ea7cae09474787afcac892be042b",
              "IPY_MODEL_89260693d14340c89428335acba05003"
            ],
            "layout": "IPY_MODEL_15fded1adb644c55a5a221a78ea7cc72"
          }
        },
        "eb7d27e79cb64634bc0eebecca7b0b9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5567fbd014f459588d4a9fa79c4e6c2",
            "placeholder": "​",
            "style": "IPY_MODEL_66fa0c67ebbe46518a75c438bb5c3721",
            "value": "vocab.txt: 100%"
          }
        },
        "35d5ea7cae09474787afcac892be042b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f487ed9487c41538b386fdf8999287d",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cfbd3a72eba14e22993752358011f73e",
            "value": 231508
          }
        },
        "89260693d14340c89428335acba05003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f75a3fc899f4eaa94a4ff415f0dc945",
            "placeholder": "​",
            "style": "IPY_MODEL_ff911d365a9a4bc0bb1bafff249e7396",
            "value": " 232k/232k [00:00&lt;00:00, 4.45MB/s]"
          }
        },
        "15fded1adb644c55a5a221a78ea7cc72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5567fbd014f459588d4a9fa79c4e6c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66fa0c67ebbe46518a75c438bb5c3721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f487ed9487c41538b386fdf8999287d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfbd3a72eba14e22993752358011f73e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f75a3fc899f4eaa94a4ff415f0dc945": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff911d365a9a4bc0bb1bafff249e7396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5277e2eeb38a46ab80e974d4bbe7a9a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6353fd638ce947d7be06d4fcfc7343b7",
              "IPY_MODEL_3bea1e6a288846a3b8d0be2bb0ea2dde",
              "IPY_MODEL_5131c6c504344992a4d17dd8221b8123"
            ],
            "layout": "IPY_MODEL_01bb1a1774ac4e9abcd1ec5ae7d02e8f"
          }
        },
        "6353fd638ce947d7be06d4fcfc7343b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78177cc809e745d2874532c38d05d7e6",
            "placeholder": "​",
            "style": "IPY_MODEL_f0826e8374e74f4a880ee5d01e896192",
            "value": "tokenizer.json: 100%"
          }
        },
        "3bea1e6a288846a3b8d0be2bb0ea2dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac595dd1dd1c4bea86ae9cab4d214d32",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77c257c5bded4abeb2d03375d9c83aa7",
            "value": 466247
          }
        },
        "5131c6c504344992a4d17dd8221b8123": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dad096a73fc484998f19d0a2d2127b7",
            "placeholder": "​",
            "style": "IPY_MODEL_dd12a71611864633b1110b6fd76cb77f",
            "value": " 466k/466k [00:00&lt;00:00, 7.16MB/s]"
          }
        },
        "01bb1a1774ac4e9abcd1ec5ae7d02e8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78177cc809e745d2874532c38d05d7e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0826e8374e74f4a880ee5d01e896192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac595dd1dd1c4bea86ae9cab4d214d32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77c257c5bded4abeb2d03375d9c83aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8dad096a73fc484998f19d0a2d2127b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd12a71611864633b1110b6fd76cb77f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cdde0f5d98245eda373093607bd9eb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45ca6e056c2947e09f403671a2d38541",
              "IPY_MODEL_07db4a897f2c4608a82932f71fd4da2a",
              "IPY_MODEL_3b2015fd6e9c486aabf827bda5663dff"
            ],
            "layout": "IPY_MODEL_192a3667d75442e7ba41398091ab3b8a"
          }
        },
        "45ca6e056c2947e09f403671a2d38541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b1a4393624d49cfb721f681a3fe1958",
            "placeholder": "​",
            "style": "IPY_MODEL_eba14666a10f4db5ac3f60a1c798458f",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "07db4a897f2c4608a82932f71fd4da2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dd0e5d8c8c349d7b975eea02638d647",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4b56f246b6a4d2fba824eff93f2b742",
            "value": 112
          }
        },
        "3b2015fd6e9c486aabf827bda5663dff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9b03e1274744630977ba0c758e7f637",
            "placeholder": "​",
            "style": "IPY_MODEL_f18db924b4414fca876767b9d7e27d5f",
            "value": " 112/112 [00:00&lt;00:00, 9.50kB/s]"
          }
        },
        "192a3667d75442e7ba41398091ab3b8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b1a4393624d49cfb721f681a3fe1958": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eba14666a10f4db5ac3f60a1c798458f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dd0e5d8c8c349d7b975eea02638d647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b56f246b6a4d2fba824eff93f2b742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9b03e1274744630977ba0c758e7f637": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f18db924b4414fca876767b9d7e27d5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "659be4d6d53242efb04245a0ac797a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb7c4805fe27464eb4832925c51b5eaa",
              "IPY_MODEL_0265191da409481f8f5b41ecbf0ca87b",
              "IPY_MODEL_dfb5b88db13b4bf6a469dfddafbebce0"
            ],
            "layout": "IPY_MODEL_4bd478008471450694e14109747ee904"
          }
        },
        "eb7c4805fe27464eb4832925c51b5eaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4261dc24640c4dc891415d091402bb76",
            "placeholder": "​",
            "style": "IPY_MODEL_ea1bf8d55c984635858fc218486f18fe",
            "value": "config.json: 100%"
          }
        },
        "0265191da409481f8f5b41ecbf0ca87b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9aa1b586584741d3958f243f6f7505be",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a84e44e4e4d48bf9280f1f4f7aa9e98",
            "value": 190
          }
        },
        "dfb5b88db13b4bf6a469dfddafbebce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cadc978da3846479c005541328c58bb",
            "placeholder": "​",
            "style": "IPY_MODEL_608f80422fc3472b8ef6c7d0c6198005",
            "value": " 190/190 [00:00&lt;00:00, 13.0kB/s]"
          }
        },
        "4bd478008471450694e14109747ee904": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4261dc24640c4dc891415d091402bb76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea1bf8d55c984635858fc218486f18fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9aa1b586584741d3958f243f6f7505be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a84e44e4e4d48bf9280f1f4f7aa9e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9cadc978da3846479c005541328c58bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "608f80422fc3472b8ef6c7d0c6198005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markup1970/Multi-agentic-model/blob/main/Copia_di_May_2nd%2C_2025%2C_multi_LLM_Based_Multi_Agent_System_with_Born_Roles_and_Full_Seeding_multi_web_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ==================== 📦 CELL 1 — SETUP AND INITIALIZATION ===\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "import google.generativeai as genai\n",
        "%matplotlib inline\n",
        "import importlib\n",
        "import subprocess\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "# --- Utility for timestamped logging ---\n",
        "def log(message):\n",
        "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")\n",
        "\n",
        "\n",
        "\n",
        "## --- Install core dependencies only if missing ---\n",
        "def ensure_pip(package, module_name=None):\n",
        "    \"\"\"\n",
        "    Try to import `module_name` (or `package` if none); if ImportError, pip-install.\n",
        "    \"\"\"\n",
        "    name = module_name or package\n",
        "    try:\n",
        "        importlib.import_module(name)\n",
        "        log(f\"📦 {package} already installed.\")\n",
        "    except ImportError:\n",
        "        log(f\"📦 Installing {package}…\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", package])\n",
        "\n",
        "# ── End helper definition ─────────────────────────────────────\n",
        "# Conditionally install only what’s missing:\n",
        "ensure_pip(\"google-generativeai\", \"google.generativeai\")\n",
        "ensure_pip(\"openai\")\n",
        "ensure_pip(\"faiss-cpu\", \"faiss\")\n",
        "ensure_pip(\"sentence-transformers\", \"sentence_transformers\")\n",
        "ensure_pip(\"python-dotenv\", \"dotenv\")\n",
        "\n",
        "\n",
        "# ── end conditional installs ─────────────────────────\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# --- Load .env (for local environments) ---\n",
        "load_dotenv()\n",
        "\n",
        "# --- Universal Chrome + Selenium + Chromedriver Setup ---\n",
        "def is_colab():\n",
        "    return \"google.colab\" in sys.modules\n",
        "\n",
        "\n",
        "def has_chrome():\n",
        "    \"\"\"\n",
        "    Detect a Chrome/Chromium executable.\n",
        "    Covers common binary names on Linux, macOS, and Windows.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        shutil.which(\"google-chrome\")          # Debian/Ubuntu\n",
        "        or shutil.which(\"google-chrome-stable\")# Alt Debian name\n",
        "        or shutil.which(\"chrome\")              # macOS Homebrew cask\n",
        "        or shutil.which(\"chromium-browser\")    # Chromium on Linux\n",
        "        or shutil.which(\"chromium\")            # Generic Chromium\n",
        "    )\n",
        "\n",
        "\n",
        "def has_chromedriver():\n",
        "    return shutil.which(\"chromedriver\")\n",
        "\n",
        "def install_chrome_and_chromedriver():\n",
        "    if is_colab():\n",
        "        # --- Colab branch (unchanged from patch 1) ---\n",
        "        print(\"📦 Installing Chrome and Chromedriver for Colab…\")\n",
        "        subprocess.check_call([\"apt-get\", \"update\", \"-y\"])\n",
        "        subprocess.check_call([\"apt-get\", \"install\", \"-y\",\n",
        "                               \"chromium-browser\", \"chromium-chromedriver\"])\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"selenium\"])\n",
        "        os.environ[\"PATH\"] += \":/usr/bin\"\n",
        "    else:\n",
        "        # --- LOCAL branch (laptop / server) ---\n",
        "        print(\"📦 Installing Selenium and Chromedriver locally…\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                               \"selenium\", \"chromedriver-autoinstaller\"])\n",
        "\n",
        "        # ▸ NEW safety check ▸ ensure a Chrome/Chromium binary exists\n",
        "        if not has_chrome():\n",
        "            try:\n",
        "                # Quick install for Debian/Ubuntu users\n",
        "                subprocess.check_call([\"apt-get\", \"update\", \"-y\"])\n",
        "                subprocess.check_call([\"apt-get\", \"install\", \"-y\",\n",
        "                                       \"google-chrome-stable\"])\n",
        "                log(\"✅ Google‑Chrome installed automatically.\")\n",
        "            except Exception:\n",
        "                log(\"⚠️  Chrome not installed automatically — \"\n",
        "                    \"please install it manually for Selenium.\")\n",
        "\n",
        "        # Finally install/update the matching chromedriver\n",
        "        import chromedriver_autoinstaller\n",
        "        chromedriver_autoinstaller.install()\n",
        "\n",
        "\n",
        "def ensure_chrome_and_chromedriver():\n",
        "    if has_chrome() and has_chromedriver():\n",
        "        print(\"✅ Chrome and Chromedriver are already installed.\")\n",
        "        return\n",
        "    install_chrome_and_chromedriver()\n",
        "\n",
        "# --- Ensure Chrome + Chromedriver are available ---\n",
        "ensure_chrome_and_chromedriver()\n",
        "\n",
        "# --- Unified API Key Loader (safe fallback included) ---\n",
        "def get_api_key(key_name):\n",
        "    api_key = None\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        try:\n",
        "            api_key = userdata.get(key_name)\n",
        "            if api_key:\n",
        "                log(f\"{key_name} loaded from Colab userdata.\")\n",
        "            else:\n",
        "                log(f\"{key_name} not found in Colab secrets.\")\n",
        "        except Exception as e:\n",
        "            log(f\"{key_name} not found in Colab secrets (exception handled).\")\n",
        "    except ImportError:\n",
        "        pass  # Not in Colab\n",
        "\n",
        "    if not api_key:\n",
        "        api_key = os.getenv(key_name)\n",
        "        if api_key:\n",
        "            log(f\"{key_name} loaded from environment variables or .env file.\")\n",
        "\n",
        "    return api_key\n",
        "\n",
        "# --- Load API keys ---\n",
        "google_api_key = get_api_key(\"GOOGLE_API_KEY\")\n",
        "openai_api_key = get_api_key(\"OPENAI_API_KEY\")\n",
        "claude_api_key = get_api_key(\"CLAUDE_API_KEY\")\n",
        "llama_api_key = get_api_key(\"LLAMA_API_KEY\")\n",
        "\n",
        "# --- Configurable model names and endpoints ---\n",
        "GEMINI_MODEL_NAME = os.getenv(\"GEMINI_MODEL_NAME\", \"models/gemini-2.5-pro-preview-03-25\")\n",
        "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\", \"gpt-4o\")\n",
        "CLAUDE_MODEL_NAME = os.getenv(\"CLAUDE_MODEL_NAME\", \"anthropic/claude-3-sonnet-20240229\")\n",
        "LLAMA_MODEL_NAME = os.getenv(\"LLAMA_MODEL_NAME\", \"meta-llama-3-8b-instruct\")\n",
        "\n",
        "from openai import OpenAI   # v1.x SDK\n",
        "\n",
        "def get_available_models():\n",
        "    \"\"\"Return dict {nick: client_or_model} without key overwrites.\"\"\"\n",
        "    available = {}\n",
        "\n",
        "    # ── Google Gemini ──────────────────────────────────────────\n",
        "    if google_api_key:\n",
        "        try:\n",
        "            genai.configure(api_key=google_api_key)\n",
        "            available[\"gemini\"] = genai.GenerativeModel(GEMINI_MODEL_NAME)\n",
        "        except Exception as e:\n",
        "            log(f\"❌ Google Gemini init failed: {e}\")\n",
        "\n",
        "    # ── OpenAI GPT‑4o direct ──────────────────────────────────\n",
        "    if openai_api_key:\n",
        "        try:\n",
        "            available[\"openai\"] = OpenAI(api_key=openai_api_key)  # default base\n",
        "        except Exception as e:\n",
        "            log(f\"❌ OpenAI init failed: {e}\")\n",
        "\n",
        "    # ── Claude via OpenRouter (example) ───────────────────────\n",
        "    if claude_api_key:\n",
        "        try:\n",
        "            available[\"claude\"] = OpenAI(\n",
        "                api_key=claude_api_key,\n",
        "                base_url=\"https://openrouter.ai/api/v1\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            log(f\"❌ Claude init failed: {e}\")\n",
        "\n",
        "    # ── Llama via OpenRouter (example) ────────────────────────\n",
        "    if llama_api_key:\n",
        "        try:\n",
        "            available[\"llama\"] = OpenAI(\n",
        "                api_key=llama_api_key,\n",
        "                base_url=\"https://openrouter.ai/api/v1\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            log(f\"❌ Llama init failed: {e}\")\n",
        "\n",
        "    return available\n",
        "\n",
        "\n",
        "# Initialize available models\n",
        "\n",
        "\n",
        "available_models = get_available_models()\n",
        "log(f\"Available models: {', '.join(available_models.keys())}\")\n",
        "\n",
        "\n",
        "\n",
        "def llm_generate(model_key: str, prompt: str, temperature: float = 1.0) -> str:\n",
        "    client = available_models.get(model_key)\n",
        "    if client is None:\n",
        "        raise ValueError(f\"No such model registered: {model_key!r}\")\n",
        "\n",
        "    # ── Google Gemini branch ────────────────────────────────────────────────\n",
        "\n",
        "    if model_key == \"gemini\":\n",
        "        # Preferred: the working generate_content method in your Colab\n",
        "        try:\n",
        "            # Only pass the prompt here\n",
        "            resp = client.generate_content(prompt)\n",
        "            return resp.text\n",
        "        except TypeError:\n",
        "            # Fallback to call_with_response or call (which do accept temperature)\n",
        "            if hasattr(client, \"call_with_response\"):\n",
        "                resp = client.call_with_response(\n",
        "                    prompt=prompt,\n",
        "                    temperature=temperature,\n",
        "                    max_output_tokens=512,\n",
        "                )\n",
        "                return resp.candidates[0].output\n",
        "            if hasattr(client, \"call\"):\n",
        "                return client.call(\n",
        "                    prompt=prompt,\n",
        "                    temperature=temperature,\n",
        "                    max_output_tokens=512,\n",
        "                )\n",
        "            raise RuntimeError(\"Gemini client doesn’t support any known generate methods.\")\n",
        "\n",
        "    # ── OpenAI GPT‑4o direct branch ───────────────────────────────────────\n",
        "    if model_key == \"openai\":\n",
        "        resp = client.chat.completions.create(\n",
        "            model=OPENAI_MODEL_NAME,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        return resp.choices[0].message.content\n",
        "\n",
        "    # ── Claude via OpenRouter branch ─────────────────────────────────────\n",
        "    if model_key == \"claude\":\n",
        "        resp = client.chat.completions.create(\n",
        "            model=CLAUDE_MODEL_NAME,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        return resp.choices[0].message.content\n",
        "\n",
        "    # ── Llama via OpenRouter branch ──────────────────────────────────────\n",
        "    if model_key == \"llama\":\n",
        "        resp = client.chat.completions.create(\n",
        "            model=LLAMA_MODEL_NAME,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        return resp.choices[0].message.content\n",
        "\n",
        "    # ── Fallback ─────────────────────────────────────────────────────────\n",
        "    raise RuntimeError(f\"Don't know how to call model: {model_key!r}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Embedding model (for semantic similarity and task-role mapping) ---\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "EMB_NAME = os.getenv(\"EMBEDDING_MODEL_NAME\", \"all-MiniLM-L6-v2\")\n",
        "\n",
        "if \"embedding_model\" not in globals():\n",
        "    # HF downloads go to ~/.cache/ by default – keep across VM reuse\n",
        "    embedding_model = SentenceTransformer(EMB_NAME, cache_folder=os.path.expanduser(\"~/.cache\"))\n",
        "    log(f\"✅ Embedding model '{EMB_NAME}' loaded.\")\n",
        "else:\n",
        "    log(\"✅ Embedding model already in memory.\")\n",
        "\n",
        "# Embedding dimension for fallback zero-vectors\n",
        "EMB_DIM = embedding_model.get_sentence_embedding_dimension()\n",
        "log(f\"ℹ️  Embedding dimension: {EMB_DIM}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f7lldOCQF6td",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770,
          "referenced_widgets": [
            "9f8b90d3bd3846449051bb8e537e8748",
            "24c7fc3b37954e32a239a911bb136995",
            "2d4e02d324c44db49349611a2fc93b5a",
            "0b4b1de1dad84a588b3ac685b3234810",
            "1d494419805b4bd08391bfe043081995",
            "a1c69a7eac9c43b4a64329da5161f27b",
            "df7bc265f6f94b57bec7967768b5f6a8",
            "c97c4d3d213e4aeba33d50b53c5d5249",
            "354648369d9a4e2f9fdf43fcfcdd3963",
            "96b072db4587482daf88e670c790ffb2",
            "d7c6b5ab180e4594bfa18938c9787c76",
            "cef0c9f4959f438fb35ff263099c9307",
            "8f454c193e2d44a7a08ada987b9fb910",
            "cf27768fdd3a48e09cfcf9ddbb57cdf3",
            "9ed696e8838a4487bd95125b38ba62a6",
            "cd394ccf8b1c487690714f2f5edecef3",
            "31a65de77a4d444b8eabd6f81565abfa",
            "f4c8a16c3f3d40a4a2d17260a28cd9d9",
            "d833c45520b04f0cb6d2c32b4e0f98c3",
            "26f1427c77be418aa8c485dfc55255d1",
            "05c040be3e954a548e1bdef0de92ebda",
            "5e240f8536c0423f8b4e6822bd4438ef",
            "958c6e1a116c4b6b9d5a97690f0c973f",
            "fac0db0dc4e94596a597e2bea2da1ef1",
            "c8a904629b8b4062a65a961aef949529",
            "8659c7034eed4a79b53a87015f07c692",
            "72e6342993454ea6b9ed284c6d889b84",
            "7ce4943461284929a558c28488e9ccdc",
            "f1a1197bc33d44fd9de3f9ff296cf0ab",
            "4aeee6c978924886814b2180b7066062",
            "686a5540c83342848b6cba96cd11acbd",
            "41bfbb0b37c34b2c852aa9be3a3f3898",
            "a097a1fe9a184884b05e6dae42ceefd4",
            "ad278f5ef0e0484a9a412ee707ad60b1",
            "b6b8bdb1850b48dcb51e271793b21dcc",
            "5a1e59865b1a4ca7934413e64fe96848",
            "134898d28392478fba958fffbefc3de6",
            "3320b04c60c547a58f24283148498b45",
            "8af635192d914bbd95d592bbd13a1c0d",
            "827268ff92694c9f9c45b5e1a1f983c5",
            "d51ec36464684ca49814db67e7659968",
            "f91866d05c644364b0d271341f32376d",
            "2e18cf371416494cb38d4414703eab15",
            "a2d48f41ac5a4cac825d0e065d32ba34",
            "b6c4acb26cfc4d09835e1b1fc67ca144",
            "b106c14dfbc143ac8dbff83a59ecc9c9",
            "13e19d7124d1419c95f55013d205c2b5",
            "08604234311a470c9b8e99e4fd610dd7",
            "cac88a60663c465fb69574a7c8660d7a",
            "46b82bce04a84438acb7372425b79aa3",
            "6d8088f660a44364ae9ad8c9388d8ef8",
            "5443f7787e7646be9f3f3fb8e4132f41",
            "f316e2614e874db2b8fe6cd110b4479f",
            "530ce10b0ad9478fad2a72c8129160f6",
            "602cea43dea94458865798dbf1df4fa6",
            "d274525fa69c41538403dc5d51cfca60",
            "2f0931853ce044bea1082851a1aac9c7",
            "774cc2dd661c4b14a13f5ee79d20faac",
            "b4304a0c32fd475d9265f967c57c08d7",
            "3bcacb3ca61d4d15845f1754f2856512",
            "7db9469f7b49481eb261e70399335baa",
            "42980a2c508949dfb13e5dc7597e601f",
            "74ed4ac2136e4026b77280083c4b5e44",
            "8d71f0db118f4f35aa1f7b3b292607c0",
            "7395876b9aec41738e23dacc07deb15f",
            "18c2a9abc2c648dda091a0369f93f44f",
            "b4ac8a4897bd4c03ab1f0b67c64e9449",
            "7f6803a11d58431a9c4fea1b69d1279f",
            "ae147b9923754afd8c6febd5c894f0f6",
            "20ff85a5f6c34f53879e88e4a453df6b",
            "c7ee695c55e5473e8f805827578aa922",
            "c5cff4ceda674f90b23bc2d08df9830b",
            "6093e09f50ba4eb795d2599e3e0aab3e",
            "425272acbf8e416aba3a7163de6233df",
            "607ae71576354cd19fabd8a5b6f94bcf",
            "f50c497f969a4ee899b20b252087ac04",
            "99f6f06a8c4447168b697592ac15abb6",
            "83231728eaf74b84b59911b6711a16e4",
            "eb7d27e79cb64634bc0eebecca7b0b9a",
            "35d5ea7cae09474787afcac892be042b",
            "89260693d14340c89428335acba05003",
            "15fded1adb644c55a5a221a78ea7cc72",
            "d5567fbd014f459588d4a9fa79c4e6c2",
            "66fa0c67ebbe46518a75c438bb5c3721",
            "4f487ed9487c41538b386fdf8999287d",
            "cfbd3a72eba14e22993752358011f73e",
            "7f75a3fc899f4eaa94a4ff415f0dc945",
            "ff911d365a9a4bc0bb1bafff249e7396",
            "5277e2eeb38a46ab80e974d4bbe7a9a6",
            "6353fd638ce947d7be06d4fcfc7343b7",
            "3bea1e6a288846a3b8d0be2bb0ea2dde",
            "5131c6c504344992a4d17dd8221b8123",
            "01bb1a1774ac4e9abcd1ec5ae7d02e8f",
            "78177cc809e745d2874532c38d05d7e6",
            "f0826e8374e74f4a880ee5d01e896192",
            "ac595dd1dd1c4bea86ae9cab4d214d32",
            "77c257c5bded4abeb2d03375d9c83aa7",
            "8dad096a73fc484998f19d0a2d2127b7",
            "dd12a71611864633b1110b6fd76cb77f",
            "5cdde0f5d98245eda373093607bd9eb1",
            "45ca6e056c2947e09f403671a2d38541",
            "07db4a897f2c4608a82932f71fd4da2a",
            "3b2015fd6e9c486aabf827bda5663dff",
            "192a3667d75442e7ba41398091ab3b8a",
            "8b1a4393624d49cfb721f681a3fe1958",
            "eba14666a10f4db5ac3f60a1c798458f",
            "8dd0e5d8c8c349d7b975eea02638d647",
            "d4b56f246b6a4d2fba824eff93f2b742",
            "d9b03e1274744630977ba0c758e7f637",
            "f18db924b4414fca876767b9d7e27d5f",
            "659be4d6d53242efb04245a0ac797a84",
            "eb7c4805fe27464eb4832925c51b5eaa",
            "0265191da409481f8f5b41ecbf0ca87b",
            "dfb5b88db13b4bf6a469dfddafbebce0",
            "4bd478008471450694e14109747ee904",
            "4261dc24640c4dc891415d091402bb76",
            "ea1bf8d55c984635858fc218486f18fe",
            "9aa1b586584741d3958f243f6f7505be",
            "2a84e44e4e4d48bf9280f1f4f7aa9e98",
            "9cadc978da3846479c005541328c58bb",
            "608f80422fc3472b8ef6c7d0c6198005"
          ]
        },
        "outputId": "47c3d08e-f0de-41f2-8b88-8ab1ecfab36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-19 00:33:55] 📦 google-generativeai already installed.\n",
            "[2025-05-19 00:33:59] 📦 openai already installed.\n",
            "[2025-05-19 00:33:59] 📦 Installing faiss-cpu…\n",
            "[2025-05-19 00:34:46] 📦 sentence-transformers already installed.\n",
            "[2025-05-19 00:34:46] 📦 Installing python-dotenv…\n",
            "📦 Installing Chrome and Chromedriver for Colab…\n",
            "[2025-05-19 00:35:22] GOOGLE_API_KEY loaded from Colab userdata.\n",
            "[2025-05-19 00:35:23] OPENAI_API_KEY not found in Colab secrets (exception handled).\n",
            "[2025-05-19 00:35:23] CLAUDE_API_KEY not found in Colab secrets (exception handled).\n",
            "[2025-05-19 00:35:23] LLAMA_API_KEY not found in Colab secrets (exception handled).\n",
            "[2025-05-19 00:35:23] Available models: gemini\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f8b90d3bd3846449051bb8e537e8748"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cef0c9f4959f438fb35ff263099c9307"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "958c6e1a116c4b6b9d5a97690f0c973f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad278f5ef0e0484a9a412ee707ad60b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6c4acb26cfc4d09835e1b1fc67ca144"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d274525fa69c41538403dc5d51cfca60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4ac8a4897bd4c03ab1f0b67c64e9449"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83231728eaf74b84b59911b6711a16e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5277e2eeb38a46ab80e974d4bbe7a9a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cdde0f5d98245eda373093607bd9eb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "659be4d6d53242efb04245a0ac797a84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-19 00:35:33] ✅ Embedding model 'all-MiniLM-L6-v2' loaded.\n",
            "[2025-05-19 00:35:33] ℹ️  Embedding dimension: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ==================== 🧰 CELL 2 — UTILITY FUNCTIONS ==========\n",
        "# ============================================================\n",
        "\n",
        "import collections\n",
        "import re\n",
        "\n",
        "\n",
        "# --- General utilities (display and text processing) ---\n",
        "from IPython.display import HTML\n",
        "import html\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Global constants for task‐scoring bonuses\n",
        "SCORE_BONUS_KEYWORDS = [\"ai\", \"model\", \"data\", \"algorithm\"]\n",
        "\n",
        "\n",
        "def is_repetitive(text: str, fraction: float = 0.7) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if a single character accounts for ≥ fraction of the string.\n",
        "    Catches things like “aaaaaaa” or “!!!!!!!!!!\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return False\n",
        "    cnts = collections.Counter(text)\n",
        "    most_common_freq = cnts.most_common(1)[0][1]\n",
        "    return most_common_freq / len(text) >= fraction\n",
        "\n",
        "def too_few_words(text: str, min_words: int = 3) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if the text has fewer than min_words whitespace-separated tokens.\n",
        "    Catches very short gibberish.\n",
        "    \"\"\"\n",
        "    return len(text.split()) < min_words\n",
        "\n",
        "def low_alnum_fraction(text: str, threshold: float = 0.5) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if fewer than threshold fraction of characters are alphanumeric.\n",
        "    Catches strings dominated by punctuation or symbols.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return False\n",
        "    alnums = len(re.findall(r\"\\w\", text))\n",
        "    return (alnums / len(text)) < threshold\n",
        "\n",
        "\n",
        "# --- Detect placeholder or blank output ---\n",
        "def is_placeholder_output(output):\n",
        "    \"\"\"\n",
        "    Check if output is blank or looks like a placeholder error.\n",
        "    \"\"\"\n",
        "    return (not output.strip()) or \"Gemini Error\" in output\n",
        "\n",
        "# --- Compute semantic similarity to detect meaningless outputs ---\n",
        "\n",
        "def is_meaningless_output(prompt, output, threshold=0.4):\n",
        "    \"\"\"\n",
        "    Decide if an LLM output is meaningless by:\n",
        "      1) Heuristic checks on the raw string\n",
        "      2) Embedding-based semantic similarity to the prompt\n",
        "    Returns (is_meaningless: bool, similarity_score: float)\n",
        "    \"\"\"\n",
        "\n",
        "    # ── 1) Heuristic bail-outs ───────────────────────────────────────────────\n",
        "    # Empty or placeholder\n",
        "    if not output.strip() or \"Gemini Error\" in output:\n",
        "        return True, 0.0\n",
        "\n",
        "    # Too repetitive?\n",
        "    if is_repetitive(output):\n",
        "        return True, 0.0\n",
        "\n",
        "    # Too few words (e.g. \"<5 words\")?\n",
        "    if too_few_words(output):\n",
        "        return True, 0.0\n",
        "\n",
        "    # Low fraction of alphanumeric chars?\n",
        "    if low_alnum_fraction(output):\n",
        "        return True, 0.0\n",
        "\n",
        "    # ── 2) Embedding‐based semantic check ────────────────────────────────────\n",
        "    try:\n",
        "        prompt_vec = embedding_model.encode(prompt)\n",
        "        output_vec = embedding_model.encode(output)\n",
        "\n",
        "        # Normalize to numpy arrays\n",
        "        if prompt_vec is None:\n",
        "            prompt_vec = np.zeros(EMB_DIM)\n",
        "        elif isinstance(prompt_vec, list):\n",
        "            prompt_vec = np.array(prompt_vec)\n",
        "\n",
        "        if output_vec is None:\n",
        "            output_vec = np.zeros(EMB_DIM)\n",
        "        elif isinstance(output_vec, list):\n",
        "            output_vec = np.array(output_vec)\n",
        "\n",
        "        sim = cosine_similarity([prompt_vec], [output_vec])[0][0]\n",
        "        # If below your threshold, flag as meaningless\n",
        "        return (sim < threshold), sim\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Embedding check failed (treating as meaningless): {e}\")\n",
        "        return True, 0.0\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 📊  Physiology curves with mid‑interval adjustments\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "# k‑1 sample → at the very start (after the bump); k‑0.5 sample → immediately after drop‑out & pruning: later, just before act() returns, you add the final k sample.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 📊  Physiology curves with length check + mid‑interval adjustments\n",
        "# ---------------------------------------------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_agent_physiology(runner, *, agents=None, figsize=(14, 7)):\n",
        "    \"\"\"\n",
        "    Draw attention / fatigue / hunger histories.\n",
        "    If any history list is shorter than the others for an agent,\n",
        "    it trims all three to the shortest length and emits a warning.\n",
        "    X-axis ticks at each full interval (0, 1, 2, …).\n",
        "    \"\"\"\n",
        "    agents = agents or runner.agents\n",
        "    fig, axs = plt.subplots(3, 1, sharex=True, figsize=figsize)\n",
        "    labels = [\"Attention\", \"Fatigue\", \"Hunger\"]\n",
        "\n",
        "\n",
        "    for ag in agents:\n",
        "        # 1) Detect mismatches and pick a common length\n",
        "        lens = [\n",
        "            len(ag.attention_history),\n",
        "            len(ag.fatigue_history),\n",
        "            len(ag.hunger_history),\n",
        "        ]\n",
        "        if len(set(lens)) > 1:\n",
        "            min_len = min(lens)\n",
        "            print(f\"⚠️  {ag.name}: mismatched history lengths {lens}; trimming to {min_len} samples.\")\n",
        "        else:\n",
        "            min_len = lens[0]\n",
        "\n",
        "        # 2) Trim histories\n",
        "        att = ag.attention_history[:min_len]\n",
        "        fat = ag.fatigue_history[:min_len]\n",
        "        hung = ag.hunger_history[:min_len]\n",
        "\n",
        "        # 3) Build xs based on half‐interval samples: 0, 0.5, 1.0, …\n",
        "        xs = [i * 0.5 for i in range(min_len)]\n",
        "\n",
        "\n",
        "        # 4) Plot trimmed series\n",
        "        axs[0].plot(xs, att,  marker=\"o\", label=ag.name)\n",
        "        axs[1].plot(xs, fat,  marker=\"o\", label=ag.name)\n",
        "        axs[2].plot(xs, hung, marker=\"o\", label=ag.name)\n",
        "\n",
        "\n",
        "\n",
        "    # Force x-axis ticks 0–10 and y-axis from 0–1 (physiology always ∈ [0,1])\n",
        "    for ax, lab in zip(axs, labels):\n",
        "        ax.set_title(lab)\n",
        "        ax.set_xticks(range(0, 11))    # show intervals 0 through 10\n",
        "        ax.set_xlim(0, 10)             # constrain x-axis to 0–10\n",
        "        ax.set_ylim(0, 1)              # physiology scores normalized to [0,1]\n",
        "        ax.set_xlabel(\"Interval #   (0 = start, … up to 10)\")\n",
        "        ax.grid(alpha=0.3)\n",
        "        if ax.get_legend_handles_labels()[0]:\n",
        "            ax.legend(loc=\"upper right\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "6iTAN2C17NQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ==================== 🧠 CELL 3 — ROLES, TASKS AND CAPABILITIES ===\n",
        "# ============================================================\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# --- Capabilities (always active internal functions) ---\n",
        "CAPABILITY_DESCRIPTIONS = {\n",
        "    \"memory\": \"Store and retrieve previous outputs or mission paths in short- and long-term memory for reuse, similarity, or validation purposes.\",\n",
        "    \"validation\": \"Assess outputs based on internal metrics (e.g., efficiency, meaningfulness) and enforce quality gates for meta-learning.\",\n",
        "    \"foresight\": \"Predict scenario types (e.g., Black Swan, Tipping Point) based on prompt/output similarity and divergence from stored paths.\",\n",
        "    \"logic\": \"Support consistent reasoning through entailment, deduction, and structural alignment.\",\n",
        "    \"computation\": \"Perform symbolic or numeric calculations as needed to support analytical outputs.\",\n",
        "    \"hardcoded_retrieval\": \"Use embedding-based similarity search within local memory or external/internal persistent db to support context-aware generation.\",\n",
        "    \"triggerable_retrieval\": \"Use non-similarity search within local memory or external/internal persistent db\",\n",
        "    \"embedding_similarity\": \"Use vector space similarity to compare prompts, tasks, or past outputs based on their semantic embeddings.\",\n",
        "    \"llm_call\": \"Delegate sub-decisions or refinement tasks to a language model in-context during reasoning or generation.\",\n",
        "    \"gated_inline_role_activation_mechanism\": \"Any role may  be activated on a second pass (suitable for a full digital-twin or simulation or a domain-specific need) only activated inline if their associated metric exceeds a threshold and they were not already selected via semantic similarity in the initial role selection.\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ROLE_DESCRIPTIONS = {\n",
        "    \"Strategist\":   \"Analyze long-term risks and plan adaptive strategies.\",\n",
        "    \"Scout\":        \"Retrieve evidence, search sources, and gather data.\",\n",
        "    \"Executor\":     \"Summarize content, finalize decisions, and synthesize findings.Ensure completion of the intended task by applying summarization, formatting, or domain-specific finalization inline.\",\n",
        "    \"Analyst\":      \"Verify facts, perform calculations, and assess logical validity.\",\n",
        "    \"Builder\":      \"Design structured models, write code, optimize solutions, and assemble systems.\",\n",
        "    \"Decomposer\":   \"Break down complex systems, reverse-engineer processes, and abstract key components.\",\n",
        "    \"Verifier\":     \"Substantiate claims, provide evidence, and ground arguments in facts.\",\n",
        "    \"Mathematician\":\"Compute results, formalize ideas mathematically, and solve equations.\",\n",
        "    \"Logician\":     \"Apply deductive and inductive reasoning to structure arguments and rationalize decisions.\",\n",
        "    \"Interpreter\":  \"Infer hidden meanings, clarify ambiguous information, and specify precise interpretations.\",\n",
        "    \"Contextualizer\": \"Embed facts into relevant local or global context for clarity and relevance.\",\n",
        "    \"Tester\":       \"Validate assumptions or outputs by testing behavior, reliability, or edge cases.\",\n",
        "    \"Tracker\":      \"Follow evolving patterns, updates, or trajectories across time or datasets.\",\n",
        "    \"Monitor\":      \"Audit systems or data streams for changes, drift, or anomalies.\",\n",
        "    \"Translator\":   \"Convert information between different formats, languages, or conceptual systems.\",\n",
        "    \"Forecaster\":   \"Model time-based developments, anticipate outcomes, and evaluate future scenarios.\",\n",
        "    \"Planner\":      \"Formulate multi-step procedures, organize goal hierarchies, and structure action plans.\",\n",
        "    \"Grapher\":      \"Extract and represent causal relationships or process flows using logical or visual structures.\",\n",
        "    \"Statisticien\": \"Perform statistical inference, analyze quantitative patterns, and support deductive or abductive reasoning.\",\n",
        "    \"Agent_Orchestrator\": \"Coordinate distributed agent behavior, manage task delegation, and align collaborative execution.\",\n",
        "    \"Twin_Digitalizer\": \"Translate outputs into structured, executable representations such as causal graphs, workflows, or digital schematics.\",\n",
        "    \"Simulator\": \"Run scenarios or test logic under controlled conditions to evaluate outcomes, consequences, or counterfactuals.\",\n",
        "    \"Refiner\": \"Improve clarity, coherence, or quality of the output before it is finalized.\",\n",
        "    \"Validator\": \"Assess the generated content for factual accuracy, logical consistency, or internal coherence during generation.\"\n",
        "}\n",
        "\n",
        "\n",
        "# --- Role → Task mapping ---\n",
        "# A fixed reference declared once (e.g., in a config or constants cell). Typically during initialization (each time this cell is run). Static, but meta-learned priority and pruning apply → tasks get refreshed and pruned across rounds.\n",
        "# It maps ideal/default tasks to each role.\n",
        "# Does not change during execution.\n",
        "# However, self.role_task_map — Dynamic clone is created from ROLE_TASK_MAP but mutated at runtime and reflects pruning, reactivation, and current task availability.It is used in actual role-task assignment logic.\n",
        "\n",
        "\n",
        "\n",
        "ROLE_TASK_MAP = {\n",
        "    \"Strategist\":     [\"analyze\", \"synthesize\", \"plan\"],\n",
        "    \"Scout\":          [\"retrieve\", \"search\", \"synthesize\"],\n",
        "    \"Executor\":       [\"summarize\", \"finalize\", \"synthesize\"],\n",
        "    \"Analyst\":        [\"analyze\", \"verify\", \"compute\"],\n",
        "    \"Builder\":        [\"model\", \"code\", \"optimize\", \"assemble\"],\n",
        "    \"Decomposer\":     [\"reverse-engineer\", \"abstractize\", \"decompose\"],\n",
        "    \"Verifier\":       [\"substantiate\", \"ground\", \"fact-check\"],\n",
        "    \"Mathematician\":  [\"compute\", \"mathematicize\", \"formalize\"],\n",
        "    \"Logician\":       [\"deduct\", \"infer\", \"logicalize\", \"validate\"],\n",
        "    \"Interpreter\":    [\"clarify\", \"infer\", \"disambiguate\"],\n",
        "    \"Contextualizer\": [\"contextualize\", \"generalize\", \"specify\"],\n",
        "    \"Tester\":         [\"test\", \"verify\", \"debug\", \"stress-test\"],\n",
        "    \"Tracker\":        [\"track\", \"monitor\", \"forecast\", \"follow\"],\n",
        "    \"Monitor\":        [\"monitor\", \"track\", \"verify\", \"audit\"],\n",
        "    \"Translator\":     [\"translate\", \"clarify\", \"restructure\"],\n",
        "    \"Forecaster\":     [\"forecast\", \"project\", \"model_trends\"],\n",
        "    \"Planner\":        [\"sequence\", \"structure\", \"organize\", \"prioritize\"],\n",
        "    \"Grapher\":        [\"extract_causality\", \"represent_relations\", \"build_graph\"],\n",
        "    \"Statisticien\":   [\"infer\", \"estimate\", \"analyze_stats\", \"reason_quantitatively\"],\n",
        "    \"Agent_Orchestrator\": [\"delegate\", \"coordinate\", \"synchronize\", \"align\"],\n",
        "    \"Twin_Digitalizer\": [\"extract_structure\", \"encode_logic\", \"digitalize_output\"],\n",
        "    \"Simulator\": [\"simulate\", \"test_outcomes\", \"explore_scenarios\"],\n",
        "    \"Refiner\": [\"refine\", \"enhance\", \"improve\"],\n",
        "    \"Validator\": [\"validate\", \"check_consistency\", \"fact_check_inline\"]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "POSSIBLE_ROLES = list(ROLE_DESCRIPTIONS.keys())\n",
        "\n",
        "\n",
        "# --- Born role preferences ---\n",
        "BORN_ROLES = {\n",
        "    \"Node_0\": [],\n",
        "    \"Node_1\": [],\n",
        "    \"Node_2\": []\n",
        "}\n",
        "\n",
        "# --- Precompute role embeddings ---\n",
        "if embedding_model is None:\n",
        "    raise RuntimeError(\"embedding_model is not initialized. Please check SETUP (Cell 1).\")\n",
        "\n",
        "\n",
        "\n",
        "ROLE_EMBEDDINGS = {}\n",
        "\n",
        "for role, description in ROLE_DESCRIPTIONS.items():\n",
        "    embed = embedding_model.encode(description)  # OK\n",
        "\n",
        "    # ✅ Normalize embedding\n",
        "    if embed is None:\n",
        "        embed = np.zeros(EMB_DIM)\n",
        "    elif isinstance(embed, list):\n",
        "        embed = np.array(embed)\n",
        "\n",
        "    # ⚡ Normalize embedding for cosine similarity\n",
        "    norm = np.linalg.norm(embed)\n",
        "    if norm > 0:\n",
        "        embed = embed / norm\n",
        "\n",
        "\n",
        "    ROLE_EMBEDDINGS[role] = embed\n",
        "\n",
        "# --- Validation ---\n",
        "for role, embedding in ROLE_EMBEDDINGS.items():\n",
        "    if embedding is None or (hasattr(embedding, \"__len__\") and len(embedding) == 0):\n",
        "        raise ValueError(f\"❌ Missing or empty embedding for role: {role}\")\n",
        "\n",
        "print(\"✅ All role embeddings initialized correctly.\")\n",
        "\n",
        "# Optional validation: every role's task must exist in TASK_DESCRIPTIONS\n",
        "for role, tasks in ROLE_TASK_MAP.items():\n",
        "    for task in tasks:\n",
        "        if task not in TASK_DESCRIPTIONS:\n",
        "            raise ValueError(f\"🚨 Task '{task}' for role '{role}' is not defined in TASK_DESCRIPTIONS.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Task descriptions (semantic grounding) ---\n",
        "TASK_DESCRIPTIONS = {\n",
        "    \"analyze\": \"Perform deep analysis of the content or data.\",\n",
        "    \"external_retrieval\": \"Query external APIs, structured tools, or function-based services to fetch up-to-date or specialized information. Does not include web scraping.\",\n",
        "    \"retrieve\": \"Fetch relevant information from stored memory or internal/external persistent databases using embedding similarity or non-similarity.\",\n",
        "    \"synthesize\": \"Combine multiple inputs into a coherent output.\",\n",
        "    \"summarize\": \"Produce a concise summary of the key points.\",\n",
        "    \"compute\": \"Perform numerical or logical calculations.\",\n",
        "    \"deduct\": \"Draw logical conclusions from given facts.\",\n",
        "    \"infer\": \"Make inferences based on context and evidence.\",\n",
        "    \"plan\": \"Lay out a step-by-step strategy or roadmap.\",\n",
        "    \"model\": \"Build a structured model or simulation.\",\n",
        "    \"code\": \"Write or generate executable code.\",\n",
        "    \"verify\": \"Check facts or validate data against trusted sources.\",\n",
        "    \"substantiate\": \"Provide evidence or references to support a claim.\",\n",
        "    \"ground\": \"Anchor abstract reasoning in concrete examples or data.\",\n",
        "    \"design\": \"Architect a system, workflow, or high-level structure.\",\n",
        "    \"debug\": \"Identify and fix errors in code or logic.\",\n",
        "    \"optimize\": \"Improve performance or efficiency of a solution.\",\n",
        "    \"critique\": \"Evaluate an argument or design and suggest improvements.\",\n",
        "    \"visualize\": \"Generate or describe a chart, diagram, or visual aid.\",\n",
        "    \"reverse-engineer\": \"Deconstruct a system or process to understand its components.\",\n",
        "    \"forecast\": \"Predict future trends or outcomes based on data.\",\n",
        "    \"mathematicize\": \"Formalize an idea in mathematical terms or equations.\",\n",
        "    \"logicalize\": \"Apply formal logic to structure arguments or proofs.\",\n",
        "    \"argument\": \"Construct or analyze a persuasive argument.\",\n",
        "    \"factualize\": \"Convert a statement into factual, evidence-based form.\",\n",
        "    \"rationalize\": \"Explain the reasoning or logic behind a decision or idea.\",\n",
        "    \"create\": \"Generate new content or ideas from scratch.\",\n",
        "    \"idealize\": \"Envision the perfect or optimal version of something.\",\n",
        "    \"abstractize\": \"Extract the underlying essence or abstraction of a concept.\",\n",
        "    \"generalize\": \"Form broad principles or patterns from specific examples.\",\n",
        "    \"specify\": \"Provide detailed, precise information or requirements.\",\n",
        "    \"contextualize\": \"Place information or findings in an appropriate and meaningful context.\",\n",
        "    \"test\": \"Evaluate accuracy, performance, or validity under defined conditions.\",\n",
        "    \"track\": \"Follow the progression or change of elements over time.\",\n",
        "    \"monitor\": \"Continuously observe data, systems, or activity for change or irregularities.\",\n",
        "    \"translate\": \"Convert information between languages, formats, or conceptual domains.\",\n",
        "    \"clarify\": \"Make complex, ambiguous, or vague information more understandable and precise.\",\n",
        "    \"restructure\": \"Reorganize the format, structure, or sequence of information to improve logic, usability, or coherence.\",\n",
        "    \"project\": \"Extend current patterns or trends into plausible future states.\",\n",
        "    \"model_trends\": \"Represent multi-factor temporal dynamics using structured abstractions.\",\n",
        "    \"sequence\": \"Organize actions or steps in the correct temporal or logical order.\",\n",
        "    \"structure\": \"Lay out components in a coherent, functional configuration.\",\n",
        "    \"organize\": \"Arrange information or elements to improve usability or clarity.\",\n",
        "    \"prioritize\": \"Assign importance or urgency levels to tasks or elements.\",\n",
        "    \"extract_causality\": \"Identify and isolate causal relationships between elements.\",\n",
        "    \"represent_relations\": \"Format connections or dependencies in a logical or visual structure.\",\n",
        "    \"build_graph\": \"Construct a structured representation of causality or process flow.\",\n",
        "    \"estimate\": \"Approximate quantities or outcomes using partial or statistical information.\",\n",
        "    \"analyze_stats\": \"Interpret numerical trends, distributions, or correlations.\",\n",
        "    \"reason_quantitatively\": \"Apply statistical or numerical reasoning to draw conclusions.\",\n",
        "    \"delegate\": \"Assign responsibilities or subtasks to appropriate agents or subcomponents.\",\n",
        "    \"coordinate\": \"Align timing, responsibilities, and dependencies across components or agents.\",\n",
        "    \"synchronize\": \"Ensure concurrent elements act in harmony or follow a shared schedule.\",\n",
        "    \"align\": \"Bring tasks, outputs, or strategies into logical and goal-consistent order.\",\n",
        "    \"extract_structure\": \"Identify and extract underlying structure, logic, or dependencies from the output.\",\n",
        "    \"encode_logic\": \"Convert implicit reasoning into a structured, machine-interpretable form.\",\n",
        "    \"digitalize_output\": \"Transform qualitative content into a digital twin, graph, or schema.\",\n",
        "    \"simulate\": \"Execute a scenario or logic structure under defined parameters to observe results.\",\n",
        "    \"test_outcomes\": \"Assess results or consequences by running defined simulations or trials.\",\n",
        "    \"explore_scenarios\": \"Investigate alternative outcomes through structured simulation or branching logic.\"\n",
        "}\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Final lists regenerated ---\n",
        "POSSIBLE_TASKS = list(TASK_DESCRIPTIONS.keys())\n",
        "\n",
        "# --- Precompute task embeddings (SAFE with np.array fallback) ---\n",
        "TASK_EMBEDDINGS = {}\n",
        "\n",
        "for task, desc in TASK_DESCRIPTIONS.items():\n",
        "    embed = embedding_model.encode(desc)     # OK\n",
        "\n",
        "    if embed is None:\n",
        "        embed = np.zeros(EMB_DIM)\n",
        "    elif isinstance(embed, list):\n",
        "        embed = np.array(embed)\n",
        "\n",
        "    norm = np.linalg.norm(embed)\n",
        "    if norm > 0:\n",
        "        embed = embed / norm\n",
        "\n",
        "    TASK_EMBEDDINGS[task] = embed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "METRIC_DESCRIPTIONS = {\n",
        "    \"accuracy\": \"The correctness and precision of the output.\",\n",
        "    \"coverage\": \"How completely the solution addresses all requirements.\",\n",
        "    \"coherence\": \"Logical consistency and connectedness of the answer.\",\n",
        "    \"novelty\": \"Degree of originality or innovation.\",\n",
        "    \"speed\": \"How quickly the solution or output is produced.\",\n",
        "    \"resource_usage\": \"Efficiency in the use of time or compute resources.\",\n",
        "    \"factuality\": \"Groundedness in established facts or data.\",\n",
        "    \"strategic_alignment\": \"Alignment with long-term goals or strategies.\",\n",
        "    \"relevance\": \"Appropriateness to the prompt or objectives.\",\n",
        "    \"temporal_dependency_score\": \"The extent to which the output reasoning depends on time-sequenced events or time-based causality (e.g., forecasts, policy timelines).\",\n",
        "    \"trend_complexity\": \"Intricacy of multi-factor trends described.\",\n",
        "    \"factual_conflictuality\": \"Presence of factual tension or contradiction within the output.\",\n",
        "    \"entailment_discrepancy\": \"How well the output logically follows from the prompt or retrieved grounding context.\",\n",
        "    \"causal_complexity\": \"Number and depth of causal relations (chains, branches) within the output.\",\n",
        "    \"inferential_depth\": \"Presence of layered inference steps (abduction, deduction, induction) required or performed.\",\n",
        "    \"numerical_volatility\": \"Detects unstable or non-linear numeric behavior in reasoning (e.g., switching units, wide ranges, or poorly justified estimates).\",\n",
        "    \"mission_chain_depth\": \"The number of conceptual or subgoal layers in the mission prompt itself.\",\n",
        "    \"action_sequence_complexity\": \"Evaluates how elaborate or ordered the required actions or procedural steps are in the output.\",\n",
        "    \"digitalizability\": \"How well an output (or prompt) can be converted into a structured, machine-interpretable representation.\",\n",
        "    \"simulation_utility\": \"Estimates the potential value or feasibility of simulating the output to test outcomes, run scenarios, or evaluate consequences.\"\n",
        "}\n",
        "\n",
        "POSSIBLE_METRICS = list(METRIC_DESCRIPTIONS.keys())\n",
        "\n",
        "# --- Precompute metric embeddings (SAFE with np.array fallback) ---\n",
        "METRIC_EMBEDDINGS = {}\n",
        "for metric, desc in METRIC_DESCRIPTIONS.items():\n",
        "    emb = embedding_model.encode(desc)\n",
        "    if emb is None:\n",
        "        emb = np.zeros(EMB_DIM)\n",
        "    elif isinstance(emb, list):\n",
        "        emb = np.array(emb)\n",
        "    norm = np.linalg.norm(emb)\n",
        "    if norm > 0:\n",
        "        emb = emb / norm\n",
        "    METRIC_EMBEDDINGS[metric] = emb\n",
        "\n",
        "# Validate\n",
        "for metric, emb in METRIC_EMBEDDINGS.items():\n",
        "    if emb is None or (hasattr(emb, \"__len__\") and len(emb) == 0):\n",
        "        raise ValueError(f\"❌ Missing or empty embedding for metric: {metric}\")\n",
        "print(\"✅ All metric embeddings initialized correctly.\")\n",
        "\n",
        "\n",
        "# Static, used only for inline role activation via metric thresholds → not refreshed or pruned, just checked at runtime. Used for gating only.\n",
        "ROLE_METRIC_MAP = {\n",
        "    \"Grapher\":            \"causal_complexity\",\n",
        "    \"Statisticien\":       \"inferential_depth\",\n",
        "    \"Twin_Digitalizer\":   \"digitalizability\",\n",
        "    \"Simulator\":          \"simulation_utility\",\n",
        "    \"Strategist\":         \"strategic_depth\",\n",
        "    \"Planner\":            \"planning_span\",\n",
        "    \"Analyst\":            \"analytic_load\",\n",
        "    \"Interpreter\":        \"ambiguity_level\",\n",
        "    \"Verifier\":           \"grounding_gap\",\n",
        "    \"Forecaster\":         \"predictive_uncertainty\",\n",
        "    \"Decomposer\":         \"problem_entropy\",\n",
        "    \"Agent_Orchestrator\": \"coordination_cost\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "FEATURE_DESCRIPTIONS = {\n",
        "    \"external_retrieval\": \"Query external APIs, structured tools, or function-based services to fetch up-to-date or specialized information. Does not include web scraping.\",\n",
        "    \"web_scraping\":          \"Access live, current, or real-time data from the web.\",\n",
        "    \"knowledge_graph_query\": \"Query structured knowledge or relationships between entities.\",\n",
        "    \"code_execution\":        \"Write or execute code, implement functions, or provide scripts.\",\n",
        "    \"sentiment_analysis\":    \"Analyze sentiment, feedback, or opinions in the text.\",\n",
        "    \"entity_extraction\":     \"Extract names, entities, or lists from the content.\",\n",
        "    \"data_query\":            \"Query numeric data, statistics, tables, or perform calculations.\",\n",
        "    \"validation\":            \"Verify, validate, or fact-check the given information.\",\n",
        "    \"summarization\":         \"Summarize, brief, or condense the provided content.\",\n",
        "    \"groundedness_check\": \"Use factuality scoring, entailment, or statistical verification to validate whether output is consistent with real-world references.\",\n",
        "    \"inline_refinement\": \"Enable inline refinement of output during initial generation based on internal thresholds, avoiding a second LLM call.\"\n",
        "}\n",
        "\n",
        "FEATURE_EMBEDDINGS = compute_feature_description_embeddings(embedding_model, FEATURE_DESCRIPTIONS, EMB_DIM)\n",
        "\n",
        "\n",
        "def compute_feature_description_embeddings(embedding_model, descriptions=FEATURE_DESCRIPTIONS, emb_dim=384):\n",
        "    embeddings = {}\n",
        "    for key, desc in descriptions.items():\n",
        "        embed = embedding_model.encode(desc)\n",
        "        if embed is None:\n",
        "            embed = np.zeros(emb_dim)\n",
        "        elif isinstance(embed, list):\n",
        "            embed = np.array(embed)\n",
        "        norm = np.linalg.norm(embed)\n",
        "        if norm > 0:\n",
        "            embed = embed / norm\n",
        "        embeddings[key] = embed\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_uvr_status(values, agent):\n",
        "    min_window = agent.meta_parameters.get(\"uvr_min_window\", 5)\n",
        "    inflection_threshold = agent.meta_parameters.get(\"uvr_inflection_ratio\", 1.5)\n",
        "\n",
        "    if len(values) < 2 * min_window:\n",
        "        return False, 0.0, 0.0, 1.0\n",
        "\n",
        "    recent = values[-min_window:]\n",
        "    prior = values[-2 * min_window:-min_window]\n",
        "\n",
        "    recent_var = np.var(recent)\n",
        "    prior_var = np.var(prior)\n",
        "    var_ratio = (recent_var / (prior_var + 1e-8)) if prior_var > 0 else float(\"inf\")\n",
        "\n",
        "    uvr_triggered = var_ratio > inflection_threshold\n",
        "    return uvr_triggered, recent_var, prior_var, var_ratio\n",
        "\n",
        "\n",
        "# Pre-output born roles and semantic similarity-based role assignment at initizliaztion: no meta-parameters.\n",
        "# Pre-output inline role injection (first pass)=default inline role prompt shaping (at every round). Role's activation probability or meta-weight is boosted if the role is among born roles. Meta-parameters influence the LLM's first generation and need to be tightly coupled to prompt shaping and generation dynamics.\n",
        "# Post-output rerouting triggered via fallback logic only if validation or metric thresholds fail (second pass). Meta-parameters here should mirror those used for the pre-output inline roles, or at least include similar attributes.\n",
        "# Post-output inline roles via metric-triggered activation never cause rerouting if the first output is valid (no second pass, just metric-based tagging). Used for tagging, scoring, or internal logging. Meta-parameters here are more policy-based (e.g., thresholds for tagging) and don’t control LLM behavior. These don’t need temperature/top-p.\n",
        "# The post-output inline roles (used for tagging) are the core of the gating mechanism that supplements or overrides the semantic similarity–based meaningfulness score.\n",
        "# Tagging roles do not alter output. They log that they would have been useful\n",
        "# Annotation-based modification roles do not call LLM as the tagging ones but alter outputs. Some roles can do both tagging and modification.\n",
        "# True Rewriting (Structured Post-Processing) roles do not involve LLM call, but structurally change output.\n",
        "INLINE_ROLE_BEHAVIOR = {\n",
        "    # ──────────────── Pre-output inline roles (shape LLM generation) ────────────────\n",
        "    \"Refiner\": {\n",
        "        \"temperature\": 0.2,\n",
        "        \"top_p\": 0.9,\n",
        "        \"reroutable\": True,\n",
        "        \"pre_output\": True,\n",
        "        \"post_output\": False,\n",
        "    },\n",
        "    \"Validator\": {\n",
        "        \"temperature\": 0.3,\n",
        "        \"top_p\": 0.8,\n",
        "        \"reroutable\": True,\n",
        "        \"pre_output\": True,\n",
        "        \"post_output\": True,  # Also used in metric tagging\n",
        "    },\n",
        "    \"Executor\": {\n",
        "        \"temperature\": 0.1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"reroutable\": True,\n",
        "        \"pre_output\": True,\n",
        "        \"post_output\": True,\n",
        "    },\n",
        "\n",
        "    # ──────────────── Post-output only (no prompt shaping) ────────────────\n",
        "    \"Simulator\": {\n",
        "        \"pre_output\": False,\n",
        "        \"post_output\": True,\n",
        "        \"reroutable\": False,\n",
        "    },\n",
        "    \"Digitalizer\": {\n",
        "        \"pre_output\": False,\n",
        "        \"post_output\": True,\n",
        "        \"reroutable\": False,\n",
        "    },\n",
        "    \"Statisticien\": {\n",
        "        \"pre_output\": False,\n",
        "        \"post_output\": True,\n",
        "        \"reroutable\": False,\n",
        "    },\n",
        "    \"Grapher\": {\n",
        "        \"pre_output\": False,\n",
        "        \"post_output\": True,\n",
        "        \"reroutable\": False,\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OuBOauD300tF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a306345b-72aa-40c9-cf71-070baae9878d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All role embeddings initialized correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ==================== 🧠 CELL 4 — PROMPT EMBEDDING CACHE =====\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "# Global cache for prompt embeddings to avoid recomputation\n",
        "PROMPT_EMBED_CACHE = {}\n",
        "\n",
        "def get_task_embedding(task: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Retrieve the embedding for a known task, or return a zero-vector if missing.\n",
        "    \"\"\"\n",
        "    # TASK_EMBEDDINGS is your dict mapping task names to np.ndarray embeddings\n",
        "    return TASK_EMBEDDINGS.get(task, np.zeros(EMB_DIM))\n",
        "\n",
        "def get_prompt_embedding(prompt):\n",
        "    \"\"\"\n",
        "    Return cached embedding for a normalized prompt, or compute and store it.\n",
        "    Normalization avoids duplicate cache entries for similar text.\n",
        "    Ensures that the returned embedding is always a numpy array.\n",
        "    \"\"\"\n",
        "    normalized_prompt = prompt.strip().lower()\n",
        "\n",
        "    if normalized_prompt not in PROMPT_EMBED_CACHE:\n",
        "        embedding = embedding_model.encode(normalized_prompt)   # OK\n",
        "\n",
        "        # ✅ Normalize the embedding → always numpy array\n",
        "        if embedding is None:\n",
        "            embedding = np.zeros(EMB_DIM)\n",
        "        elif isinstance(embedding, list):\n",
        "            embedding = np.array(embedding)\n",
        "\n",
        "        PROMPT_EMBED_CACHE[normalized_prompt] = embedding\n",
        "\n",
        "    return PROMPT_EMBED_CACHE[normalized_prompt]\n",
        "\n",
        "\n",
        "# Create the FAISS index for graph embeddings\n",
        "graph_index = faiss.IndexFlatL2(GRAPH_EMB_DIM)\n",
        "graph_key_map = {}  # To map FAISS index positions to mission keys\n",
        "\n",
        "def find_similar_graphs(graph_embedding, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieve the most similar causal graphs from FAISS based on graph embedding.\n",
        "    Returns a list of (prompt_key, distance) pairs.\n",
        "    \"\"\"\n",
        "    vec = np.array(graph_embedding).astype(\"float32\").reshape(1, -1)\n",
        "    if graph_index.ntotal == 0:\n",
        "        return []\n",
        "\n",
        "    D, I = graph_index.search(vec, top_k)\n",
        "    return [\n",
        "        (graph_key_map.get(i, \"UNKNOWN_KEY\"), D[0][j])\n",
        "        for j, i in enumerate(I[0]) if i < len(graph_key_map)\n",
        "    ]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R_0g1CjJnnDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ==================== 🌐 CELL 5 — WEB SCRAPING + UTILITIES ===\n",
        "# ============================================================\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from math import exp\n",
        "from typing import List\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "# --- Utility sigmoid (optional in scoring) ---\n",
        "def sigmoid(x: float) -> float:\n",
        "    return 1 / (1 + exp(-x))\n",
        "\n",
        "# --- Per-engine Selenium scrapers ------------------------------------------\n",
        "\n",
        "def scrape_google_with_selenium(query: str, max_chars: int = 1000) -> str:\n",
        "    \"\"\"Return concatenated snippets from Google search results.\"\"\"\n",
        "    options = Options()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--window-size=1920,1080\")\n",
        "    options.add_argument(\"--user-agent=Mozilla/5.0\")\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    try:\n",
        "        url = f\"https://www.google.com/search?q={query.replace(' ', '+')}\"\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "        els = driver.find_elements(By.CSS_SELECTOR, \"div.MjjYud div.VwiC3b\")\n",
        "        text = \" \".join(el.text for el in els if el.text.strip())\n",
        "        return text[:max_chars] if text else \"No Google web results found.\"\n",
        "    except Exception as e:\n",
        "        return f\"⚠️ Google scraping failed: {e}\"\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "def scrape_bing_with_selenium(query: str, max_chars: int = 1000) -> str:\n",
        "    \"\"\"Return concatenated snippets from Bing search results.\"\"\"\n",
        "    options = Options()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--window-size=1920,1080\")\n",
        "    options.add_argument(\"--user-agent=Mozilla/5.0\")\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    try:\n",
        "        url = f\"https://www.bing.com/search?q={query.replace(' ', '+')}\"\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "        els = driver.find_elements(By.CSS_SELECTOR, \"div.b_caption p\")\n",
        "        text = \" \".join(el.text for el in els if el.text.strip())\n",
        "        return text[:max_chars] if text else \"No Bing web results found.\"\n",
        "    except Exception as e:\n",
        "        return f\"⚠️ Bing scraping failed: {e}\"\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "def scrape_baidu_with_selenium(query: str, max_chars: int = 1000) -> str:\n",
        "    \"\"\"Return concatenated snippets from Baidu search results.\"\"\"\n",
        "    options = Options()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--window-size=1920,1080\")\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    try:\n",
        "        url = f\"https://www.baidu.com/s?wd={query.replace(' ', '+')}\"\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "        els = driver.find_elements(By.CSS_SELECTOR, \"div.result\")\n",
        "        text = \" \".join(el.text for el in els if el.text.strip())\n",
        "        return text[:max_chars] if text else \"No Baidu web results found.\"\n",
        "    except Exception as e:\n",
        "        return f\"⚠️ Baidu scraping failed: {e}\"\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "# --- Semantic‐filtering stub -----------------------------------------------\n",
        "\n",
        "def semantic_filter(\n",
        "    snippets: List[str],\n",
        "    query: str,\n",
        "    top_k: int,\n",
        "    similarity_threshold: float\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Keep up to `top_k` snippets whose embedding cosine similarity to `query`\n",
        "    is ≥ `similarity_threshold`, ranked from most to least similar.\n",
        "    \"\"\"\n",
        "\n",
        "    if not snippets:\n",
        "        return []\n",
        "\n",
        "    # ── Encode query ───────────────────────────────────────────────────────\n",
        "    query_vec = embedding_model.encode(query)\n",
        "    if query_vec is None:\n",
        "        dim = embedding_model.get_sentence_embedding_dimension()\n",
        "        query_vec = np.zeros(dim)\n",
        "    elif isinstance(query_vec, list):\n",
        "        query_vec = np.array(query_vec)\n",
        "\n",
        "    # ── Encode snippets (batch) ────────────────────────────────────────────\n",
        "    snip_vecs = embedding_model.encode(snippets, convert_to_numpy=True)\n",
        "    if snip_vecs is None:\n",
        "        dim = query_vec.shape[0]\n",
        "        snip_vecs = np.zeros((len(snippets), dim))\n",
        "    elif isinstance(snip_vecs, list):\n",
        "        snip_vecs = np.array(snip_vecs)\n",
        "\n",
        "    # ── Compute cosine similarities ───────────────────────────────────────\n",
        "    sims = cosine_similarity([query_vec], snip_vecs)[0]\n",
        "\n",
        "    # ── Filter + rank ─────────────────────────────────────────────────────\n",
        "    filtered = [(s, sc) for s, sc in zip(snippets, sims) if sc >= similarity_threshold]\n",
        "    filtered.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # ── Return top_k snippets only ────────────────────────────────────────\n",
        "    return [s for s, _ in filtered[:top_k]]\n",
        "\n",
        "\n",
        "\n",
        "# --- Unified wrapper with semantic filtering ------------------------------\n",
        "def web_scrape(\n",
        "    query: str,\n",
        "    engine: str            = \"google\",\n",
        "    max_chars: int         = 1000,\n",
        "    top_k: int             = 5,\n",
        "    similarity_threshold: float = 0.3\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    1) Run Selenium scrape on chosen engine.\n",
        "    2) Split the raw text into individual snippets.\n",
        "    3) Pass through semantic_filter() to pick top_k relevant results.\n",
        "    \"\"\"\n",
        "    if engine == \"google\":\n",
        "        raw = scrape_google_with_selenium(query, max_chars)\n",
        "    elif engine == \"bing\":\n",
        "        raw = scrape_bing_with_selenium(query, max_chars)\n",
        "    elif engine == \"baidu\":\n",
        "        raw = scrape_baidu_with_selenium(query, max_chars)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported search engine: {engine}\")\n",
        "\n",
        "    # turn the returned string into a list of candidate snippets\n",
        "    lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]\n",
        "    # further split any single‐line bullets\n",
        "    snippets: List[str] = []\n",
        "    for ln in lines:\n",
        "        parts = [p.strip() for p in ln.split(\"·\") if p.strip()]\n",
        "        snippets.extend(parts)\n",
        "\n",
        "    # finally apply semantic filtering\n",
        "    return semantic_filter(snippets, query, top_k, similarity_threshold)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EgAVGM41dGFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ==================== 🧪 CELL 6 — TEST WEB SCRAPING =========\n",
        "# ============================================================\n",
        "\n",
        "# Make sure you’ve already run CELL 5 so web_scrape() is defined.\n",
        "\n",
        "def test_scraping_semantic(\n",
        "    query: str,\n",
        "    engines: list[str] = [\"google\", \"bing\", \"baidu\"],\n",
        "    top_k: int = 3,\n",
        "    similarity_threshold: float = 0.4\n",
        "):\n",
        "    for engine in engines:\n",
        "        print(f\"\\n🔎 Testing {engine.capitalize()} for “{query}” (top_k={top_k}, sim_thresh={similarity_threshold})…\")\n",
        "        try:\n",
        "            hits = web_scrape(\n",
        "                query,\n",
        "                engine=engine,\n",
        "                max_chars=1000,\n",
        "                top_k=top_k,\n",
        "                similarity_threshold=similarity_threshold\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error calling web_scrape: {e}\")\n",
        "            continue\n",
        "\n",
        "        if not hits:\n",
        "            print(\" - No snippets returned.\")\n",
        "        else:\n",
        "            for idx, snippet in enumerate(hits, start=1):\n",
        "                print(f\" {idx}. {snippet}\")\n",
        "\n",
        "# Run the semantic test\n",
        "test_scraping_semantic(\"Artificial Intelligence\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HW3J3Wmw0wWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2751a742-f3d2-49d9-921a-3e620e1066c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 Testing Google for “Artificial Intelligence” (top_k=3, sim_thresh=0.4)…\n",
            " - No snippets returned.\n",
            "\n",
            "🔎 Testing Bing for “Artificial Intelligence” (top_k=3, sim_thresh=0.4)…\n",
            " - No snippets returned.\n",
            "\n",
            "🔎 Testing Baidu for “Artificial Intelligence” (top_k=3, sim_thresh=0.4)…\n",
            " 1. 百度百科 artificial intelligence是什么意思_artificial intellige...\n",
            " 2. 金山词霸 artificial-intelligence - 搜索 词典\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ==================== 🤖 CELL 7 — STRATEGIC AGENT =====\n",
        "# ============================================================\n",
        "import random\n",
        "import time\n",
        "import math               # ← new\n",
        "import difflib            # ← new\n",
        "import numpy as np        # already in globals but explicit here is safer\n",
        "from IPython.display import display, Markdown  # ← for highlight_text_differences\n",
        "from scipy import stats\n",
        "from collections import Counter\n",
        "import networkx as nx\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "\n",
        "# =======================\n",
        "# 7.1. **Initialization and Setup**\n",
        "# =======================\n",
        "class StrategicAgent:\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name,\n",
        "        model_key,\n",
        "        born_roles=None,\n",
        "        born_metrics=None,\n",
        "        runner=None\n",
        "    ):\n",
        "        self.name = name\n",
        "        self.model_key = model_key\n",
        "\n",
        "\n",
        "\n",
        "        # Model selection\n",
        "        if model_key == \"openai\":\n",
        "            self.model_name = OPENAI_MODEL_NAME\n",
        "        elif model_key == \"gemini\":\n",
        "            self.model_name = GEMINI_MODEL_NAME\n",
        "        elif model_key == \"claude\":\n",
        "            self.model_name = CLAUDE_MODEL_NAME\n",
        "        elif model_key == \"llama\":\n",
        "            self.model_name = LLAMA_MODEL_NAME\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model_key {model_key!r}; cannot set self.model_name\")\n",
        "\n",
        "        self.born_roles = born_roles[:] if born_roles else []\n",
        "        self.roles = born_roles[:] if born_roles else []\n",
        "        self.roles_history = [self.roles[:]]\n",
        "        self.purge_log = []\n",
        "        self.model = available_models[model_key]\n",
        "        self.runner = runner\n",
        "        self.task_store = {\n",
        "            key: {\n",
        "                \"key\": key,\n",
        "                \"desc\": TASK_DESCRIPTIONS[key],\n",
        "                \"embedding\": None,\n",
        "                \"last_score\": 0.0\n",
        "            }\n",
        "            for key in TASK_DESCRIPTIONS\n",
        "        }\n",
        "        self.born_metrics = born_metrics[:] if born_metrics else []\n",
        "        self.metrics = born_metrics[:] if born_metrics else []\n",
        "        self.metrics_history = [self.metrics[:]]  # (Optional, for tracking each round)\n",
        "\n",
        "\n",
        "        # Compute born_tasks from born_roles\n",
        "        self.born_tasks = list(set(\n",
        "            t for r in self.born_roles for t in ROLE_TASK_MAP.get(r, [])\n",
        "        ))\n",
        "        self.tasks = self.born_tasks[:]  # Initialize with full set (can be pruned later)\n",
        "        self.tasks_history = [self.tasks[:]]  # Optional: track task assignment over rounds\n",
        "\n",
        "        # Spike memory and logging\n",
        "        self.signal_spike_log = deque(maxlen=100)  # Store recent high-signal events\n",
        "        self.signal_clusters = []\n",
        "\n",
        "\n",
        "\n",
        "        # --- Physiology tracking ---\n",
        "        self.attention = 1.0\n",
        "        self.fatigue = 0.0\n",
        "        self.hunger = 0.0\n",
        "        self.attention_history = []\n",
        "        self.fatigue_history = []\n",
        "        self.hunger_history = []\n",
        "        self._phys_log_current = []\n",
        "\n",
        "        # --- Validation & time tracking ---\n",
        "        self.local_time_log = []\n",
        "        self.local_interval_log = []\n",
        "        self.local_round = 1\n",
        "        self.interval = 1\n",
        "        self.local_mission_time_log = []\n",
        "\n",
        "        # --- Recovery/stagnation tracking ---\n",
        "        self.stagnation_counter = 0\n",
        "\n",
        "\n",
        "        self.features = []\n",
        "\n",
        "        self.available_models = available_models\n",
        "        self.short_memory = []\n",
        "        self.long_memory = {}\n",
        "        self.usage_count = 0\n",
        "        self.in_cooperation = False\n",
        "        self.meaningless_output_counter = 0\n",
        "        self.external_access_count = 0\n",
        "        self.enable_web_scraping = True\n",
        "        self.start_time = time.time()\n",
        "        # 🆕 Reroute safeguard flag\n",
        "        self.has_been_rerouted = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Dual-layer variance tracking for foresight reactivation\n",
        "        self.variance_history = {\n",
        "            \"uvr_similarity_short\": deque(maxlen=5),      # short-term rolling window\n",
        "            \"uvr_similarity_reference\": deque(maxlen=10), # stable past window\n",
        "            \"reactivation_flag\": False                    # dynamic gate\n",
        "        }\n",
        "\n",
        "        # --- Unified meta-parameters (short-term & long-term) ---\n",
        "        self.meta_parameters = {\n",
        "            # --- Structure: roles, pruning, dropout, reactivation ---\n",
        "            \"role_assignment_threshold\": 0.7,\n",
        "            \"role_threshold_range\": (0.4, 0.9),\n",
        "            \"pruning_threshold\": 0.2,\n",
        "            \"pruning_threshold_range\": (0.2, 0.8),\n",
        "            \"pruning_decay_rate\": 3.0,\n",
        "            \"pruning_decay_range\": (1.0, 5.0),\n",
        "            \"pruning_reactivation_prob\": 0.1,\n",
        "            \"pruning_reactivation_range\": (0.05, 0.3),\n",
        "            \"dropout_base_probability\": 0.1,\n",
        "            \"dropout_base_prob_range\": (0.1, 0.5),\n",
        "            \"dropout_decay_rate\": 1.5,\n",
        "            \"dropout_decay_range\": (1.0, 5.0),\n",
        "\n",
        "            # --- Task and role selection ---\n",
        "            \"max_roles\": 3,\n",
        "            \"max_roles_range\": (1, 5),\n",
        "            \"max_tasks\": 5,\n",
        "            \"max_tasks_range\": (1, 7),\n",
        "            \"top_k_tasks\": 2,\n",
        "            \"top_k_tasks_range\": (1, 5),\n",
        "            \"task_threshold\": 0.4,\n",
        "            \"task_threshold_range\": (0.2, 0.7),\n",
        "            \"task_similarity_threshold\": 0.4,\n",
        "            \"task_similarity_range\": (0.2, 0.8),\n",
        "            \"reuse_similarity_threshold\": 0.75,\n",
        "            \"reuse_similarity_range\": (0.6, 0.9),\n",
        "\n",
        "            # Metric selection\n",
        "            \"metric_threshold_range\": (0.3, 0.8),\n",
        "            \"metric_selection_threshold\": 0.5,\n",
        "            \"metric_threshold_lr\": 0.01,\n",
        "            \"top_k_metrics\": 3\n",
        "\n",
        "            \"max_metrics\": 3,\n",
        "            \"max_metrics_range\": (1, 5),\n",
        "            \"top_k_metrics_range\": (1, 5),\n",
        "            \"metric_similarity_threshold\": 0.4,\n",
        "            \"metric_similarity_range\": (0.2, 0.8),\n",
        "\n",
        "\n",
        "            \"entropy_expected\": 0.65,\n",
        "            \"entropy_margin\": 0.15,\n",
        "            \"entropy_softening_factor\": 0.05,\n",
        "\n",
        "            # --- Jollycard & strategic exploration ---\n",
        "            \"jollycard_injection_weight\": 0.5,\n",
        "            \"jollycard_injection_weight_range\": (0.1, 0.9),\n",
        "            \"jollycard_importance_threshold\": 0.3,\n",
        "            \"jollycard_importance_threshold_range\": (0.2, 0.7),\n",
        "            \"jollycard_sampling_temperature\": 1.0,\n",
        "            \"jollycard_sampling_temperature_range\": (0.5, 2.0),\n",
        "\n",
        "            # --- Noise, randomness, learning ---\n",
        "            \"score_noise_min\": 0.01,\n",
        "            \"score_noise_max\": 0.2,\n",
        "            \"score_noise_min_floor\": 0.001,\n",
        "            \"score_noise_max_ceiling\": 0.3,\n",
        "            \"score_noise_lr\": 0.01,\n",
        "            \"lr_efficiency_weight\": 0.4,\n",
        "            \"lr_meaningfulness_weight\": 0.4,\n",
        "            \"lr_decay_rate\": 0.05,\n",
        "            \"lr_min\": 0.001,\n",
        "            \"lr_max\": 0.05,\n",
        "            \"lr_min_range\": (0.0005, 0.005),\n",
        "            \"lr_max_range\": (0.02, 0.1),\n",
        "\n",
        "            # --- Decay, stagnation, and misc. ---\n",
        "            \"stagnation_recovery_threshold\": 5,\n",
        "            \"lambda_time\": 0.5,\n",
        "            \"lambda_time_range\": (0.1, 1.0),\n",
        "            \"lambda_usage\": 0.3,\n",
        "            \"lambda_usage_range\": (0.1, 0.8),\n",
        "            \"external_decay_time_weight\": 0.05,\n",
        "            \"external_decay_usage_weight\": 1.0,\n",
        "            \"external_decay_time_range\": (0.01, 0.2),\n",
        "            \"external_decay_usage_range\": (0.1, 2.0),\n",
        "            \"external_decay_score_threshold\": 0.2,\n",
        "            \"external_decay_score_range\": (0.05, 0.5),\n",
        "\n",
        "            # --- Physiology meta-parameters ---\n",
        "            \"attention_time_weight\": 0.05,\n",
        "            \"attention_fatigue_weight\": 0.05,\n",
        "            \"attention_hunger_weight\": 0.05,\n",
        "            \"attention_threshold\": 0.3,\n",
        "            \"fatigue_task_weight\": 0.15,\n",
        "            \"fatigue_time_weight\": 0.03,\n",
        "            \"dropout_time_weight\": 0.05,\n",
        "            \"hunger_usage_weight\": 0.1,\n",
        "            \"dropout_attention_weight\": 0.1,\n",
        "            \"dropout_fatigue_weight\": 0.1,\n",
        "            \"dropout_hunger_weight\": 0.1,\n",
        "            \"pruning_attention_weight\": 0.2,\n",
        "            \"pruning_fatigue_weight\": 0.2,\n",
        "            \"pruning_hunger_weight\": 0.2,\n",
        "            \"external_call_base_prob\": 0.4,\n",
        "            \"external_attention_weight\": 0.2,\n",
        "            \"external_fatigue_weight\": 0.2,\n",
        "            \"external_hunger_weight\": 0.2,\n",
        "\n",
        "            # --- Short-term meta-parameters (merged here) ---\n",
        "            \"task_feature_coupling\": 0.5,\n",
        "            \"strategy_fit\": 0.5,\n",
        "            \"cooperation_bias\": 0.5,\n",
        "            \"cooperation_baseline\": 0.5,\n",
        "            \"cooperation_baseline_range\": (0.2, 0.8),\n",
        "            \"cooperation_randomness_range\": (0.05, 0.3),\n",
        "\n",
        "\n",
        "\n",
        "            \"uvr_weight_prompt\": 0.2,          # weight of prompt similarity in UVR\n",
        "            \"uvr_weight_output\": 0.2,          # weight of output similarity in UVR\n",
        "            \"uvr_weight_graph\":  0.2,          # weight of graph similarity in UVR\n",
        "            \"uvr_weight_path\":   0.2,          # weight of path similarity in UVR\n",
        "            \"uvr_weight_physio\": 0.2,          # weight of physiological similarity in UVR\n",
        "            \"uvr_volatility_threshold\": 0.1,   # total volatility needed to trigger short-term UVR adjustment\n",
        "            \"uvr_min_window\": 5,               # rolling window size for computing variance\n",
        "            \"uvr_inflection_ratio\": 1.5,        # how much recent variance must exceed prior to trigger UVR\n",
        "\n",
        "\n",
        "            \"uvr_weight_prompt\":  0.2,\n",
        "            \"uvr_weight_output\":  0.2,\n",
        "            \"uvr_weight_graph\":   0.2,\n",
        "            \"uvr_weight_path\":    0.2,\n",
        "            \"uvr_weight_physio\":  0.2,\n",
        "\n",
        "\n",
        "            # ─── Pre-Output Inline Role Parameters ───────────────────────────────\n",
        "\n",
        "            # Temperature and top-p for prompt shaping during first LLM call\n",
        "            \"refiner_temperature\": 0.2,\n",
        "            \"refiner_top_p\": 0.9,\n",
        "\n",
        "            \"validator_temperature\": 0.3,\n",
        "            \"validator_top_p\": 0.8,\n",
        "\n",
        "            \"executor_temperature\": 0.1,\n",
        "            \"executor_top_p\": 0.95,\n",
        "\n",
        "            # Meta-weights for pre-output roles (used in activation decision)\n",
        "            \"refiner_weight\": 1.0,\n",
        "            \"validator_weight\": 1.0,\n",
        "            \"executor_weight\": 1.0,\n",
        "\n",
        "            # Optional: Boost if role is also a born role (pre-output only)\n",
        "            \"born_role_boost\": 0.2,                        # ← applies to pre-output weights\n",
        "            \"inline_role_weight_boost_if_born\": 1.2,      # ← alternate label (keep one)\n",
        "\n",
        "            # ─── Post-Output Inline Role Thresholds (Tagging & Gating) ───────────\n",
        "\n",
        "            \"validation_threshold\": 0.75,         # trigger Validator tag\n",
        "            \"refinement_trigger_score\": 0.6,      # trigger Refiner tag\n",
        "            \"inline_rerouting_threshold\": 0.6,    # reroute only if fallback needed\n",
        "\n",
        "            # Meta-weights for post-output taggable roles (used in utility or memory tagging)\n",
        "            \"simulator_weight\": 1.0,\n",
        "            \"digitalizer_weight\": 1.0,\n",
        "            \"grapher_weight\": 1.0,\n",
        "            \"statistician_weight\": 1.0,\n",
        "            \"strategist_weight\": 1.0,\n",
        "            \"analyst_weight\": 1.0,\n",
        "            \"builder_weight\": 1.0,\n",
        "            \"scout_weight\": 1.0,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            \"uvr_min_window\": 5,\n",
        "            \"uvr_inflection_threshold\": 1.5,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # --- Static task pool per role ---\n",
        "        self.role_task_map = {}\n",
        "        for role in self.born_roles:\n",
        "            tasks = self.get_task_candidates_for_roles(\n",
        "                roles=[role],\n",
        "                prompt=getattr(self, 'last_mission_prompt', None),\n",
        "                top_k=self.meta_parameters.get(\"max_tasks\", 5)\n",
        "            )\n",
        "            self.role_task_map[role] = tasks\n",
        "\n",
        "\n",
        "    def inject_inline_roles_into_prompt(self, base_prompt):\n",
        "        \"\"\"\n",
        "        Inject pre-output inline roles (Refiner, Validator, Executor) into the LLM prompt\n",
        "        based on meta-learned weights and born-role boosting.\n",
        "        \"\"\"\n",
        "        inline_roles_to_inject = []\n",
        "\n",
        "        for role in [\"Refiner\", \"Validator\", \"Executor\"]:\n",
        "            if self.should_inject_inline_role(role):\n",
        "                inline_roles_to_inject.append(role)\n",
        "\n",
        "        if not inline_roles_to_inject:\n",
        "            return base_prompt  # No changes\n",
        "\n",
        "        # Build role-shaping instructions\n",
        "        role_instructions = []\n",
        "        for role in inline_roles_to_inject:\n",
        "            behavior = INLINE_ROLE_BEHAVIOR.get(role, {})\n",
        "            temperature = self.meta_parameters.get(f\"{role.lower()}_temperature\", behavior.get(\"temperature\", 0.7))\n",
        "            top_p = self.meta_parameters.get(f\"{role.lower()}_top_p\", behavior.get(\"top_p\", 0.9))\n",
        "\n",
        "            instruction = (\n",
        "                f\"\\n\\n---\\n\"\n",
        "                f\"🔧 Role: {role}\\n\"\n",
        "                f\"• Style Guidance: Behave as a {role.lower()}.\\n\"\n",
        "                f\"• Generation Parameters: temperature={temperature}, top_p={top_p}\\n\"\n",
        "                f\"---\\n\"\n",
        "            )\n",
        "            role_instructions.append(instruction)\n",
        "\n",
        "        # Prepend to the original prompt\n",
        "        return \"\".join(role_instructions) + base_prompt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_effective_meta_weight(self, role_name, base_weight_key, boost_if_born=1.2):\n",
        "        \"\"\"\n",
        "        Compute the effective meta-weight for an inline role.\n",
        "        - `role_name`: The inline role being evaluated (e.g., \"Refiner\").\n",
        "        - `base_weight_key`: The meta-parameter key for this role (e.g., \"refiner_weight\").\n",
        "        - `boost_if_born`: Optional multiplicative boost if the role is a born role.\n",
        "        This utility applies only to pre-output inline roles that influence the first LLM generation. It uses: meta_parameters[\"<role>_weight\"] and a boost if the role is also a born role.\n",
        "        \"\"\"\n",
        "        # Get the base weight (meta-learned or default)\n",
        "        base_weight = self.meta_parameters.get(base_weight_key, 1.0)\n",
        "\n",
        "        # Apply boost if this role is a born role\n",
        "        if role_name in self.born_roles:\n",
        "            return base_weight * boost_if_born\n",
        "        return base_weight\n",
        "\n",
        "\n",
        "    def should_inject_inline_role(self, role_name):\n",
        "        \"\"\"\n",
        "        Decide whether to inject a pre-output inline role (e.g., Refiner, Validator, Executor)\n",
        "        based on its effective meta-weight.\n",
        "\n",
        "        Uses:\n",
        "        - meta_parameters[\"<role>_weight\"]\n",
        "        - meta_parameters[\"inline_role_weight_boost_if_born\"]\n",
        "        \"\"\"\n",
        "        base_key = f\"{role_name.lower()}_weight\"\n",
        "        boost_if_born = self.meta_parameters.get(\"inline_role_weight_boost_if_born\", 1.2)\n",
        "\n",
        "        effective_weight = self.get_effective_meta_weight(role_name, base_key, boost_if_born=boost_if_born)\n",
        "\n",
        "        # Optional threshold (can be dynamic later)\n",
        "        return effective_weight > 0.5\n",
        "\n",
        "\n",
        "    def load_long_memory(self, path=\"long_memory.json\"):\n",
        "        try:\n",
        "            with open(path, \"r\") as f:\n",
        "                raw = json.load(f)\n",
        "\n",
        "            # Extract meta section if present\n",
        "            meta = raw.pop(\"__meta__\", {})\n",
        "\n",
        "            # Restore long memory entries\n",
        "            restored = {}\n",
        "            for k, v in raw.items():\n",
        "                entry = dict(v)\n",
        "                if isinstance(entry.get(\"graph_embedding\"), list):\n",
        "                    entry[\"graph_embedding\"] = np.array(entry[\"graph_embedding\"])\n",
        "                restored[k] = entry\n",
        "            self.long_memory = restored\n",
        "\n",
        "            # Restore output_clusters\n",
        "            self.output_clusters = []\n",
        "            for cluster in meta.get(\"output_clusters\", []):\n",
        "                restored_cluster = {\n",
        "                    \"members\": cluster.get(\"members\", []),\n",
        "                    \"centroid\": np.array(cluster[\"centroid\"]) if isinstance(cluster[\"centroid\"], list) else cluster[\"centroid\"],\n",
        "                    \"prompt_key\": cluster.get(\"prompt_key\"),\n",
        "                    \"task\": cluster.get(\"task\"),\n",
        "                    \"usage_count\": cluster.get(\"usage_count\", 1),\n",
        "                    \"best_output\": cluster.get(\"best_output\"),\n",
        "                    \"best_score\": cluster.get(\"best_score\")\n",
        "                }\n",
        "                self.output_clusters.append(restored_cluster)\n",
        "\n",
        "            print(f\"✅ {self.name} loaded long memory ({len(restored)} entries + {len(self.output_clusters)} output clusters) ← {path}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"⚠️ {self.name} no existing long memory at {path} — starting fresh.\")\n",
        "            self.long_memory = {}\n",
        "            self.output_clusters = []\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ {self.name} failed to load long memory: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def save_long_memory(self, path=\"long_memory.json\"):\n",
        "        try:\n",
        "            # Serialize long memory entries\n",
        "            serializable = {}\n",
        "            for k, v in self.long_memory.items():\n",
        "                entry = dict(v)\n",
        "                if isinstance(entry.get(\"graph_embedding\"), np.ndarray):\n",
        "                    entry[\"graph_embedding\"] = entry[\"graph_embedding\"].tolist()\n",
        "                serializable[k] = entry\n",
        "\n",
        "            # Attach output_clusters to a meta section\n",
        "            if hasattr(self, \"output_clusters\"):\n",
        "                serializable[\"__meta__\"] = {\n",
        "                    \"output_clusters\": [\n",
        "                        {\n",
        "                            \"members\": cluster.get(\"members\", []),\n",
        "                            \"centroid\": cluster[\"centroid\"].tolist()\n",
        "                            if isinstance(cluster[\"centroid\"], np.ndarray)\n",
        "                            else cluster[\"centroid\"],\n",
        "                            \"prompt_key\": cluster.get(\"prompt_key\"),\n",
        "                            \"task\": cluster.get(\"task\"),\n",
        "                            \"usage_count\": cluster.get(\"usage_count\", 1),\n",
        "                            \"best_output\": cluster.get(\"best_output\"),\n",
        "                            \"best_score\": cluster.get(\"best_score\")\n",
        "                        }\n",
        "                        for cluster in self.output_clusters\n",
        "                    ]\n",
        "                }\n",
        "\n",
        "            with open(path, \"w\") as f:\n",
        "                json.dump(serializable, f)\n",
        "            print(f\"✅ {self.name} saved long memory ({len(serializable)} entries) → {path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ {self.name} failed to save long memory: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # =======================\n",
        "    # 7.2. **Model Interaction & Initialization**\n",
        "    # =======================\n",
        "\n",
        "    # ── Physiology helpers ────────────────────────────────────────────────\n",
        "\n",
        "    def _append_phys(self, att, fat, hung):\n",
        "        \"\"\"Convenience: push one sample to the three histories.\"\"\"\n",
        "        self.attention_history.append(att)\n",
        "        self.fatigue_history.append(fat)\n",
        "        self.hunger_history.append(hung)\n",
        "\n",
        "\n",
        "    def seed_first_phys_sample(self):\n",
        "        \"\"\"\n",
        "        Call immediately after (re‑)starting a local round.\n",
        "        • Attention = 1.0 (fresh baseline)\n",
        "        • Fatigue = Hunger = 0.0 (baseline)\n",
        "        \"\"\"\n",
        "        self._append_phys(1.0, 0.0, 0.0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def select_model(self):\n",
        "        \"\"\"\n",
        "        Pick the first available model in priority order, set both\n",
        "        self.model_key, self.model, and self.model_name for API calls.\n",
        "        \"\"\"\n",
        "        priority = [\"gemini\", \"openai\", \"claude\", \"llama\"]\n",
        "        # Map internal keys to the config names\n",
        "        mapping = {\n",
        "            \"gemini\": GEMINI_MODEL_NAME,\n",
        "            \"openai\": OPENAI_MODEL_NAME,\n",
        "            \"claude\": CLAUDE_MODEL_NAME,\n",
        "            \"llama\": LLAMA_MODEL_NAME\n",
        "        }\n",
        "\n",
        "        for key in priority:\n",
        "            if key in self.available_models:\n",
        "                self.model_key   = key\n",
        "                self.model       = self.available_models[key]\n",
        "                self.model_name  = mapping[key]\n",
        "                return key, self.model\n",
        "\n",
        "        # Fallback: use any available\n",
        "        key = next(iter(self.available_models))\n",
        "        self.model_key   = key\n",
        "        self.model       = self.available_models[key]\n",
        "        self.model_name  = mapping.get(key)\n",
        "        return key, self.model\n",
        "\n",
        "    def execute_task(self, task_type, prompt):\n",
        "        \"\"\"\n",
        "        Executes a task using the selected model.\n",
        "        task_type is currently unused but kept for future routing logic.\n",
        "        \"\"\"\n",
        "        model_key, model_obj = self.select_model()\n",
        "\n",
        "        if model_key == \"gemini\":\n",
        "            return model_obj.generate_content(prompt).text\n",
        "        else:  # openai / claude / llama (OpenAI‑compatible clients)\n",
        "            return model_obj.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            ).choices[0].message.content\n",
        "\n",
        "    def initialize_from_prompt(\n",
        "        self,\n",
        "        prompt,\n",
        "        global_round=1,\n",
        "        top_k_roles=3,\n",
        "        top_k_tasks=2,\n",
        "        threshold=0.4,\n",
        "        verbose=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Round-aware initialization from a mission prompt.\n",
        "        - If global_round==1 → brand-new mission:\n",
        "            • Reset global_round (passed in by runner)\n",
        "            • Reset local_round → 1\n",
        "            • Reset interval → 1 (so first act() bumps it to 1)\n",
        "        - If global_round>1 → prompt reuse:\n",
        "            • Keep global_round as-is\n",
        "            • Still reset local_round → 1\n",
        "            • Reset interval → 1\n",
        "        \"\"\"\n",
        "        # ── Reset all per-mission counters & timers ────────────────────────────\n",
        "        self.global_round   = global_round\n",
        "        self.local_round    = 1\n",
        "        self.interval  = 1\n",
        "        self.start_time     = time.time()\n",
        "\n",
        "        if verbose:\n",
        "            tag = \"NEW\" if global_round == 1 else \"REUSE\"\n",
        "            print(f\"{tag} MISSION ({self.name}) global_round={global_round}, local_round=1, interval=1: “{prompt[:60]}…”\")\n",
        "\n",
        "        # ── Now do the usual role‐assignment logic ────────────────────────────\n",
        "        prompt_vec = get_prompt_embedding(prompt)\n",
        "\n",
        "        # (rest of your existing code unchanged…)\n",
        "\n",
        "        prompt_vec = get_prompt_embedding(prompt)\n",
        "\n",
        "        # Use meta-learned threshold if available\n",
        "        threshold = self.meta_parameters.get(\"role_assignment_threshold\", threshold)\n",
        "\n",
        "        similarities = {\n",
        "            role: cosine_similarity([prompt_vec], [vec])[0][0]\n",
        "            for role, vec in ROLE_EMBEDDINGS.items()\n",
        "        }\n",
        "\n",
        "        sorted_roles = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "        new_roles = [role for role, score in sorted_roles if score >= threshold]\n",
        "\n",
        "        if not new_roles:\n",
        "            print(f\"⚠️ {self.name} no strong role match found — using fallback.\")\n",
        "            self.fallback_choose_role_tasks(POSSIBLE_ROLES, POSSIBLE_TASKS, verbose=verbose)\n",
        "            return\n",
        "\n",
        "        # ⏳ Restrict to 1 role in early rounds\n",
        "        if global_round <= 2:\n",
        "            new_roles = [new_roles[0]]\n",
        "\n",
        "        # 🚀 Cap role count starting from round 3 using meta-learned parameter\n",
        "        elif global_round >= 3:\n",
        "            max_roles = self.meta_parameters.get(\"max_roles\", top_k_roles)\n",
        "            new_roles = new_roles[:max_roles]\n",
        "\n",
        "        self.roles = list(dict.fromkeys(new_roles))\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"✅ {self.name} assigned roles: {self.roles}\")\n",
        "\n",
        "        # allow_jollycard = global_round >= 2\n",
        "        # self.assign_tasks_from_roles_multi_round(\n",
        "        #     prompt=prompt,\n",
        "        #     global_round=global_round,\n",
        "        #     top_k=top_k_tasks,\n",
        "        #     threshold=threshold,\n",
        "        #     allow_jollycard=allow_jollycard,\n",
        "        #     verbose=verbose\n",
        "        # )\n",
        "\n",
        "\n",
        "    def initialize_from_memory(self, strategy=\"most_similar\", top_k_roles=3, top_k_tasks=2, threshold=0.4, allow_jollycard=True, verbose=True):\n",
        "        \"\"\"\n",
        "        Initialize roles and tasks from stored long-term memory (e.g., past prompts).\n",
        "        Strategy can be:\n",
        "        - 'most_similar': semantically closest past prompt\n",
        "        - 'last': most recent validated prompt\n",
        "        \"\"\"\n",
        "        if not self.long_memory:\n",
        "            if verbose:\n",
        "                print(f\"⚠️ {self.name} has no long-term memory. Cannot initialize.\")\n",
        "            return\n",
        "\n",
        "        # Select prompt from memory\n",
        "        if strategy == \"last\":\n",
        "            past_prompt = list(self.long_memory.values())[-1].get(\"prompt\", \"\")\n",
        "        elif strategy == \"most_similar\":\n",
        "            keys = list(self.long_memory.keys())\n",
        "            past_prompts = [self.long_memory[k].get(\"prompt\", \"\") for k in keys]\n",
        "\n",
        "            # Compute similarity with the current prompt\n",
        "            query_vec = get_prompt_embedding(self.name + \" role-task intent\")\n",
        "            similarities = {\n",
        "                i: cosine_similarity([query_vec], [get_prompt_embedding(p)])[0][0]\n",
        "                for i, p in enumerate(past_prompts) if p.strip()\n",
        "            }\n",
        "\n",
        "            if not similarities:\n",
        "                if verbose:\n",
        "                    print(f\"⚠️ {self.name} has no valid prompts in memory.\")\n",
        "                return\n",
        "\n",
        "            best_index = max(similarities.items(), key=lambda x: x[1])[0]\n",
        "            past_prompt = past_prompts[best_index]\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n🔁 {self.name} reusing memory-based prompt:\\n\\\"{past_prompt}\\\"\\n\")\n",
        "\n",
        "        # Initialize roles and tasks using the selected prompt\n",
        "        self.assign_roles_from_prompt(past_prompt, top_k=top_k_roles, verbose=verbose)\n",
        "\n",
        "        # self.assign_tasks_from_roles_multi_round(\n",
        "        #     prompt=past_prompt,\n",
        "        #     global_round=1,\n",
        "        #     allow_jollycard=allow_jollycard,\n",
        "        #     verbose=verbose\n",
        "        # )\n",
        "\n",
        "\n",
        "        self.reevaluate_roles(past_prompt, efficiency_threshold=0.55, verbose=verbose)\n",
        "\n",
        "\n",
        "    def cluster_spikes(self, time_window=30.0, var_threshold=0.02):\n",
        "        \"\"\"\n",
        "        Group nearby spikes into clusters based on time proximity and similarity.\n",
        "        Each cluster gets an inferred causal tag.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"signal_spike_log\") or not self.signal_spike_log:\n",
        "            return\n",
        "\n",
        "        from collections import deque\n",
        "        self.signal_clusters = []  # reset every time\n",
        "\n",
        "        spikes = sorted(self.signal_spike_log, key=lambda x: x[\"timestamp\"])\n",
        "        current_cluster = deque()\n",
        "        cluster_start_time = None\n",
        "\n",
        "        for spike in spikes:\n",
        "            ts = spike[\"timestamp\"]\n",
        "            var = spike[\"output_similarity_variance\"]\n",
        "\n",
        "            if not current_cluster:\n",
        "                current_cluster.append(spike)\n",
        "                cluster_start_time = ts\n",
        "                continue\n",
        "\n",
        "            # Check if spike belongs in current cluster\n",
        "            if ts - cluster_start_time <= time_window and var >= var_threshold:\n",
        "                current_cluster.append(spike)\n",
        "            else:\n",
        "                # Finalize current cluster\n",
        "                if len(current_cluster) > 1:\n",
        "                    self.signal_clusters.append(list(current_cluster))\n",
        "                current_cluster = deque([spike])\n",
        "                cluster_start_time = ts\n",
        "\n",
        "        if len(current_cluster) > 1:\n",
        "            self.signal_clusters.append(list(current_cluster))\n",
        "\n",
        "\n",
        "    def is_signal_clear(self, signal, threshold=0.05, margin=0.02, window=3, scenario_sensitive=True, entropy_sensitive=True, verbose=True):\n",
        "        \"\"\"\n",
        "        Determine if a foresight signal is clear and actionable.\n",
        "\n",
        "        Arguments:\n",
        "        - signal: the signal dictionary from signal_spike_log\n",
        "        - threshold: base variance threshold for triggering\n",
        "        - margin: margin above threshold to filter borderline signals\n",
        "        - window: how many recent intervals to check for persistence\n",
        "        - scenario_sensitive: require the scenario to be volatile\n",
        "        - entropy_sensitive: trigger if entropy deviates from expected\n",
        "        - verbose: whether to print details\n",
        "\n",
        "        Returns:\n",
        "        - True if signal is strong and should trigger rerouting\n",
        "        \"\"\"\n",
        "\n",
        "        # ── 1. Magnitude Check ──\n",
        "        val = signal.get(\"output_similarity_variance\", 0.0)\n",
        "        if val < threshold + margin:\n",
        "            return False\n",
        "\n",
        "        # ── 2. Persistence Check ──\n",
        "        recent_vars = self.uvr_similarity_history[-window:]\n",
        "        persistent = [v for v in recent_vars if v > threshold]\n",
        "        if len(persistent) < 2:\n",
        "            return False\n",
        "\n",
        "        # ── 3. Cluster Tag Check ──\n",
        "        tag = signal.get(\"cluster_tag\", \"\")\n",
        "        if tag not in {\"strategy drift\", \"unresolved oscillation\", \"behavioral divergence\", \"strategy collapse\"}:\n",
        "            return False\n",
        "\n",
        "        # ── 4. Scenario Sensitivity ──\n",
        "        if scenario_sensitive:\n",
        "            scenario = signal.get(\"scenario\", \"\")\n",
        "            if scenario not in {\"Black Swan\", \"Tipping Point\", \"Strategy Collapse\"}:\n",
        "                return False\n",
        "\n",
        "        # ── 5. Adaptive Entropy Check ──\n",
        "        if entropy_sensitive:\n",
        "            from collections import Counter\n",
        "            import math\n",
        "\n",
        "            history = self.interval_score_log[-10:]\n",
        "            rc, tc = Counter(), Counter()\n",
        "            for entry in history:\n",
        "                for r in entry.get(\"roles\", []): rc[r] += 1\n",
        "                for t in entry.get(\"tasks\", []): tc[t] += 1\n",
        "\n",
        "            def entropy(c):\n",
        "                total = sum(c.values())\n",
        "                return -sum((v / total) * math.log2(v / total) for v in c.values()) if total > 0 else 0.0\n",
        "\n",
        "            role_entropy = entropy(rc)\n",
        "            task_entropy = entropy(tc)\n",
        "\n",
        "            expected = self.meta_parameters.get(\"entropy_expected\", 0.65)\n",
        "            margin   = self.meta_parameters.get(\"entropy_margin\", 0.15)\n",
        "            soften   = self.meta_parameters.get(\"entropy_softening_factor\", 0.05)\n",
        "            lower, upper = max(expected - margin - soften, 0.0), min(expected + margin + soften, 1.0)\n",
        "\n",
        "            if not (lower <= role_entropy <= upper or lower <= task_entropy <= upper):\n",
        "                if verbose:\n",
        "                    print(f\"⚠️ Entropy trigger → role={role_entropy:.2f}, task={task_entropy:.2f} (expected={expected:.2f})\")\n",
        "            else:\n",
        "                return False\n",
        "\n",
        "        # ── 6. Meta-learned Expected Gain Check ──\n",
        "        if self.meta_parameters.get(\"expected_reroute_gain\", 0.0) < 0.01:\n",
        "            if verbose:\n",
        "                print(f\"⚠️ Reroute gain too low → skipping reroute.\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def reroute_strategy(self, severity=\"moderate\", verbose=True):\n",
        "        \"\"\"\n",
        "        Trigger an adaptive reroute of the agent's strategy.\n",
        "        \"\"\"\n",
        "        if severity == \"mild\":\n",
        "            # Only adjust tasks\n",
        "            self.fallback_choose_tasks(verbose=verbose)\n",
        "\n",
        "        elif severity == \"moderate\":\n",
        "            # Drop low-performing roles and reassign\n",
        "            efficiency_threshold = self.meta_parameters.get(\"reroute_efficiency_threshold\", 0.5)\n",
        "            self.reevaluate_roles(prompt=self.runner.last_mission_prompt, efficiency_threshold=efficiency_threshold, verbose=verbose)\n",
        "            self.assign_tasks_from_roles_multi_round(prompt=self.runner.last_mission_prompt, global_round=self.runner.global_round + 1, verbose=verbose)\n",
        "\n",
        "        elif severity == \"severe\":\n",
        "            # Full reset of roles, tasks, and features\n",
        "            self.roles = []\n",
        "            self.tasks = []\n",
        "            self.features = []\n",
        "            self.short_memory.clear()\n",
        "            self.assign_roles_from_prompt(prompt=self.runner.last_mission_prompt, global_round=self.runner.global_round + 1, verbose=verbose)\n",
        "            self.assign_tasks_from_roles_multi_round(prompt=self.runner.last_mission_prompt, global_round=self.runner.global_round + 1, verbose=verbose)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"🔄 {self.name} performed rerouting ({severity}) → Roles: {self.roles}, Tasks: {self.tasks}\")\n",
        "\n",
        "\n",
        "\n",
        "    # =======================\n",
        "    # 7.3. **Performance Evaluation & Learning**\n",
        "    # =======================\n",
        "\n",
        "\n",
        "    def update_long_memory(\n",
        "        self,\n",
        "        prompt,\n",
        "        output,\n",
        "        score,\n",
        "        signals,\n",
        "        mission_metrics=None,\n",
        "        prompt_key=None,\n",
        "        verbose=False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Update long memory with the current mission if it passes criteria.\n",
        "        Handles formatting, embeddings, signal tracking, and persistence.\n",
        "        \"\"\"\n",
        "        if prompt_key is None:\n",
        "            prompt_key = self.generate_prompt_key(prompt)\n",
        "\n",
        "        entry = {\n",
        "            \"prompt\": prompt,\n",
        "            \"prompt_key\": prompt_key,\n",
        "            \"output\": output,\n",
        "            \"score\": score,\n",
        "            \"metrics\": mission_metrics or {},\n",
        "            \"timestamp\": time.time(),\n",
        "            \"graph_embedding\": (\n",
        "                self.last_graph_embedding.tolist()\n",
        "                if hasattr(self, \"last_graph_embedding\") else None\n",
        "            ),\n",
        "            \"graph_text\": (\n",
        "                self.last_graph_text\n",
        "                if hasattr(self, \"last_graph_text\") else None\n",
        "            ),\n",
        "            \"attention_history\": self.attention_history[-1] if self.attention_history else None,\n",
        "            \"fatigue_history\": self.fatigue_history[-1] if self.fatigue_history else None,\n",
        "            \"hunger_history\": self.hunger_history[-1] if self.hunger_history else None,\n",
        "            \"roles\": self.roles[:],\n",
        "            \"tasks\": self.tasks[:],\n",
        "            \"features\": self.features[:],\n",
        "            \"inline_activated_roles\": self.inline_activated_roles[:],\n",
        "            \"role_task_map\": deepcopy(self.role_task_map),\n",
        "            \"role_metric_map\": deepcopy(self.role_metric_map),\n",
        "            \"uvr_similarity\": signals.get(\"uvr_similarity\"),\n",
        "            \"output_similarity_variance\": signals.get(\"output_similarity_variance\"),\n",
        "        }\n",
        "\n",
        "        self.long_memory[prompt_key] = entry\n",
        "        self.save_long_memory(f\"{self.name}_long_memory.json\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"🧠 {self.name} long memory updated → key: {prompt_key[:12]}... | score={score:.3f}\")\n",
        "\n",
        "\n",
        "    def wants_global_promotion(self, new_score, previous_score):\n",
        "        \"\"\"\n",
        "        Decide if this agent believes the new mission should trigger a global reset.\n",
        "\n",
        "        Arguments:\n",
        "        - new_score: the score of the newly validated output.\n",
        "        - previous_score: the score currently stored in the external DB.\n",
        "\n",
        "        Returns:\n",
        "        - True if the agent thinks the global mission should be promoted.\n",
        "        \"\"\"\n",
        "        threshold_margin = self.meta_parameters.get(\"promotion_margin\", 0.05)\n",
        "        entropy_phase = self.variance_history.get(\"phase\", \"unknown\")\n",
        "        signal_spike = self.signal_spike_log[-1] if self.signal_spike_log else {}\n",
        "\n",
        "        # Condition 1: strong improvement\n",
        "        if new_score > previous_score + threshold_margin:\n",
        "            return True\n",
        "\n",
        "        # Condition 2: clear rerouting signal + validated\n",
        "        if self.is_signal_clear(signal_spike, verbose=False):\n",
        "            return True\n",
        "\n",
        "        # Condition 3: entropy suggests strategy shift\n",
        "        if entropy_phase in (\"inflection\", \"post-inflection\"):\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "\n",
        "    def evaluate_interval(self, score, attention, efficiency, verbose=True):\n",
        "        \"\"\"\n",
        "        Determine whether this interval should be considered validated (Stage 1).\n",
        "        Stage 1 = meaningfulness threshold.\n",
        "        \"\"\"\n",
        "        threshold = self.meta_parameters.get(\"meaningfulness_threshold\", 0.65)\n",
        "        validated = score >= threshold\n",
        "\n",
        "        if validated:\n",
        "            self.local_interval_log.append(self.interval)\n",
        "            self.local_time_log.append(time.time())\n",
        "            self.increment_local_round(reason=\"validated\")\n",
        "            if verbose:\n",
        "                print(f\"✅ {self.name} passed Stage 1 (meaningfulness): score={score:.3f} (threshold={threshold:.3f})\")\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"⏳ {self.name} did not pass Stage 1 (score={score:.3f}, threshold={threshold:.3f})\")\n",
        "\n",
        "        return validated\n",
        "\n",
        "    def evaluate_local_mission(self, score, previous_score, prompt_key, output, signals, mission_metrics=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Determine whether the current output should replace the previous long-memory entry.\n",
        "        Stage 2 = efficiency improvement over last validated mission.\n",
        "        \"\"\"\n",
        "        passed = score > previous_score\n",
        "\n",
        "        if passed:\n",
        "            self.update_long_memory(\n",
        "                prompt=self.runner.last_mission_prompt,\n",
        "                output=output,\n",
        "                score=score,\n",
        "                signals=signals,\n",
        "                mission_metrics=mission_metrics,\n",
        "                prompt_key=prompt_key,\n",
        "                verbose=verbose\n",
        "            )\n",
        "            self.reset_local_state()\n",
        "            self.increment_local_round(reason=\"validated\")\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"✅ {self.name} passed Stage 2 (efficiency). New local mission accepted.\")\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"🔁 {self.name} rejected output (score={score:.3f} ≤ prev={previous_score:.3f})\")\n",
        "\n",
        "        return passed\n",
        "\n",
        "\n",
        "\n",
        "    def increment_local_round(self, reason=None):\n",
        "        \"\"\"\n",
        "        Increments the local round only when explicitly validated.\n",
        "        Use this instead of direct += 1 to avoid accidental increments.\n",
        "        \"\"\"\n",
        "        if reason != \"validated\":\n",
        "            raise RuntimeError(f\"🚫 Attempted to increment local_round without validation (reason='{reason}')\")\n",
        "        self.local_round += 1\n",
        "\n",
        "\n",
        "    def compute_learning_rate(self, efficiency_score, meaningfulness_score):\n",
        "        \"\"\"\n",
        "        Computes a meta-learned learning rate based on efficiency and meaningfulness only.\n",
        "        Attention does not directly affect learning rate (its effect is reflected via efficiency).\n",
        "        \"\"\"\n",
        "\n",
        "        meta = self.meta_parameters\n",
        "        w_eff = meta.get(\"lr_efficiency_weight\", 0.4)\n",
        "        w_mean = meta.get(\"lr_meaningfulness_weight\", 0.4)\n",
        "\n",
        "        lr_decay = meta.get(\"lr_decay_rate\", 0.05)\n",
        "        lr_min = meta.get(\"lr_min\", 0.001)\n",
        "        lr_max = meta.get(\"lr_max\", 0.05)\n",
        "\n",
        "        # Exploration and decay modifiers\n",
        "        exploration_boost = 1.0 / (1.0 + self.local_round)\n",
        "        stability_decay = math.exp(-lr_decay * self.local_round)\n",
        "\n",
        "        # Attention removed: only efficiency + meaningfulness used\n",
        "        base_lr = (\n",
        "            w_eff * efficiency_score +\n",
        "            w_mean * meaningfulness_score\n",
        "        )\n",
        "\n",
        "        learning_rate = base_lr * exploration_boost * stability_decay\n",
        "        return min(max(learning_rate, lr_min), lr_max)\n",
        "\n",
        "\n",
        "    def get_global_round_scenario_mode(local_round_score_log):\n",
        "        \"\"\"\n",
        "        Return the most frequent (mode) scenario label for the current global round.\n",
        "        Normalizes causal suffixes (e.g., “(Causal)”).\n",
        "        \"\"\"\n",
        "        scenarios = []\n",
        "        for interval_scores in local_round_score_log:\n",
        "            for entry in interval_scores:\n",
        "                if \"scenario\" in entry:\n",
        "                    base = normalize_foresight_key(entry[\"scenario\"])\n",
        "                    scenarios.append(base)\n",
        "        if not scenarios:\n",
        "            return \"\"\n",
        "        counter = Counter(scenarios)\n",
        "        mode_scenario, _ = counter.most_common(1)[0]\n",
        "        return mode_scenario\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update_long_term_meta_parameters(\n",
        "        self,\n",
        "        foresight_signal=None,\n",
        "        avg_efficiency_long_term=None,\n",
        "        avg_meaningfulness_long_term=None,\n",
        "        global_avg_attn=None,\n",
        "        global_avg_fatigue=None,\n",
        "        global_avg_hunger=None,\n",
        "        verbose=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Update all meta-parameters based on efficiency, meaningfulness, and attention (local mechanisms).\n",
        "        Update long-term meta-parameters (self.meta_parameters) based on\n",
        "        efficiency, meaningfulness, and attention averaged over the GLOBAL round.\n",
        "        \"\"\"\n",
        "        efficiency_score = avg_efficiency_long_term if avg_efficiency_long_term is not None else 0.5\n",
        "        meaningfulness_score = avg_meaningfulness_long_term if avg_meaningfulness_long_term is not None else 0.5\n",
        "\n",
        "        learning_rate = self.compute_learning_rate(efficiency_score, meaningfulness_score)\n",
        "        self.meta_parameters[\"current_learning_rate\"] = learning_rate\n",
        "\n",
        "        meaningful_and_efficient = meaningfulness_score > 0.6 and efficiency_score > 0.6\n",
        "\n",
        "\n",
        "        if long_term_uvr_volatility > 0.15:  # 🔧 Meta-learn later\n",
        "            self.meta_parameters[\"pruning_threshold\"] = min(\n",
        "                self.meta_parameters.get(\"pruning_threshold\", 0.4) + 0.02, 0.7\n",
        "            )\n",
        "            self.meta_parameters[\"dropout_base_probability\"] = min(\n",
        "                self.meta_parameters.get(\"dropout_base_probability\", 0.3) + 0.01, 0.5\n",
        "            )\n",
        "            self.meta_parameters[\"role_assignment_threshold\"] = max(\n",
        "                self.meta_parameters.get(\"role_assignment_threshold\", 0.7) - 0.01, 0.4\n",
        "            )\n",
        "            if verbose:\n",
        "                print(f\"⚠️ [META] {self.name} sustained UVR volatility — increased dropout/pruning, lowered role threshold\")\n",
        "\n",
        "\n",
        "        # --- Meta-learn UVR detection thresholds ---\n",
        "        current_window = self.meta_parameters.get(\"uvr_min_window\", 5)\n",
        "        current_thresh = self.meta_parameters.get(\"uvr_inflection_threshold\", 1.5)\n",
        "\n",
        "        if meaningful_and_efficient:\n",
        "            # Slightly stabilize (reduce sensitivity to noise)\n",
        "            new_window = min(current_window + 1, 15)         # Cap window\n",
        "            new_thresh = min(current_thresh + 0.05, 2.0)     # Cap threshold\n",
        "        else:\n",
        "            # Be more sensitive to volatility\n",
        "            new_window = max(current_window - 1, 3)          # Floor window\n",
        "            new_thresh = max(current_thresh - 0.05, 1.0)     # Floor threshold\n",
        "\n",
        "        self.meta_parameters[\"uvr_min_window\"] = new_window\n",
        "        self.meta_parameters[\"uvr_inflection_threshold\"] = round(new_thresh, 3)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"📉 {self.name} UVR window → {new_window}, threshold → {new_thresh:.2f}\")\n",
        "\n",
        "        # 🔄 Environmental scenario-based inflection sensitivity\n",
        "        scenario_raw = (foresight_signal or {}).get(\"scenario\", \"\")\n",
        "        scenario = normalize_foresight_key(scenario_raw)\n",
        "\n",
        "        scenario_sensitivity_map = {\n",
        "            \"✅ Convergent Paths\":         (-1, -0.05),\n",
        "            \"🧭 White Swan\":               (-1, -0.03),\n",
        "            \"🌫️ Grey Swan\":                (0, 0.0),\n",
        "            \"🤝 Grey Rhino\":               (+1, +0.02),\n",
        "            \"🪂 Tipping Point\":            (+1, +0.05),\n",
        "            \"🧩 Cascading Discontinuity\":  (+2, +0.08),\n",
        "            \"🐉 Wild Card\":                (+2, +0.10),\n",
        "            \"🕳️ Black Swan\":               (+3, +0.15),\n",
        "        }\n",
        "\n",
        "        delta_w, delta_thresh = scenario_sensitivity_map.get(scenario, (0, 0.0))\n",
        "\n",
        "        # Apply adaptive update\n",
        "        self.meta_parameters[\"uvr_min_window\"] = np.clip(\n",
        "            self.meta_parameters[\"uvr_min_window\"] + delta_w, 3, 15\n",
        "        )\n",
        "        self.meta_parameters[\"uvr_inflection_threshold\"] = round(np.clip(\n",
        "            self.meta_parameters[\"uvr_inflection_threshold\"] + delta_thresh, 1.0, 2.5\n",
        "        ), 3)\n",
        "\n",
        "        if verbose:\n",
        "           print(f\"🌐 {self.name} env-aware UVR tuning: +window={delta_w}, +thresh={delta_thresh:.2f} → \"\n",
        "                  f\"new window={self.meta_parameters['uvr_min_window']}, \"\n",
        "                  f\"new threshold={self.meta_parameters['uvr_inflection_threshold']:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # --- GLOBAL ATTENTION ADAPTATION (insert this block here!) ---\n",
        "        # --- Adapt all attention- and dropout-related weights to global round average ---\n",
        "        if global_avg_attn is not None and global_avg_fatigue is not None and global_avg_hunger is not None:\n",
        "            configs = [\n",
        "                (\"attention_time_weight\",    global_avg_attn,    0.05, 0.3),\n",
        "                (\"attention_fatigue_weight\", global_avg_fatigue, 0.05, 0.3),\n",
        "                (\"attention_hunger_weight\",  global_avg_hunger,  0.05, 0.3),\n",
        "                (\"dropout_time_weight\",      global_avg_attn,    0.05, 0.3),\n",
        "                (\"dropout_fatigue_weight\",   global_avg_fatigue, 0.05, 0.3),\n",
        "                (\"dropout_hunger_weight\",    global_avg_hunger,  0.05, 0.3),\n",
        "                (\"external_attention_weight\", global_avg_attn,    0.05, 0.5),\n",
        "                (\"external_fatigue_weight\",   global_avg_fatigue, 0.05, 0.5),\n",
        "                (\"external_hunger_weight\",    global_avg_hunger,  0.05, 0.5),\n",
        "            ]\n",
        "            for key, avg, min_val, max_val in configs:\n",
        "                current = self.meta_parameters.get(key, min_val)\n",
        "                if avg > 0.6:\n",
        "                    self.meta_parameters[key] = max(current * 0.99, min_val)\n",
        "                else:\n",
        "                    self.meta_parameters[key] = min(current * 1.01, max_val)\n",
        "            if verbose:\n",
        "                print(\n",
        "                    f\"🌐 {self.name} [Long-term META-PHYSIO] Weights adapted:\"\n",
        "                    f\"\\n  attention_time_weight={self.meta_parameters['attention_time_weight']:.3f},\"\n",
        "                    f\" attention_fatigue_weight={self.meta_parameters['attention_fatigue_weight']:.3f},\"\n",
        "                    f\" attention_hunger_weight={self.meta_parameters['attention_hunger_weight']:.3f},\"\n",
        "                    f\"\\n  dropout_time_weight={self.meta_parameters['dropout_time_weight']:.3f},\"\n",
        "                    f\" dropout_fatigue_weight={self.meta_parameters['dropout_fatigue_weight']:.3f},\"\n",
        "                    f\" dropout_hunger_weight={self.meta_parameters['dropout_hunger_weight']:.3f},\"\n",
        "                    f\"\\n  external_attention_weight={self.meta_parameters['external_attention_weight']:.3f},\"\n",
        "                    f\" external_fatigue_weight={self.meta_parameters['external_fatigue_weight']:.3f},\"\n",
        "                    f\" external_hunger_weight={self.meta_parameters['external_hunger_weight']:.3f}\"\n",
        "                )\n",
        "\n",
        "\n",
        "\n",
        "        # --- Learning rate bounds ---\n",
        "        lr_min_range = self.meta_parameters.get(\"lr_min_range\", (0.0005, 0.005))\n",
        "        lr_max_range = self.meta_parameters.get(\"lr_max_range\", (0.02, 0.1))\n",
        "        lr_min = self.meta_parameters.get(\"lr_min\", 0.001)\n",
        "        lr_max = self.meta_parameters.get(\"lr_max\", 0.05)\n",
        "\n",
        "        if meaningful_and_efficient:\n",
        "            new_lr_min = max(lr_min * 0.99, lr_min_range[0])\n",
        "            new_lr_max = max(lr_max * 0.99, new_lr_min + 0.001)\n",
        "        else:\n",
        "            new_lr_min = min(lr_min * 1.01, lr_max - 0.001)\n",
        "            new_lr_max = min(lr_max * 1.01, lr_max_range[1])\n",
        "\n",
        "        self.meta_parameters[\"lr_min\"] = new_lr_min\n",
        "        self.meta_parameters[\"lr_max\"] = new_lr_max\n",
        "\n",
        "        # --- Noise range ---It adapts the range for noise (used in scoring, e.g., for controlled randomness) based on whether the agent had a \"meaningful and efficient\" global round\n",
        "        # The noise block below the learning rate adjusts score_noise_min and score_noise_max dynamically based on recent round performance (i.e., whether the round was meaningful/efficient).\n",
        "        # If the round was good, noise shrinks. If the round was poor, noise grows.\n",
        "        noise_lr = self.meta_parameters.get(\"score_noise_lr\", 0.01)\n",
        "        min_floor = self.meta_parameters.get(\"score_noise_min_floor\", 0.001)\n",
        "        max_ceiling = self.meta_parameters.get(\"score_noise_max_ceiling\", 0.3)\n",
        "        score_min = self.meta_parameters.get(\"score_noise_min\", 0.01)\n",
        "        score_max = self.meta_parameters.get(\"score_noise_max\", 0.2)\n",
        "\n",
        "        if meaningful_and_efficient:\n",
        "            new_min = max(score_min * (1 - noise_lr), min_floor)\n",
        "            new_max = max(score_max * (1 - noise_lr), new_min + 0.01)\n",
        "        else:\n",
        "            new_min = min(score_min * (1 + noise_lr), score_max - 0.01)\n",
        "            new_max = min(score_max * (1 + noise_lr), max_ceiling)\n",
        "\n",
        "        self.meta_parameters[\"score_noise_min\"] = new_min\n",
        "        self.meta_parameters[\"score_noise_max\"] = new_max\n",
        "\n",
        "        # --- Noise learning rate ---\n",
        "        current = self.meta_parameters.get(\"score_noise_lr\", 0.01)\n",
        "        if meaningful_and_efficient:\n",
        "            self.meta_parameters[\"score_noise_lr\"] = max(current * 0.999, 0.0005)\n",
        "        else:\n",
        "            self.meta_parameters[\"score_noise_lr\"] = min(current * 1.001, 0.05)\n",
        "\n",
        "\n",
        "        # --- Role and Pruning thresholds ---\n",
        "        role_min, role_max = self.meta_parameters.get(\"role_threshold_range\", (0.4, 0.9))\n",
        "        pruning_min, pruning_max = self.meta_parameters.get(\"pruning_threshold_range\", (0.2, 0.8))\n",
        "\n",
        "        current_role = self.meta_parameters.get(\"role_assignment_threshold\", 0.7)\n",
        "        current_pruning = self.meta_parameters.get(\"pruning_threshold\", 0.4)\n",
        "\n",
        "        self.meta_parameters[\"role_assignment_threshold\"] = min(max(current_role - learning_rate * (1 - meaningfulness_score), role_min), role_max)\n",
        "        self.meta_parameters[\"pruning_threshold\"] = min(max(current_pruning - learning_rate * (1 - efficiency_score), pruning_min), pruning_max)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"📏 {self.name} role_assignment_threshold → {self.meta_parameters['role_assignment_threshold']:.3f}\")\n",
        "            print(f\"✂️ {self.name} pruning_threshold → {self.meta_parameters['pruning_threshold']:.3f}\")\n",
        "\n",
        "\n",
        "        # --- Threshold: Stagnation Recovery ---\n",
        "        stagnation_thresh_min, stagnation_thresh_max = 3, 10  # Define your min/max bounds here (safe zone)\n",
        "\n",
        "        current_stagnation_threshold = self.meta_parameters.get(\"stagnation_recovery_threshold\", 5)\n",
        "\n",
        "        # Adapt threshold based on low efficiency → slower reaction if generally efficient, faster if inefficient\n",
        "        delta_stagnation = learning_rate * (1.0 - efficiency_score)\n",
        "\n",
        "        self.meta_parameters[\"stagnation_recovery_threshold\"] = min(\n",
        "            max(current_stagnation_threshold - delta_stagnation, stagnation_thresh_min), stagnation_thresh_max\n",
        "        )\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"🛑 {self.name} stagnation_recovery_threshold → {self.meta_parameters['stagnation_recovery_threshold']:.3f}\")\n",
        "\n",
        "\n",
        "        # --- Reinforce role/pruning/reuse thresholds ---\n",
        "        for key, score, floor, ceil in [(\"role_assignment_threshold\", meaningfulness_score, 0.4, 0.9),\n",
        "                                        (\"pruning_threshold\", efficiency_score, 0.2, 0.7),\n",
        "                                        (\"reuse_similarity_threshold\", meaningfulness_score, 0.6, 0.9)]:\n",
        "\n",
        "            current = self.meta_parameters.get(key, (floor + ceil) / 2)\n",
        "            delta = 0.005 if score > 0.6 else -0.005\n",
        "            self.meta_parameters[key] = min(max(current + delta, floor), ceil)\n",
        "\n",
        "        # --- Reuse Similarity Threshold ---\n",
        "        reuse_min, reuse_max = self.meta_parameters.get(\"reuse_similarity_range\", (0.6, 0.9))\n",
        "        current_reuse = self.meta_parameters.get(\"reuse_similarity_threshold\", 0.75)\n",
        "\n",
        "        if meaningfulness_score > 0.6 and efficiency_score > 0.6:\n",
        "            self.meta_parameters[\"reuse_similarity_threshold\"] = min(current_reuse + learning_rate * 0.01, reuse_max)\n",
        "        else:\n",
        "            self.meta_parameters[\"reuse_similarity_threshold\"] = max(current_reuse - learning_rate * 0.01, reuse_min)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"🔄 {self.name} reuse_similarity_threshold → {self.meta_parameters['reuse_similarity_threshold']:.3f}\")\n",
        "\n",
        "\n",
        "        # ── Stagnation tracking via runner ─────────────────────────\n",
        "        if hasattr(self, \"runner\") and self.runner:\n",
        "            # Increment or reset based on latest efficiency & meaningfulness\n",
        "            self.runner.maybe_update_stagnation_counter(\n",
        "                agent=self,\n",
        "                meaningfulness_score=meaningfulness_score,\n",
        "                efficiency_score=efficiency_score\n",
        "            )\n",
        "\n",
        "        # --- Role/pruning learning rates ---\n",
        "        for key, score in [(\"role_threshold_lr\", meaningfulness_score), (\"pruning_threshold_lr\", efficiency_score)]:\n",
        "            bounds = (0.001, 0.05)\n",
        "            current = self.meta_parameters.get(key, 0.01)\n",
        "            if score > 0.6:\n",
        "                updated = max(current * (1 - learning_rate), bounds[0])\n",
        "            else:\n",
        "                updated = min(current * (1 + learning_rate), bounds[1])\n",
        "            self.meta_parameters[key] = updated\n",
        "\n",
        "\n",
        "        # Decay rate adaptation\n",
        "        ddr_min, ddr_max = self.meta_parameters.get(\"dropout_decay_range\", (1.0, 5.0))\n",
        "        current_ddr = self.meta_parameters.get(\"dropout_decay_rate\", 3.0)\n",
        "\n",
        "        if meaningful_and_efficient:\n",
        "            self.meta_parameters[\"dropout_decay_rate\"] = max(current_ddr * 0.99, ddr_min)\n",
        "        else:\n",
        "            self.meta_parameters[\"dropout_decay_rate\"] = min(current_ddr * 1.01, ddr_max)\n",
        "\n",
        "\n",
        "        # --- Decay rates ---\n",
        "        lt_min, lt_max = self.meta_parameters.get(\"lambda_time_range\", (0.1, 1.0))\n",
        "        lu_min, lu_max = self.meta_parameters.get(\"lambda_usage_range\", (0.1, 0.8))\n",
        "        lambda_time = self.meta_parameters.get(\"lambda_time\", 0.5)\n",
        "        lambda_usage = self.meta_parameters.get(\"lambda_usage\", 0.3)\n",
        "\n",
        "        if meaningful_and_efficient:\n",
        "            self.meta_parameters[\"lambda_time\"] = max(lambda_time * 0.99, lt_min)\n",
        "            self.meta_parameters[\"lambda_usage\"] = max(lambda_usage * 0.99, lu_min)\n",
        "        else:\n",
        "            self.meta_parameters[\"lambda_time\"] = min(lambda_time * 1.01, lt_max)\n",
        "            self.meta_parameters[\"lambda_usage\"] = min(lambda_usage * 1.01, lu_max)\n",
        "\n",
        "\n",
        "        # -- Compute foresight scenario influence --Each scenario now supplies a delta for noise as well (negative for “good” scenarios, positive for “bad”—so uncertainty rises in crisis/disruption).\n",
        "        # The numbers in the brackets in your scenario_update_map like represent the direct adjustments (\"deltas\") to the meta-parameters that control:\n",
        "        # - the pruning threshold (first value) (how aggressively you prune roles/tasks)\n",
        "        # - the dropout base probability (second value) (how likely things are dropped out randomly)\n",
        "        # - and the noise (update score_noise_min and score_noise_max)\n",
        "\n",
        "        scenario_raw = (foresight_signal or {}).get(\"scenario\", \"\")\n",
        "        scenario = normalize_foresight_key(scenario_raw)\n",
        "\n",
        "        scenario_update_map = {\n",
        "            \"✅ Convergent Paths\":         (+0.02, +0.02, -0.005),\n",
        "            \"🧭 White Swan\":               (+0.01, +0.01, -0.002),\n",
        "            \"🌫️ Grey Swan\":                ( 0.0,  0.0,  0.0),\n",
        "            \"🤝 Grey Rhino\":               (-0.01, 0.0,  +0.003),\n",
        "            \"🪂 Tipping Point\":            (-0.02, -0.01, +0.005),\n",
        "            \"🧩 Cascading Discontinuity\":  (-0.03, -0.02, +0.008),\n",
        "            \"🐉 Wild Card\":                (-0.04, -0.03, +0.010),\n",
        "            \"🕳️ Black Swan\":               (-0.05, -0.04, +0.015),\n",
        "        }\n",
        "        delta_prune, delta_dropout, delta_noise = scenario_update_map.get(scenario, (0, 0, 0))  # DONE\n",
        "\n",
        "        # 🧠 Output similarity volatility trend (used in UVR diagnostics)\n",
        "\n",
        "        uvr_output_vars = [\n",
        "            x.get(\"output_similarity_variance\", 0.0)\n",
        "            for x in intervals\n",
        "            if \"output_similarity_variance\" in x\n",
        "        ]\n",
        "\n",
        "        avg_output_var = np.mean(uvr_output_vars) if uvr_output_vars else 0.0\n",
        "\n",
        "        output_similarity_volatility = avg_output_var  # Replaces prior long_term_uvr_volatility\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Adjust relevant thresholds:\n",
        "        self.meta_parameters[\"pruning_threshold\"] = np.clip(\n",
        "            self.meta_parameters[\"pruning_threshold\"] + delta_prune, pruning_min, pruning_max)\n",
        "        self.meta_parameters[\"dropout_base_probability\"] = np.clip(\n",
        "            self.meta_parameters[\"dropout_base_probability\"] + delta_dropout, dropout_min, dropout_max)\n",
        "\n",
        "        # Adjust noise min/max (keep within bounds)\n",
        "        score_noise_min_floor = self.meta_parameters.get(\"score_noise_min_floor\", 0.001)\n",
        "        score_noise_max_ceiling = self.meta_parameters.get(\"score_noise_max_ceiling\", 0.3)\n",
        "        new_noise_min = np.clip(self.meta_parameters[\"score_noise_min\"] + delta_noise, score_noise_min_floor, self.meta_parameters[\"score_noise_max\"])\n",
        "        new_noise_max = np.clip(self.meta_parameters[\"score_noise_max\"] + delta_noise, new_noise_min, score_noise_max_ceiling)\n",
        "        self.meta_parameters[\"score_noise_min\"] = new_noise_min\n",
        "        self.meta_parameters[\"score_noise_max\"] = new_noise_max\n",
        "\n",
        "\n",
        "        # --- Cooperation baseline and randomness ---We have added an override in the short-term update method to make the system more reactive.\n",
        "        coop_min, coop_max = self.meta_parameters.get(\"cooperation_randomness_range\", (0.05, 0.3))\n",
        "        if meaningful_and_efficient:\n",
        "            self.meta_parameters[\"cooperation_baseline\"] = min(self.meta_parameters.get(\"cooperation_baseline\", 0.5) + 0.01, 0.8)\n",
        "            self.meta_parameters[\"cooperation_randomness_range\"] = (max(coop_min * 0.98, 0.01), max(coop_max * 0.98, 0.05))\n",
        "        else:\n",
        "            self.meta_parameters[\"cooperation_baseline\"] = max(self.meta_parameters.get(\"cooperation_baseline\", 0.5) - 0.01, 0.2)\n",
        "            self.meta_parameters[\"cooperation_randomness_range\"] = (min(coop_min * 1.02, 0.2), min(coop_max * 1.02, 0.6))\n",
        "\n",
        "        # --- Pruning decay rate ---\n",
        "        prune_min, prune_max = self.meta_parameters.get(\"pruning_decay_range\", (1.0, 5.0))\n",
        "        current_decay = self.meta_parameters.get(\"pruning_decay_rate\", 3.0)\n",
        "\n",
        "        if meaningful_and_efficient:\n",
        "            self.meta_parameters[\"pruning_decay_rate\"] = max(current_decay * 0.99, prune_min)\n",
        "        else:\n",
        "            self.meta_parameters[\"pruning_decay_rate\"] = min(current_decay * 1.01, prune_max)\n",
        "\n",
        "        # --- Attention weighting factors ---These control how much time, fatigue, and hunger contribute to the attention calculation\n",
        "        for key, min_val, max_val in [(\"attention_time_weight\", 0.05, 0.3),\n",
        "                                      (\"attention_fatigue_weight\", 0.05, 0.3),\n",
        "                                      (\"attention_hunger_weight\", 0.05, 0.3)]:\n",
        "            current = self.meta_parameters.get(key, 0.1)\n",
        "            if meaningful_and_efficient:\n",
        "                self.meta_parameters[key] = max(current * 0.99, min_val)\n",
        "            else:\n",
        "                self.meta_parameters[key] = min(current * 1.01, max_val)\n",
        "\n",
        "        # --- GLOBAL-ROUND ADAPTATION FOR ATTENTION/PHYSIOLOGY WEIGHTS ---\n",
        "\n",
        "        if global_avg_attn is not None:\n",
        "            # Attention time weight\n",
        "            attn_time_w = self.meta_parameters.get(\"attention_time_weight\", 0.1)\n",
        "            if global_avg_attn > 0.6:\n",
        "                self.meta_parameters[\"attention_time_weight\"] = max(attn_time_w * 0.99, 0.05)\n",
        "            else:\n",
        "                self.meta_parameters[\"attention_time_weight\"] = min(attn_time_w * 1.01, 0.3)\n",
        "\n",
        "            # Dropout/other weights can be similarly adapted\n",
        "            attn_fatigue_w = self.meta_parameters.get(\"attention_fatigue_weight\", 0.1)\n",
        "            if global_avg_attn > 0.6:\n",
        "                self.meta_parameters[\"attention_fatigue_weight\"] = max(attn_fatigue_w * 0.99, 0.05)\n",
        "            else:\n",
        "                self.meta_parameters[\"attention_fatigue_weight\"] = min(attn_fatigue_w * 1.01, 0.3)\n",
        "\n",
        "            attn_hunger_w = self.meta_parameters.get(\"attention_hunger_weight\", 0.1)\n",
        "            if global_avg_attn > 0.6:\n",
        "                self.meta_parameters[\"attention_hunger_weight\"] = max(attn_hunger_w * 0.99, 0.05)\n",
        "            else:\n",
        "                self.meta_parameters[\"attention_hunger_weight\"] = min(attn_hunger_w * 1.01, 0.3)\n",
        "\n",
        "        # (Repeat/adapt for any other weights you want to globally modulate)\n",
        "\n",
        "\n",
        "        # --- Attention threshold (optional meta‑learning) ---This is the global attention threshold for triggering dropout in agents\n",
        "        thresh_min, thresh_max = 0.1, 0.5\n",
        "        current_thresh = self.meta_parameters.get(\"attention_threshold\", 0.3)\n",
        "\n",
        "        # Raise threshold if attention is chronically low; lower if high\n",
        "        if attention_score < 0.4:\n",
        "            # make it easier to trigger dropout\n",
        "            new_thresh = min(current_thresh + learning_rate * 0.01, thresh_max)\n",
        "        else:\n",
        "            # harder to trigger dropout\n",
        "            new_thresh = max(current_thresh - learning_rate * 0.01, thresh_min)\n",
        "\n",
        "        self.meta_parameters[\"attention_threshold\"] = round(new_thresh, 3)\n",
        "\n",
        "\n",
        "        # --- Dropout weighting factors ---\n",
        "        for key, min_val, max_val in [(\"dropout_time_weight\", 0.05, 0.3),\n",
        "                                      (\"dropout_fatigue_weight\", 0.05, 0.3),\n",
        "                                      (\"dropout_hunger_weight\", 0.05, 0.3)]:\n",
        "            current = self.meta_parameters.get(key, 0.1)\n",
        "            if meaningful_and_efficient:\n",
        "                # productive round → slightly reduce dropout influence\n",
        "                self.meta_parameters[key] = max(current * 0.99, min_val)\n",
        "            else:\n",
        "                # unproductive round → increase dropout influence\n",
        "                self.meta_parameters[key] = min(current * 1.01, max_val)\n",
        "\n",
        "\n",
        "        # --- Task selection parameters ---\n",
        "        min_k, max_k = self.meta_parameters.get(\"top_k_tasks_range\", (1, 5))\n",
        "        min_thresh, max_thresh = self.meta_parameters.get(\"task_threshold_range\", (0.3, 0.7))\n",
        "        current_k = self.meta_parameters.get(\"top_k_tasks\", 2)\n",
        "        current_thresh = self.meta_parameters.get(\"task_threshold\", 0.4)\n",
        "\n",
        "        if meaningful_and_efficient:\n",
        "            self.meta_parameters[\"top_k_tasks\"] = min(current_k + 1, max_k)\n",
        "            self.meta_parameters[\"task_threshold\"] = max(current_thresh - 0.01, min_thresh)\n",
        "        else:\n",
        "            self.meta_parameters[\"top_k_tasks\"] = max(current_k - 1, min_k)\n",
        "            self.meta_parameters[\"task_threshold\"] = min(current_thresh + 0.01, max_thresh)\n",
        "\n",
        "        # --- Max roles ---\n",
        "        min_roles, max_roles = self.meta_parameters.get(\"max_roles_range\", (1, 5))\n",
        "        current_max_roles = self.meta_parameters.get(\"max_roles\", 3)\n",
        "        delta = 1 if meaningful_and_efficient else -1\n",
        "        self.meta_parameters[\"max_roles\"] = min(max(current_max_roles + delta, min_roles), max_roles)\n",
        "\n",
        "        # --- Jollycard importance threshold and sampling temperature ---\n",
        "        importance_min, importance_max = self.meta_parameters.get(\"jollycard_importance_threshold_range\", (0.2, 0.7))\n",
        "        temperature_min, temperature_max = self.meta_parameters.get(\"jollycard_sampling_temperature_range\", (0.5, 2.0))\n",
        "\n",
        "        importance = self.meta_parameters.get(\"jollycard_importance_threshold\", 0.3)\n",
        "        temperature = self.meta_parameters.get(\"jollycard_sampling_temperature\", 1.0)\n",
        "\n",
        "        if meaningful_and_efficient:\n",
        "            self.meta_parameters[\"jollycard_importance_threshold\"] = min(importance + 0.01, importance_max)\n",
        "            self.meta_parameters[\"jollycard_sampling_temperature\"] = max(temperature * 0.98, temperature_min)\n",
        "        else:\n",
        "            self.meta_parameters[\"jollycard_importance_threshold\"] = max(importance - 0.01, importance_min)\n",
        "            self.meta_parameters[\"jollycard_sampling_temperature\"] = min(temperature * 1.02, temperature_max)\n",
        "\n",
        "        # --- Meta-learn jollycard injection weight ---\n",
        "        jollycard_min, jollycard_max = self.meta_parameters.get(\"jollycard_injection_weight_range\", (0.1, 0.9))\n",
        "        current_injection_weight = self.meta_parameters.get(\"jollycard_injection_weight\", 0.5)\n",
        "\n",
        "        if meaningful_and_efficient:\n",
        "            self.meta_parameters[\"jollycard_injection_weight\"] = min(current_injection_weight + 0.01, jollycard_max)\n",
        "        else:\n",
        "            self.meta_parameters[\"jollycard_injection_weight\"] = max(current_injection_weight - 0.01, jollycard_min)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"🎲 {self.name} adjusted jollycard_injection_weight to {self.meta_parameters['jollycard_injection_weight']:.2f}\")\n",
        "\n",
        "\n",
        "        # --- Adapt external call weights (for inline role execution) ---\n",
        "        # These weights adjust reliance on external data/tools during inline role-based tasks\n",
        "\n",
        "        for key, perf in [\n",
        "            (\"external_attention_weight\", efficiency_score),    # can use either or both efficiency/meaningfulness\n",
        "            (\"external_fatigue_weight\", efficiency_score),\n",
        "            (\"external_hunger_weight\", efficiency_score),\n",
        "        ]:\n",
        "            min_v, max_v = 0.05, 0.5\n",
        "            cur = self.meta_parameters.get(key, 0.2)\n",
        "            # If efficient & meaningful, slightly reduce dependency on that resource, else increase it\n",
        "            if meaningful_and_efficient:\n",
        "                self.meta_parameters[key] = max(cur * 0.99, min_v)\n",
        "            else:\n",
        "                self.meta_parameters[key] = min(cur * 1.01, max_v)\n",
        "\n",
        "        # --- Adapt base probability for external tool activation (inline roles) ---\n",
        "        base_prob_min, base_prob_max = 0.1, 0.8\n",
        "        cur_base_prob = self.meta_parameters.get(\"external_call_base_prob\", 0.4)\n",
        "        if meaningful_and_efficient:\n",
        "            self.meta_parameters[\"external_call_base_prob\"] = max(cur_base_prob * 0.99, base_prob_min)\n",
        "        else:\n",
        "            self.meta_parameters[\"external_call_base_prob\"] = min(cur_base_prob * 1.01, base_prob_max)\n",
        "\n",
        "\n",
        "        # --- Metric selection threshold adaptation ---\n",
        "        metric_min, metric_max = self.meta_parameters.get(\"metric_threshold_range\", (0.3, 0.8))\n",
        "        current_thresh = self.meta_parameters.get(\"metric_selection_threshold\", 0.5)\n",
        "        metric_lr = self.meta_parameters.get(\"metric_threshold_lr\", 0.01)\n",
        "\n",
        "        if meaningful_and_efficient:   # or other metric validation signal\n",
        "            self.meta_parameters[\"metric_selection_threshold\"] = max(current_thresh - metric_lr, metric_min)\n",
        "        else:\n",
        "            self.meta_parameters[\"metric_selection_threshold\"] = min(current_thresh + metric_lr, metric_max)\n",
        "\n",
        "        # Optionally adapt learning rate\n",
        "        bounds = (0.001, 0.05)\n",
        "        if meaningful_and_efficient:\n",
        "            self.meta_parameters[\"metric_threshold_lr\"] = max(metric_lr * (1 - learning_rate), bounds[0])\n",
        "        else:\n",
        "            self.meta_parameters[\"metric_threshold_lr\"] = min(metric_lr * (1 + learning_rate), bounds[1])\n",
        "\n",
        "        # --- Meta-learn UVR Reactivation Thresholds ---\n",
        "        delta_thresh = self.meta_parameters.get(\"uvr_reactivation_delta_threshold\", 0.05)\n",
        "        novelty_thresh = self.meta_parameters.get(\"uvr_reactivation_novelty_threshold\", 0.3)\n",
        "\n",
        "        if meaningfulness_score > 0.6 and efficiency_score > 0.6:\n",
        "            # More stable → raise thresholds slightly to reduce false positives\n",
        "            delta_thresh = min(delta_thresh * 1.01, 0.2)\n",
        "            novelty_thresh = min(novelty_thresh * 1.01, 0.6)\n",
        "        else:\n",
        "            # Unstable → lower thresholds to become more reactive\n",
        "            delta_thresh = max(delta_thresh * 0.99, 0.01)\n",
        "            novelty_thresh = max(novelty_thresh * 0.99, 0.1)\n",
        "\n",
        "        self.meta_parameters[\"uvr_reactivation_delta_threshold\"] = round(delta_thresh, 4)\n",
        "        self.meta_parameters[\"uvr_reactivation_novelty_threshold\"] = round(novelty_thresh, 4)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"🔁 {self.name} updated reactivation thresholds → Δ={delta_thresh:.3f}, novelty={novelty_thresh:.3f}\")\n",
        "\n",
        "\n",
        "    def gate_inline_roles(agent, metrics, thresholds, role_metric_map):\n",
        "        \"\"\"\n",
        "        Inline activation of any role triggered by internal metrics.\n",
        "\n",
        "        Arguments:\n",
        "        - agent: the agent object\n",
        "        - metrics: a dictionary of internal metrics (e.g., causal_complexity, etc.)\n",
        "        - thresholds: a dictionary of thresholds for each metric\n",
        "        - role_metric_map: mapping from roles to the metric that should trigger them\n",
        "\n",
        "        Returns:\n",
        "        - List of roles that were activated inline\n",
        "        \"\"\"\n",
        "        inline_activated = []\n",
        "\n",
        "        for role, metric_name in role_metric_map.items():\n",
        "            if role not in agent.roles:\n",
        "                value = metrics.get(metric_name, 0)\n",
        "                threshold = thresholds.get(metric_name, 0.7)  # Default threshold\n",
        "                if value > threshold:\n",
        "                    agent.inline_activate(role)\n",
        "                    inline_activated.append(role)\n",
        "\n",
        "        return inline_activated\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def compute_task_score(self, prompt, task):\n",
        "        \"\"\"\n",
        "        Compute a score for a task based on its relevance to the prompt.\n",
        "        Uses cosine similarity and adds a small keyword‐based bonus.\n",
        "        \"\"\"\n",
        "        # Base relevance via embeddings\n",
        "        prompt_embedding = get_prompt_embedding(prompt)\n",
        "        task_embedding   = get_task_embedding(task)\n",
        "        score = cosine_similarity([prompt_embedding], [task_embedding])[0][0]\n",
        "\n",
        "        # Keyword bonus for domain‐relevant terms\n",
        "        bonus_per_keyword = 0.05\n",
        "        for kw in SCORE_BONUS_KEYWORDS:\n",
        "            if kw in prompt.lower():\n",
        "                score += bonus_per_keyword\n",
        "\n",
        "        # Clamp score to [0, 1]\n",
        "        return max(0.0, min(1.0, score))\n",
        "\n",
        "\n",
        "    def adjust_role_task_priority(self, performance_data):\n",
        "        \"\"\"\n",
        "        Adjust role/task selection priorities based on performance feedback.\n",
        "        Increase priority for tasks/roles that perform well.\n",
        "        \"\"\"\n",
        "        # Example logic: increase priority for tasks with scores above a threshold\n",
        "        for role in self.roles:\n",
        "            role_performance = performance_data.get(role, {})\n",
        "            for task, score in role_performance.items():\n",
        "                if score > 0.7:  # Threshold for high priority, high‑performing task → raise priority\n",
        "                    self.meta_parameters[\"priority_\" + task] = 1\n",
        "                else:       # low‑performing task → lower priority\n",
        "                    self.meta_parameters[\"priority_\" + task] = 0\n",
        "\n",
        "\n",
        "\n",
        "    def fallback_choose_role_tasks(self, possible_roles, possible_tasks, verbose=True):\n",
        "        \"\"\"\n",
        "        Fallback method to assign a random role (if missing) and basic task selection.\n",
        "        Used in exploration or failure recovery cases.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"roles\") or not self.roles:\n",
        "            self.roles = [random.choice(possible_roles)]\n",
        "            if verbose:\n",
        "                print(f\"🎭 {self.name} randomly assigned fallback role: {self.roles[0]}\")\n",
        "\n",
        "        if self.in_cooperation:\n",
        "            if len(possible_tasks) >= 2:\n",
        "                k = min(3, len(possible_tasks))\n",
        "                self.tasks = random.sample(possible_tasks, k=random.randint(2, k))\n",
        "            else:\n",
        "                # only one task available—just take it\n",
        "                self.tasks = possible_tasks[:]\n",
        "            if verbose:\n",
        "                print(f\"🫂 {self.name} (fallback) in cooperation, selected multiple tasks: {self.tasks}\")\n",
        "        else:\n",
        "            self.tasks = [random.choice(possible_tasks)]\n",
        "            if verbose:\n",
        "                print(f\"🧍 {self.name} (fallback) independent, selected single task: {self.tasks[0]}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def score_output(self, output, prompt=None, verbose=False):\n",
        "        \"\"\"\n",
        "        Score an LLM output based on:\n",
        "          - A base score\n",
        "          - Keyword bonus (using SCORE_BONUS_KEYWORDS)\n",
        "          - Semantic similarity to the prompt\n",
        "          - A random noise component\n",
        "        Returns (final_score, details_dict).\n",
        "        \"\"\"\n",
        "        mp = self.meta_parameters\n",
        "\n",
        "        # Core weights\n",
        "        base            = mp.get(\"score_base\", 0.5)\n",
        "        bonus_weight    = mp.get(\"score_bonus_weight\", 0.05)\n",
        "        semantic_weight = mp.get(\"score_semantic_weight\", 0.2)\n",
        "        noise_min       = mp.get(\"score_random_min\", 0.05)\n",
        "        noise_max       = mp.get(\"score_random_max\", 0.15)\n",
        "\n",
        "        # 1) Keyword bonus\n",
        "        bonus = sum(term in output.lower() for term in SCORE_BONUS_KEYWORDS) * bonus_weight\n",
        "\n",
        "        # 2) Semantic similarity\n",
        "        semantic_score = 0.0\n",
        "        if prompt:\n",
        "            try:\n",
        "                vec_out = embedding_model.encode(output)     # OK\n",
        "\n",
        "                # ✅ Ensure vec_out is a numpy array\n",
        "                if vec_out is None:\n",
        "                    vec_out = np.zeros(EMB_DIM)\n",
        "                elif isinstance(vec_out, list):\n",
        "                    vec_out = np.array(vec_out)\n",
        "\n",
        "                vec_prompt = get_prompt_embedding(prompt)  # ✅ Already returns numpy array   # OK\n",
        "                semantic_score = cosine_similarity([vec_out], [vec_prompt])[0][0]\n",
        "\n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"⚠️ Semantic scoring failed: {e}\")\n",
        "                semantic_score = 0.0\n",
        "\n",
        "        # 3) Random noise\n",
        "        noise = random.uniform(noise_min, noise_max)\n",
        "\n",
        "        # 4) Final aggregated score\n",
        "        final_score = min(base + bonus + semantic_weight * semantic_score + noise, 1.0)\n",
        "        final_score = round(final_score, 3)\n",
        "\n",
        "        if verbose:\n",
        "            snippet = (output[:75] + '…') if len(output) > 75 else output\n",
        "            print(f\"📝 Scoring output: \\\"{snippet}\\\"\")\n",
        "            print(f\"   base={base}, bonus={bonus:.3f}, sem*wt={semantic_weight*semantic_score:.3f}, noise={noise:.3f} → final={final_score}\")\n",
        "\n",
        "        # Optionally return details for debugging\n",
        "        details = {\n",
        "            \"base\": base,\n",
        "            \"bonus\": bonus,\n",
        "            \"semantic_score\": semantic_score,\n",
        "            \"semantic_weight\": semantic_weight,\n",
        "            \"noise\": noise\n",
        "        }\n",
        "        return final_score, details\n",
        "\n",
        "\n",
        "\n",
        "    def cluster_output(self, output_text, task=None, prompt_key=None, score=None, verbose=False):\n",
        "        vec = embedding_model.encode(output_text)\n",
        "        if vec is None:\n",
        "            return None\n",
        "\n",
        "        vec = np.array(vec) if isinstance(vec, list) else vec\n",
        "\n",
        "        # Compare with existing clusters\n",
        "        for cluster in self.output_clusters:\n",
        "            sim = cosine_similarity([vec], [cluster[\"centroid\"]])[0][0]\n",
        "            if sim >= 0.85:\n",
        "                cluster[\"members\"].append(output_text)\n",
        "                cluster[\"usage_count\"] += 1\n",
        "\n",
        "                if score is not None:\n",
        "                    if \"best_score\" not in cluster or score > cluster[\"best_score\"]:\n",
        "                        cluster[\"best_score\"] = score\n",
        "                        cluster[\"best_output\"] = output_text\n",
        "\n",
        "                if verbose:\n",
        "                    print(f\"🔁 Output clustered with existing cluster (sim={sim:.2f})\")\n",
        "                return cluster\n",
        "\n",
        "        # Create new cluster\n",
        "        self.output_clusters.append({\n",
        "            \"centroid\": vec,\n",
        "            \"members\": [output_text],\n",
        "            \"prompt_key\": prompt_key,\n",
        "            \"task\": task,\n",
        "            \"usage_count\": 1,\n",
        "            \"best_score\": score,\n",
        "            \"best_output\": output_text\n",
        "        })\n",
        "\n",
        "        if verbose:\n",
        "            print(\"🆕 New output cluster created.\")\n",
        "        return self.output_clusters[-1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_local_round_scenario_mode(interval_score_log):\n",
        "        \"\"\"\n",
        "        Return the most frequent (mode) scenario label for the current local round.\n",
        "        Normalizes causal suffixes (e.g., “(Causal)”).\n",
        "        \"\"\"\n",
        "        scenarios = [normalize_foresight_key(entry[\"scenario\"]) for entry in interval_score_log if \"scenario\" in entry]\n",
        "        if not scenarios:\n",
        "            return \"\"\n",
        "        counter = Counter(scenarios)\n",
        "        mode_scenario, _ = counter.most_common(1)[0]\n",
        "        return mode_scenario\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update_short_term_meta_parameters(\n",
        "        self,\n",
        "        foresight_signal=None,\n",
        "        avg_efficiency_short_term=None,\n",
        "        avg_meaningfulness_short_term=None,\n",
        "        attention_score=None,\n",
        "        verbose=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Update short-term meta-parameters (e.g., cooperation overrides, strategy/tactics fit, dropout prob) at the end of each local round or interval.\n",
        "        Uses adaptive learning rate (same function as long-term), but with local round or interval stats.\n",
        "        Update short-term meta-parameters (e.g., cooperation overrides, strategy/tactics fit, dropout prob) at the end of each local round or interval.\n",
        "        Uses adaptive learning rate (same function as long-term), but with local round or interval stats.\n",
        "        Update short-term meta-parameters (e.g., cooperation overrides, strategy/tactics fit) at the end of each local round or interval.\n",
        "        Uses adaptive learning rate (same function as long-term), but with local round or interval stats.\n",
        "            Uses:\n",
        "                  - foresight_signal/scenario (for direction & scale)\n",
        "                  - avg_efficiency_short_term (average across local round)\n",
        "                  - avg_meaningfulness_short_term (average across local round)\n",
        "                  - dedicated learning rates\n",
        "            Physiology-based adaptation here but weights in long-term meta parameters update method.\n",
        "            Short-term (interval/local round) update for meta-parameters that may require fast adaptation.\n",
        "            This includes a temporary override or nudge for cooperation-related meta-parameters.\n",
        "        \"\"\"\n",
        "        # Use short-term averages or fallback\n",
        "        avg_eff = avg_efficiency_short_term if avg_efficiency_short_term is not None else 0.5\n",
        "        avg_mean = avg_meaningfulness_short_term if avg_meaningfulness_short_term is not None else 0.5\n",
        "        short_term_success = avg_mean > 0.6 and avg_eff > 0.6\n",
        "\n",
        "        # Compute adaptive learning rate (local round/interval scope)\n",
        "        learning_rate = self.compute_learning_rate(avg_eff, avg_mean)\n",
        "\n",
        "        # --- Scenario-driven deltas (for local short-term adaptation) ---\n",
        "\n",
        "        if interval_score_log is not None:\n",
        "            scenario = get_local_round_scenario_mode(interval_score_log)\n",
        "        else:\n",
        "            scenario_raw = (foresight_signal or {}).get(\"scenario\", \"\")\n",
        "            scenario = normalize_foresight_key(scenario_raw)\n",
        "\n",
        "\n",
        "        scenario_update_map = {\n",
        "            \"✅ Convergent Paths\":         (+1.0, +1.0,  +1.0),\n",
        "            \"🧭 White Swan\":               (+0.8, +0.8, +1.0),\n",
        "            \"🌫️ Grey Swan\":                (+0.2, +0.5, +0.6),\n",
        "            \"🤝 Grey Rhino\":               (-0.4, -0.2, +0.6),\n",
        "            \"🪂 Tipping Point\":            (-0.6, -0.8, -0.6),\n",
        "            \"🧩 Cascading Discontinuity\":  (-1.6, -1.6, -0.8),\n",
        "            \"🐉 Wild Card\":                (-1.0, -1.2, -1.0),\n",
        "            \"🕳️ Black Swan\":               (-2.0, -2.0, -1.4),\n",
        "        }\n",
        "        delta_c, delta_f, delta_coop = scenario_update_map.get(scenario, (0.0, 0.0, 0.0))\n",
        "\n",
        "        uvr_similarity = (foresight_signal or {}).get(\"uvr_similarity\", 0.0)\n",
        "\n",
        "        uvr_output_var = (foresight_signal or {}).get(\"output_similarity_variance\", 0.0)\n",
        "\n",
        "\n",
        "        # --- Meta-learn UVR weights (w1–w5) based on signal variance\n",
        "        uvr_weight_keys = [\n",
        "            (\"uvr_weight_prompt\",  uvr_prompt_var),\n",
        "            (\"uvr_weight_output\",  uvr_output_var),\n",
        "            (\"uvr_weight_graph\",   0.0),  # Keep stable unless you add graph var later\n",
        "            (\"uvr_weight_path\",    uvr_path_var),\n",
        "            (\"uvr_weight_physio\",  0.0),  # Physiological UVR variance not tracked (yet)\n",
        "        ]\n",
        "\n",
        "        for key, signal_var in uvr_weight_keys:\n",
        "            current = self.meta_parameters.get(key, 0.2)\n",
        "            delta = learning_rate * signal_var  # ⬅️ grow with instability\n",
        "            new_weight = np.clip(current + delta, 0.05, 0.5)  # keep it bounded\n",
        "            self.meta_parameters[key] = new_weight\n",
        "\n",
        "\n",
        "        # --- UVR Volatility-Based Adjustments ---\n",
        "        uvr_volatility = uvr_prompt_var + uvr_output_var + uvr_path_var\n",
        "\n",
        "        if uvr_volatility > 0.1:  # 🧠 You can meta-learn this threshold later\n",
        "            self.meta_parameters[\"dropout_base_probability\"] = min(\n",
        "                self.meta_parameters.get(\"dropout_base_probability\", 0.3) + 0.01, 0.5\n",
        "            )\n",
        "            self.meta_parameters[\"strategy_fit\"] = max(\n",
        "                self.meta_parameters.get(\"strategy_fit\", 0.5) - 0.05, 0.0\n",
        "            )\n",
        "            self.meta_parameters[\"cooperation_bias\"] = max(\n",
        "                self.meta_parameters.get(\"cooperation_bias\", 0.5) - 0.05, 0.0\n",
        "            )\n",
        "            if verbose:\n",
        "                print(f\"⚠️ {self.name} UVR volatility adjustment triggered — increased dropout, reduced fit/cooperation\")\n",
        "\n",
        "\n",
        "\n",
        "        # --- Retrieve current values & bounds ---\n",
        "        coop_bias = self.meta_parameters.get(\"cooperation_bias\", 0.5)\n",
        "        coop_baseline = self.meta_parameters.get(\"cooperation_baseline\", 0.5)\n",
        "        rng_lo, rng_hi = self.meta_parameters.get(\"cooperation_randomness_range\", (0.05, 0.3))\n",
        "        coop_baseline_min, coop_baseline_max = self.meta_parameters.get(\"cooperation_baseline_range\", (0.2, 0.8))\n",
        "        rng_lo_min, rng_lo_max = 0.01, 0.2\n",
        "        rng_hi_min, rng_hi_max = 0.05, 0.6\n",
        "        coupling = self.meta_parameters.get(\"task_feature_coupling\", 0.5)\n",
        "        fit = self.meta_parameters.get(\"strategy_fit\", 0.5)\n",
        "\n",
        "        # --- Scenario-driven updates for meta-parameters ---\n",
        "        # 1. task_feature_coupling (scenario + local performance)\n",
        "        new_coupling = coupling + learning_rate * delta_c\n",
        "        new_coupling += learning_rate * (avg_eff - 0.5) * 0.2\n",
        "        self.meta_parameters[\"task_feature_coupling\"] = np.clip(new_coupling, 0.0, 1.0)\n",
        "\n",
        "        # 2. strategy_fit (scenario + local performance)\n",
        "        new_fit = fit + learning_rate * delta_f\n",
        "        new_fit += learning_rate * (avg_mean - 0.5) * 0.2\n",
        "        self.meta_parameters[\"strategy_fit\"] = np.clip(new_fit, 0.0, 1.0)\n",
        "\n",
        "        # 3. cooperation_bias (scenario + short-term success)\n",
        "        delta_coop_st = 0.05 if short_term_success else -0.05\n",
        "        coop_bias = np.clip(coop_bias + learning_rate * (delta_coop + delta_coop_st), 0.0, 1.0)\n",
        "        self.meta_parameters[\"cooperation_bias\"] = coop_bias\n",
        "\n",
        "        # 4. cooperation_baseline (scenario + nudge for Convergent Paths)\n",
        "        if scenario.startswith(\"✅ Convergent Paths\"):\n",
        "            coop_baseline = min(coop_baseline + learning_rate * 0.01, coop_baseline_max)\n",
        "        else:\n",
        "            coop_baseline = max(coop_baseline - learning_rate * 0.01, coop_baseline_min)\n",
        "        coop_baseline = np.clip(coop_baseline + learning_rate * delta_coop * 0.005, coop_baseline_min, coop_baseline_max)\n",
        "        self.meta_parameters[\"cooperation_baseline\"] = coop_baseline\n",
        "\n",
        "        # 5. cooperation_randomness_range (scenario nudge)\n",
        "        rng_lo = np.clip(rng_lo + learning_rate * delta_f * 0.01, rng_lo_min, rng_lo_max)\n",
        "        rng_hi = np.clip(rng_hi + learning_rate * delta_f * 0.01, rng_hi_min, rng_hi_max)\n",
        "        self.meta_parameters[\"cooperation_randomness_range\"] = (rng_lo, rng_hi)\n",
        "\n",
        "        # --- Attention-driven Dropout probability (short-term) ---\n",
        "        dropout_min, dropout_max = self.meta_parameters.get(\"dropout_base_prob_range\", (0.1, 0.5))\n",
        "        current_dropout = self.meta_parameters.get(\"dropout_base_probability\", 0.3)\n",
        "        attn = 0.5 if attention_score is None else attention_score\n",
        "\n",
        "        if attn < 0.4:\n",
        "            self.meta_parameters[\"dropout_base_probability\"] = min(current_dropout + 0.005, dropout_max)\n",
        "        else:\n",
        "            self.meta_parameters[\"dropout_base_probability\"] = max(current_dropout - 0.005, dropout_min)\n",
        "\n",
        "        if verbose:\n",
        "            print(\n",
        "                f\"⏩ [Short-term META] \"\n",
        "                f\"task_feature_coupling={self.meta_parameters['task_feature_coupling']:.3f}, \"\n",
        "                f\"strategy_fit={self.meta_parameters['strategy_fit']:.3f}, \"\n",
        "                f\"cooperation_bias={self.meta_parameters['cooperation_bias']:.3f}, \"\n",
        "                f\"cooperation_baseline={self.meta_parameters['cooperation_baseline']:.3f}, \"\n",
        "                f\"cooperation_randomness_range={self.meta_parameters['cooperation_randomness_range']}, \"\n",
        "                f\"dropout_base_probability={self.meta_parameters['dropout_base_probability']:.3f}\"\n",
        "            )\n",
        "            print(\n",
        "                f\"🔧 [META SUMMARY] {self.name} | \"\n",
        "                f\"Coupling={self.meta_parameters['task_feature_coupling']:.3f}, \"\n",
        "                f\"Strategy Fit={self.meta_parameters['strategy_fit']:.3f}\"\n",
        "            )\n",
        "            print(\n",
        "                f\"🔧 [META PARAMS] {self.name} | \"\n",
        "                f\"CoopBias={self.meta_parameters['cooperation_bias']:.3f}, \"\n",
        "                f\"CoopBase={self.meta_parameters['cooperation_baseline']:.3f}, \"\n",
        "                f\"CoopRng={self.meta_parameters['cooperation_randomness_range']}\"\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def retrieve_from_memory(self, prompt, top_k=3):\n",
        "        \"\"\"\n",
        "        Retrieval‐augmented generation: embed the prompt, find the top_k most similar\n",
        "        past memory entries, and concatenate their outputs as context.\n",
        "        \"\"\"\n",
        "        # 1) Embed the current prompt\n",
        "        p_vec = get_prompt_embedding(prompt)\n",
        "        # 2) Compute similarity against your short_memory entries\n",
        "        sims = []\n",
        "        for entry in self.short_memory:\n",
        "            # assume each entry already has an \"embedding\" field\n",
        "            m_vec = entry.get(\"embedding\")\n",
        "            if m_vec is None:\n",
        "                continue\n",
        "            score = cosine_similarity([p_vec], [m_vec])[0][0]\n",
        "            sims.append((score, entry[\"output\"]))\n",
        "        # 3) Sort & take top_k\n",
        "        sims.sort(key=lambda x: x[0], reverse=True)\n",
        "        top_snippets = [out for _, out in sims[:top_k]]\n",
        "        # 4) Build a single block of context\n",
        "        return \"\\n\".join(top_snippets)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # =======================\n",
        "    # 7.4. **Role and Task Assignment**\n",
        "    # =======================\n",
        "\n",
        "    def _compute_dynamic_temperature(self, global_round: int) -> float:\n",
        "        \"\"\"\n",
        "        Start from base temperature, then add a bit each round\n",
        "        so later rounds get more creative.\n",
        "        \"\"\"\n",
        "        base = self.meta_parameters.get(\"jollycard_sampling_temperature\", 1.0)\n",
        "        growth = self.meta_parameters.get(\"temperature_growth_per_round\", 0.1)\n",
        "        # only start growing once we actually inject (round ≥2)\n",
        "        rounds_since = max(0, global_round - 2)\n",
        "        return base + rounds_since * growth\n",
        "\n",
        "    def assign_metrics_from_prompt(self, prompt, top_k=3, verbose=True):\n",
        "        \"\"\"\n",
        "        Assign metrics using embedding similarity, adaptive threshold, top-k, and fallback,\n",
        "        fully symmetric with role/task selection logic.\n",
        "        Current Mechanism: Embedding Similarity + Adaptive Threshold + Top-K:\n",
        "          - Sensitive to prompt phrasing: a slightly different prompt might lead to very different metrics, regardless of prior performance.\n",
        "        With Meta-Weights (each metric also has a meta-learned weight, e.g. stored in self.meta_metric_weights[metric]):\n",
        "          - If meta-weights are updated gradually, the system will tend to keep the best-performing metrics but can still “explore” less-used ones via initial neutral weights and learning rates.\n",
        "\n",
        "        \"\"\"\n",
        "        if not prompt.strip():\n",
        "            # Fallback if prompt is blank or error\n",
        "            default_metric = random.choice(list(METRIC_EMBEDDINGS.keys()))\n",
        "            self.metrics = [default_metric]\n",
        "            if verbose:\n",
        "                print(f\"⚠️ {self.name} fallback metric (blank prompt): {default_metric}\")\n",
        "            return\n",
        "\n",
        "        # Compute prompt embedding and similarities to all metrics\n",
        "        prompt_emb = self.embedding_model.encode(prompt)\n",
        "        similarities = {\n",
        "            metric: float(np.dot(prompt_emb, METRIC_EMBEDDINGS[metric]))\n",
        "            for metric in METRIC_EMBEDDINGS\n",
        "        }\n",
        "\n",
        "        # Use adaptive threshold for metric selection\n",
        "        threshold = self.meta_parameters.get(\"metric_similarity_threshold\", 0.4)\n",
        "        candidates = [(m, sim) for m, sim in similarities.items() if sim >= threshold]\n",
        "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "        selected = [m for m, _ in candidates[:top_k]]\n",
        "\n",
        "        # Fallback: if none above threshold, pick the single best\n",
        "        if not selected:\n",
        "            best = max(similarities, key=similarities.get)\n",
        "            selected = [best]\n",
        "            if verbose:\n",
        "                print(f\"⚠️ {self.name} no metrics passed threshold; fallback to best: {best}\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"🎯 {self.name} assigned metrics from prompt: {selected}\")\n",
        "\n",
        "        self.metrics = selected\n",
        "        if hasattr(self, 'metrics_history'):\n",
        "            self.metrics_history.append(selected)\n",
        "\n",
        "\n",
        "\n",
        "    def assign_roles_from_prompt(self, prompt, top_k=3, global_round=1, verbose=True):\n",
        "        \"\"\"\n",
        "        Assign one or more roles based on semantic similarity to role examples.\n",
        "        If agent.born_roles is non-empty, we seed from those roles as usual.\n",
        "        If born_roles is empty, pick one random role from among the top_k semantically relevant ones.\n",
        "        \"\"\"\n",
        "        # 1) Blank / error\n",
        "        if not prompt.strip() or \"Gemini Error\" in prompt:\n",
        "            self.roles = [random.choice(POSSIBLE_ROLES)]\n",
        "            if verbose:\n",
        "                print(f\"⚠️ {self.name} fallback role (blank/error): {self.roles}\")\n",
        "            return\n",
        "\n",
        "        # 2) Compute prompt embedding + similarities\n",
        "        prompt_vec = get_prompt_embedding(prompt)\n",
        "        sims = {\n",
        "            role: cosine_similarity([prompt_vec], [vec])[0][0]\n",
        "            for role, vec in ROLE_EMBEDDINGS.items()\n",
        "        }\n",
        "\n",
        "        # 3) Dynamic threshold\n",
        "        length = len(prompt.split())\n",
        "        threshold = 0.5 if length <= 6 else 0.65 if length <= 15 else 0.7\n",
        "\n",
        "        # 4) Sort roles by similarity (descending)\n",
        "        sorted_roles = sorted(sims.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # ── DEBUG: show top_k candidates before applying threshold\n",
        "        if verbose:\n",
        "            top_k_candidates = [r for r, _ in sorted_roles[:top_k]]\n",
        "            print(f\"🔍 {self.name} top_{top_k} role candidates (pre‑threshold): {top_k_candidates}\")\n",
        "\n",
        "        # 5) Filter roles by threshold\n",
        "        filtered = [r for r, sc in sorted_roles if sc >= threshold]\n",
        "        if verbose:\n",
        "            print(f\"🔍 {self.name} meaningful roles (score ≥ {threshold}): {filtered}\")\n",
        "\n",
        "        # 6) Fallback if none survived threshold\n",
        "        if not filtered:\n",
        "            best = sorted_roles[0][0]\n",
        "            filtered = [best]\n",
        "            if verbose:\n",
        "                print(f\"⚠️ {self.name} assigning top role despite threshold: {best}\")\n",
        "\n",
        "\n",
        "\n",
        "        # 7) Now choose based on born_roles presence\n",
        "        if self.born_roles:\n",
        "            # Seed from born_roles → pick those in filtered\n",
        "            seeded = [r for r in filtered if r in self.born_roles]\n",
        "            if not seeded:\n",
        "                # if none of the born_roles survived, just use filtered\n",
        "                seeded = filtered\n",
        "            # Cap by learned max_roles after round 2\n",
        "            if global_round >= 3:\n",
        "                max_r = self.meta_parameters.get(\"max_roles\", len(seeded))\n",
        "                self.roles = seeded[:max_r]\n",
        "            else:\n",
        "                self.roles = seeded[:1]\n",
        "            if verbose:\n",
        "                print(f\"✅ {self.name} assigned roles from born_roles: {self.roles}\")\n",
        "\n",
        "        else:\n",
        "            # born_roles empty → random pick among top_k filtered\n",
        "            k = min(top_k, len(filtered))\n",
        "            choice = random.choice(filtered[:k])\n",
        "            self.roles = [choice]\n",
        "            if verbose:\n",
        "                print(f\"🎲 {self.name} randomly assigned role from top_{k}: {self.roles}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def reseed_roles_from_prompt(\n",
        "        self, mission_prompt, global_round=1, top_k=3,\n",
        "        threshold=0.4, allow_fallback=True, verbose=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Reassign roles semantically; respects meta‑learned max_roles.\n",
        "        \"\"\"\n",
        "        self.global_round = global_round\n",
        "\n",
        "        prompt_vec = get_prompt_embedding(mission_prompt)\n",
        "        sims = {role: cosine_similarity([prompt_vec], [vec])[0][0]\n",
        "                for role, vec in ROLE_EMBEDDINGS.items()}\n",
        "        sorted_roles = sorted(sims.items(), key=lambda x: x[1], reverse=True)\n",
        "        filtered = [r for r, s in sorted_roles if s >= threshold]\n",
        "\n",
        "        if not filtered:\n",
        "            if allow_fallback:\n",
        "                print(f\"⚠️ {self.name} reseeding failed — fallback triggered.\")\n",
        "                self.fallback_choose_role_tasks(POSSIBLE_ROLES, POSSIBLE_TASKS, verbose=verbose)\n",
        "            else:\n",
        "                print(f\"❌ No matching roles found for {self.name}.\")\n",
        "            return\n",
        "\n",
        "        if global_round >= 3:\n",
        "            max_roles = self.meta_parameters.get(\"max_roles\", 3)\n",
        "            new_roles = filtered[:max_roles]\n",
        "            if verbose:\n",
        "                print(f\"🎯 {self.name} applying max_roles={max_roles} (round {global_round})\")\n",
        "        else:\n",
        "            new_roles = [filtered[0]]\n",
        "            if verbose:\n",
        "                print(f\"⏳ {self.name} selecting only 1 role (round {global_round})\")\n",
        "\n",
        "        self.roles = list(dict.fromkeys(new_roles))\n",
        "        if verbose:\n",
        "            print(f\"🔄 Roles reseeded for {self.name}: {self.roles}\")\n",
        "\n",
        "        allow_jollycard = global_round >= 2\n",
        "        # self.assign_tasks_from_roles_multi_round(\n",
        "        #    prompt=mission_prompt,\n",
        "        #    global_round=global_round,\n",
        "        #    top_k=2,\n",
        "        #    threshold=threshold,\n",
        "        #    allow_jollycard=allow_jollycard,\n",
        "        #    verbose=verbose\n",
        "        #)\n",
        "\n",
        "\n",
        "\n",
        "    def get_prompt_relevant_tasks(self, prompt, top_k=2, threshold=None, verbose=False):\n",
        "        \"\"\"\n",
        "        Return up to top_k tasks most semantically similar to the prompt.\n",
        "        Uses TASK_EMBEDDINGS and prompt embedding cache.\n",
        "        \"\"\"\n",
        "        threshold = threshold if threshold is not None else self.meta_parameters.get(\"task_similarity_threshold\", 0.4)\n",
        "\n",
        "        prompt_vec = get_prompt_embedding(prompt)\n",
        "        similarities = {\n",
        "            task: cosine_similarity([prompt_vec], [desc_vec])[0][0]\n",
        "            for task, desc_vec in TASK_EMBEDDINGS.items()\n",
        "        }\n",
        "\n",
        "        candidates = [(task, sim) for task, sim in similarities.items() if sim >= threshold]\n",
        "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "        selected = [t for t, _ in candidates[:top_k]]\n",
        "\n",
        "        # Fallback if no task meets threshold\n",
        "        if not selected:\n",
        "            selected = [max(similarities.items(), key=lambda x: x[1])[0]]\n",
        "            if verbose:\n",
        "                print(f\"⚠️ No prompt-relevant task passed threshold; fallback to best match: {selected[0]}\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"🔍 Prompt-relevant tasks selected: {selected}\")\n",
        "\n",
        "        return selected\n",
        "\n",
        "\n",
        "    def assign_tasks_from_roles_multi_round(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        global_round: int,\n",
        "        allow_jollycard: bool = True,\n",
        "        verbose: bool = False,\n",
        "        top_k: Optional[int] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Assign tasks to this agent based on its roles, global_round, and prompt.\n",
        "        1) Rounds 1–2: restrict to single (primary) role.\n",
        "        2) Round ≥3: allow multi-role.\n",
        "        3) Jollycard-task injection (exactly one) guaranteed for round ≥2.\n",
        "        4) Jollycard-role injection (exactly one) guaranteed for round ≥3.\n",
        "        5) Finally, dedupe & cap to max_tasks.\n",
        "        \"\"\"\n",
        "        # ── 1) Role selection ─────────────────────────────────────\n",
        "        if global_round < 3:\n",
        "            primary = self.roles[0] if self.roles else None\n",
        "            roles_to_use = [primary] if primary else []\n",
        "            if verbose:\n",
        "                print(f\"⏳ Restricting to single role (global round {global_round}): {roles_to_use}\")\n",
        "        else:\n",
        "            roles_to_use = list(self.roles)\n",
        "            if verbose:\n",
        "                print(f\"🚀 Multi-role mode (round {global_round}): {roles_to_use}\")\n",
        "\n",
        "        # ── 2) Seed tasks from roles ─────────────────────────────\n",
        "        if roles_to_use:\n",
        "            # Determine how many tasks to take\n",
        "            cap = top_k if top_k is not None else self.meta_parameters.get(\"max_tasks\", 5)\n",
        "            seeded = self.get_task_candidates_for_roles(\n",
        "                roles=roles_to_use,\n",
        "                prompt=prompt,\n",
        "                top_k=cap\n",
        "            )\n",
        "            if verbose:\n",
        "                print(f\"🔍 {self.name} seeded tasks from store (top_{cap}): {seeded}\")\n",
        "            task_list = seeded\n",
        "        else:\n",
        "            task_list = []\n",
        "\n",
        "\n",
        "\n",
        "        # ── 3) Jollycard-task injection (round ≥2) ──────────────────\n",
        "        if allow_jollycard and global_round >= 2:\n",
        "            temp = self._compute_dynamic_temperature(global_round)\n",
        "            wc   = self.sample_jollycard_task(prompt, global_round, temperature=temp)\n",
        "            if wc and wc not in task_list:\n",
        "                task_list.append(wc)\n",
        "                # persist it into the static pool for the primary role\n",
        "                primary = roles_to_use[0]\n",
        "                self.role_task_map.setdefault(primary, []).append(wc)\n",
        "                if verbose:\n",
        "                    print(f\"🎲 {self.name} injected jollycard task '{wc}' (temp={temp}) \"\n",
        "                          f\"into static pool for role '{primary}'\")\n",
        "\n",
        "        # ── 4) Jollycard-role injection (round ≥3) ──────────────────\n",
        "        if allow_jollycard and global_round >= 3:\n",
        "            temp = self._compute_dynamic_temperature(global_round)\n",
        "            wr   = self.sample_jollycard_role(prompt, global_round, temperature=temp)\n",
        "            if wr and wr not in self.born_roles:\n",
        "                self.born_roles.append(wr)\n",
        "                if verbose:\n",
        "                    print(f\"🎲 {self.name} injected jollycard role '{wr}' (temp={temp}) \"\n",
        "                          f\"into born_roles\")\n",
        "\n",
        "        # ── 5) Deduplicate & cap to max_tasks ───────────────────\n",
        "        max_t = self.meta_parameters.get(\"max_tasks\", 5)\n",
        "        # dict.fromkeys preserves order and removes duplicates\n",
        "        self.tasks = list(dict.fromkeys(task_list))[:max_t]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"🧠 {self.name} assigned tasks: {self.tasks}\")\n",
        "\n",
        "\n",
        "\n",
        "    def fallback_choose_tasks(self, possible_tasks=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Fallback task assignment using only current roles, optionally constrained by a task list.\n",
        "        Used when the prompt is blank, invalid, or semantically meaningless.\n",
        "        \"\"\"\n",
        "        selected = []\n",
        "        for role in getattr(self, \"roles\", []):\n",
        "            selected += self.get_task_candidates_for_roles([role], prompt=None, top_k=None)\n",
        "\n",
        "\n",
        "        # Filter by allowed task set if provided\n",
        "        if possible_tasks is not None:\n",
        "            selected = [t for t in selected if t in possible_tasks]\n",
        "\n",
        "        # Deduplicate\n",
        "        self.tasks = list(dict.fromkeys(selected))\n",
        "\n",
        "        # Fallback to a random task if empty\n",
        "        if not self.tasks:\n",
        "            fallback_pool = possible_tasks if possible_tasks else list(TASK_DESCRIPTIONS.keys())\n",
        "            self.tasks = [random.choice(fallback_pool)]\n",
        "            if verbose:\n",
        "                print(f\"⚠️ {self.name} had no tasks after fallback. Randomly assigned: {self.tasks[0]}\")\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"⚙️ {self.name} fallback-assigned tasks: {self.tasks}\")\n",
        "\n",
        "\n",
        "    def get_tasks_for_roles(self, roles=None, verbose=False):\n",
        "        \"\"\"\n",
        "        Return a deduplicated list of tasks associated with the given role(s).\n",
        "        If roles is None, defaults to self.roles. Caps the number of tasks using max_tasks.\n",
        "        \"\"\"\n",
        "        if roles is None:\n",
        "            roles = self.roles\n",
        "\n",
        "        if not roles:\n",
        "            if verbose:\n",
        "                print(f\"⚠️ {self.name} received empty role list. No tasks returned.\")\n",
        "            return []\n",
        "\n",
        "        if not isinstance(roles, list):\n",
        "            roles = [roles]\n",
        "\n",
        "        all_tasks = []\n",
        "        for role in roles:\n",
        "            tasks = self.get_task_candidates_for_roles([role], prompt=None, top_k=None)\n",
        "\n",
        "            all_tasks.extend(tasks)\n",
        "            if verbose:\n",
        "                print(f\"🔍 {self.name} tasks for role '{role}': {tasks if tasks else '[none]'}\")\n",
        "\n",
        "        unique_tasks = list(dict.fromkeys(all_tasks))  # preserves order, removes duplicates\n",
        "\n",
        "        # Cap number of tasks using meta-parameter (default fallback = 5)\n",
        "        max_tasks = self.meta_parameters.get(\"max_tasks\", 5)\n",
        "        capped_tasks = unique_tasks[:max_tasks]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"✅ {self.name} aggregated tasks (capped to {max_tasks}): {capped_tasks}\")\n",
        "\n",
        "        return capped_tasks\n",
        "\n",
        "    def get_task_candidates_for_roles(self, roles, prompt=None, top_k=None):\n",
        "        \"\"\"\n",
        "        Return up to top_k task keys from self.task_store that map to any of the given roles,\n",
        "        sorted by last_score (descending). If prompt is provided, you could also re-rank by\n",
        "        semantic similarity to prompt via TASK_EMBEDDINGS.\n",
        "        \"\"\"\n",
        "        # 1) Filter store entries whose key is in ROLE_TASK_MAP for any of the roles\n",
        "        candidates = [\n",
        "            entry for entry in self.task_store.values()\n",
        "            if any(entry[\"key\"] in self.role_task_map.get(role, []) for role in roles)\n",
        "        ]\n",
        "\n",
        "        # 2) Optionally re-rank by semantic similarity to prompt\n",
        "        if prompt is not None:\n",
        "            p_vec = get_prompt_embedding(prompt)\n",
        "            sims = {\n",
        "                entry[\"key\"]: cosine_similarity([p_vec], [entry[\"embedding\"]])[0][0]\n",
        "                for entry in candidates\n",
        "            }\n",
        "            # merge score + similarity (you can weight these as you like)\n",
        "            for entry in candidates:\n",
        "                entry[\"combined_score\"] = 0.7 * entry[\"last_score\"] + 0.3 * sims[entry[\"key\"]]\n",
        "            candidates.sort(key=lambda e: e[\"combined_score\"], reverse=True)\n",
        "        else:\n",
        "            # 3) Otherwise sort by last_score\n",
        "            candidates.sort(key=lambda e: e[\"last_score\"], reverse=True)\n",
        "\n",
        "        # 4) Cap to top_k (or to self.meta_parameters[\"max_tasks\"])\n",
        "        cap = top_k or self.meta_parameters.get(\"max_tasks\", len(candidates))\n",
        "\n",
        "        return [e[\"key\"] for e in candidates[:cap]]\n",
        "\n",
        "\n",
        "    # =======================\n",
        "    # 7.5. **Role Re-evaluation & Adjustment**\n",
        "    # =======================\n",
        "\n",
        "    def reevaluate_roles(self, prompt, efficiency_threshold=0.55, verbose=True):\n",
        "        \"\"\"\n",
        "        Reevaluates roles based on strategy fit and meta-learned adjustments.\n",
        "        If the agent’s current strategy fit is low, reduce to the most similar role.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"roles\") or not isinstance(self.roles, list):\n",
        "            self.roles = []\n",
        "\n",
        "        # Use the meta-learned strategy fit value instead of a static threshold\n",
        "        current_strategy_fit = self.meta_parameters.get(\"strategy_fit\", 0.5)\n",
        "\n",
        "        if current_strategy_fit < efficiency_threshold and len(self.roles) > 1:\n",
        "            # Use semantic similarity to reevaluate and select the most relevant role\n",
        "            prompt_vec = get_prompt_embedding(prompt)\n",
        "            similarities = {\n",
        "                role: cosine_similarity([prompt_vec], [ROLE_EMBEDDINGS[role]])[0][0]\n",
        "                for role in self.roles\n",
        "            }\n",
        "            # Select the most semantically relevant role based on similarity\n",
        "            self.roles = [max(similarities.items(), key=lambda x: x[1])[0]]\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"⚠️ {self.name} dropped to single role based on strategy fit: {self.roles[0]}\")\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"✅ {self.name} kept roles: {self.roles}\")\n",
        "\n",
        "    from typing import Optional\n",
        "\n",
        "\n",
        "    def sample_jollycard_task(self, prompt: str, global_round: int, temperature: float) -> Optional[str]:\n",
        "        if global_round < 2:\n",
        "            return None\n",
        "\n",
        "        existing = set(self.tasks)\n",
        "        role     = (self.roles[0] if self.roles else \"General\")\n",
        "\n",
        "        llm_prompt = (\n",
        "            f\"As the “{role}” on mission “{prompt}”,\\n\"\n",
        "            \"Please suggest exactly one additional task, compatible with that role,\\n\"\n",
        "            f\"that is NOT already in this list: {list(existing)}.\\n\"\n",
        "            \"Respond in the format:\\n\"\n",
        "            \"  key: <one-word-key>\\n\"\n",
        "            \"  desc: <detailed description>\\n\"\n",
        "            \"Just give me the key and desc lines.\"\n",
        "        )\n",
        "\n",
        "        raw = llm_generate(\n",
        "            model_key=self.model_key,\n",
        "            prompt=llm_prompt,\n",
        "            temperature=temperature\n",
        "        ).strip()\n",
        "\n",
        "        # Parse into key and description\n",
        "        parts = raw.split(\"desc:\")\n",
        "        if len(parts) == 2:\n",
        "            key = parts[0].split(\"key:\")[-1].strip()\n",
        "            desc = parts[1].strip()\n",
        "        else:\n",
        "            # Fallback: assume the first word is key\n",
        "            key = raw.split()[0]\n",
        "            desc = raw\n",
        "\n",
        "        # Only add new tasks to the global store\n",
        "        if key not in self.task_store:\n",
        "            emb = embedding_model.encode(desc)\n",
        "            emb = emb / np.linalg.norm(emb) if np.linalg.norm(emb) > 0 else emb\n",
        "            self.task_store[key] = {\n",
        "                \"key\": key,\n",
        "                \"desc\": desc,\n",
        "                \"embedding\": emb,\n",
        "                \"last_score\": 0.0\n",
        "            }\n",
        "\n",
        "        # Return the task key for assignment\n",
        "        return key\n",
        "\n",
        "\n",
        "    def sample_jollycard_role(self, prompt: str, global_round: int) -> Optional[str]:\n",
        "        if global_round < 3:\n",
        "            return None\n",
        "\n",
        "        existing = set(self.born_roles)\n",
        "        temperature= self._compute_dynamic_temperature(global_round)\n",
        "\n",
        "        llm_prompt = (\n",
        "            f\"As the agent playing roles {self.roles} on mission:\\n\"\n",
        "            f\"  “{prompt}”\\n\"\n",
        "            \"Please suggest exactly ONE additional role, compatible\\n\"\n",
        "            f\"with these, that is NOT already in {list(existing)}.\\n\"\n",
        "            \"Just respond with the role name.\"\n",
        "        )\n",
        "\n",
        "        suggestion = llm_generate(\n",
        "            model_key=self.model_key,\n",
        "            prompt=llm_prompt,\n",
        "            temperature=temperature\n",
        "        ).strip()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return suggestion if suggestion and suggestion not in existing else None\n",
        "\n",
        "\n",
        "\n",
        "    # =======================\n",
        "    # 7.6. Conflict Resolution\n",
        "    # =======================\n",
        "\n",
        "    def detect_and_resolve_role_task_conflict(self, prompt, verbose=True):\n",
        "        \"\"\"\n",
        "        Detect roles whose expected tasks are missing and resolve conflicts\n",
        "        by removing those roles and cleaning up tasks based on meta-learned parameters.\n",
        "        Select jollycard tasks using meta-learned rules (importance threshold, injection weight, temperature).\n",
        "        \"\"\"\n",
        "        conflicts = []\n",
        "\n",
        "        # Check roles whose expected tasks are missing\n",
        "        for role in self.roles:\n",
        "            expected_tasks = self.get_task_candidates_for_roles([role], prompt=None, top_k=None)\n",
        "\n",
        "            if not any(task in self.tasks for task in expected_tasks):\n",
        "                conflicts.append(role)\n",
        "\n",
        "        if conflicts:\n",
        "            if verbose:\n",
        "                print(f\"⚠️ {self.name} detected role-task conflict(s): {conflicts}\")\n",
        "\n",
        "            # Remove conflicting roles\n",
        "            self.roles = [r for r in self.roles if r not in conflicts]\n",
        "\n",
        "\n",
        "            # Clean up tasks unrelated to remaining roles using the dynamic store\n",
        "            valid_tasks = set()\n",
        "            for role in self.roles:\n",
        "                valid_tasks.update(\n",
        "                    self.get_task_candidates_for_roles([role], prompt=None, top_k=None)\n",
        "                )\n",
        "            self.tasks = [t for t in self.tasks if t in valid_tasks]\n",
        "\n",
        "\n",
        "            # Decide whether to inject jollycard based on meta-learned injection weight\n",
        "            inject_prob = self.meta_parameters.get(\"jollycard_injection_weight\", 0.5)\n",
        "\n",
        "            # Jollycards only from global_round >= 2\n",
        "            if hasattr(self, \"global_round\") and self.global_round >= 2:\n",
        "                if random.random() < inject_prob:\n",
        "                    jollycard_pool = [t for t in self.task_store.keys() if t not in valid_tasks]\n",
        "\n",
        "\n",
        "                    if jollycard_pool:\n",
        "                        importance_threshold = self.meta_parameters.get(\"jollycard_importance_threshold\", 0.3)\n",
        "                        sampling_temperature = self.meta_parameters.get(\"jollycard_sampling_temperature\", 1.0)\n",
        "\n",
        "                        scores = []\n",
        "                        prompt_vec = get_prompt_embedding(prompt)\n",
        "\n",
        "                        for task in jollycard_pool:\n",
        "                            task_vec = TASK_EMBEDDINGS.get(task)\n",
        "                            if task_vec is not None:\n",
        "                                sim = cosine_similarity([prompt_vec], [task_vec])[0][0]\n",
        "                            else:\n",
        "                                sim = 0.0\n",
        "                            scores.append((task, sim))\n",
        "\n",
        "                        # Filter by importance threshold\n",
        "                        filtered = [t for t, s in scores if s >= importance_threshold]\n",
        "                        if not filtered:\n",
        "                            filtered = [t for t, _ in scores]  # fallback to all\n",
        "\n",
        "                        # Apply temperature sampling\n",
        "                        weights = [s ** (1.0 / sampling_temperature) for t, s in scores if t in filtered]\n",
        "                        total = sum(weights)\n",
        "                        if total > 0:\n",
        "                            probs = [w / total for w in weights]\n",
        "                            chosen_task = random.choices(filtered, weights=probs, k=1)[0]\n",
        "                        else:\n",
        "                            chosen_task = random.choice(filtered)\n",
        "\n",
        "                        self.tasks.append(chosen_task)\n",
        "\n",
        "                        if verbose:\n",
        "                            print(f\"🎲 {self.name} added jollycard task (meta-learned): {chosen_task}\")\n",
        "                else:\n",
        "                    if verbose:\n",
        "                        print(f\"🚫 {self.name} skipped jollycard injection this time.\")\n",
        "\n",
        "            # Reassign tasks based on updated roles  DONE DON T COMMENT OUT HRE!!!!!\n",
        "            self.assign_tasks_from_roles_multi_round(prompt, global_round=getattr(self, \"global_round\", 1), allow_jollycard=True, verbose=verbose)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"✅ {self.name} resolved role-task conflicts. Roles → {self.roles}, Tasks → {self.tasks}\")\n",
        "\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"✅ {self.name} has no role-task conflicts.\")\n",
        "\n",
        "    # =======================\n",
        "    # 7.7. **Resource Management and Pruning & Dropout**\n",
        "    # =======================\n",
        "\n",
        "    def compute_fatigue(self):\n",
        "        \"\"\"\n",
        "        Compute fatigue as a function of usage count, number of tasks, and elapsed time.\n",
        "        Uses tunable meta-parameters to weight each factor.\n",
        "        Returns a bounded value in [0, 1].\n",
        "        \"\"\"\n",
        "        time_elapsed = time.time() - self.start_time\n",
        "        task_count = len(self.tasks)\n",
        "\n",
        "        w_task = self.meta_parameters.get(\"fatigue_task_weight\", 0.15)\n",
        "        w_time = self.meta_parameters.get(\"fatigue_time_weight\", 0.03)\n",
        "\n",
        "        fatigue_input = self.usage_count + w_task * task_count + w_time * time_elapsed\n",
        "        fatigue_score = sigmoid(fatigue_input)\n",
        "\n",
        "        return round(fatigue_score, 3)\n",
        "\n",
        "    def compute_hunger(self):\n",
        "        \"\"\"\n",
        "        Compute hunger based solely on usage count (internal effort).\n",
        "        Applies a sigmoid transformation to map hunger into [0, 1].\n",
        "        \"\"\"\n",
        "        hunger_weight = self.meta_parameters.get(\"dropout_hunger_weight\", 0.1)\n",
        "\n",
        "        effort = self.usage_count\n",
        "        hunger_score = sigmoid(hunger_weight * effort)\n",
        "\n",
        "        return round(hunger_score, 3)\n",
        "\n",
        "    def compute_attention(self):\n",
        "        \"\"\"\n",
        "        Computes attention as a bounded function inversely affected by usage, time, fatigue, and hunger.\n",
        "        Weights are meta-learned and adjustable.\n",
        "        \"\"\"\n",
        "        time_elapsed = time.time() - self.start_time\n",
        "        fatigue = self.compute_fatigue()\n",
        "        hunger = self.compute_hunger()\n",
        "\n",
        "        w_time = self.meta_parameters.get(\"dropout_time_weight\", 0.05)\n",
        "        w_fatigue = self.meta_parameters.get(\"dropout_fatigue_weight\", 0.2)\n",
        "        w_hunger = self.meta_parameters.get(\"dropout_hunger_weight\", 0.2)\n",
        "\n",
        "        denom = 1.0 + self.usage_count + w_time * time_elapsed + w_fatigue * fatigue + w_hunger * hunger\n",
        "        attention = 1.0 / denom\n",
        "\n",
        "        return round(attention, 3)\n",
        "\n",
        "\n",
        "    def compute_base_dropout_probability(self):\n",
        "        \"\"\"\n",
        "        Computes the base dropout probability using exponential decay based on interval progress.\n",
        "        This serves as a prior for the adaptive dropout mechanism.\n",
        "        \"\"\"\n",
        "        base_prob = self.meta_parameters.get(\"dropout_base_probability\", 0.3)\n",
        "        decay_rate = self.meta_parameters.get(\"dropout_decay_rate\", 3.0)\n",
        "        progress = self.interval / max(1, self.local_round)\n",
        "\n",
        "        return base_prob * math.exp(-decay_rate * progress)\n",
        "\n",
        "    def apply_dropout(self, prompt, verbose=True):\n",
        "        \"\"\"\n",
        "        Purpose: Stochastically (probabilistically) drops roles/tasks to enforce exploration, regularization, or handle “overload” due to low attention/energy.\n",
        "        Apply dropout to roles and tasks based on computed attention, fatigue, hunger\n",
        "        and meta-learned dropout parameters. Roles and tasks may be temporarily\n",
        "        deactivated. Returns (dropped_roles, kept_tasks_by_role).\n",
        "        Apply dropout to roles and tasks based on computed attention, fatigue, hunger and meta-learned dropout parameters.\n",
        "        Roles and tasks may be temporarily deactivated. When base_dropout_prob is sampled against random.random() (as a consequnce of being below the threshold) and\n",
        "        the agent still has at least one role / task that qualifies, dropout is triggered.\n",
        "        There is no quality score for drop‑out – it’s a stochastic off‑switch. At least one such role/task must exist; if the agent is already empty we skip.\n",
        "        Roles are dropped deterministically when attention is too low, otherwise they are dropped probabilistically by adaptive_sample.\n",
        "        Key Factors:\n",
        "        Base dropout probability decays over the local round.\n",
        "        Adaptive sampling: Modifies dropout probability using current attention, fatigue, and hunger through a meta-learned weighted sum, run through a sigmoid, then used as the probability for sampling.\n",
        "        Threshold logic: Roles dropped deterministically if attention is below threshold, otherwise probabilistically.\n",
        "        Each interval, you check (for each role/task): Should I drop this, based on a coin toss using this adjusted probability?\n",
        "        \"\"\"\n",
        "        # 0) Snapshot original state for “static” pruning report\n",
        "        original_roles = self.roles[:]\n",
        "        original_tasks = self.tasks[:]\n",
        "\n",
        "        # 1) Compute current physiology\n",
        "        attention = self.compute_attention()\n",
        "        fatigue   = self.compute_fatigue()\n",
        "        hunger    = self.compute_hunger()\n",
        "\n",
        "        # 2) Base dropout probability & threshold\n",
        "        base_dropout_prob    = self.compute_base_dropout_probability()\n",
        "        attention_threshold  = self.meta_parameters.get(\"attention_threshold\", 0.3)\n",
        "\n",
        "        # 3) Define adaptive sampling helper\n",
        "        def adaptive_sample(base_prob, attention, fatigue, hunger):\n",
        "            meta = self.meta_parameters\n",
        "            w_att = meta.get(\"dropout_attention_weight\", 0.1)\n",
        "            w_fat = meta.get(\"dropout_fatigue_weight\", 0.1)\n",
        "            w_hun = meta.get(\"dropout_hunger_weight\", 0.1)\n",
        "            influence = (- w_att * attention +\n",
        "                        w_fat * fatigue +\n",
        "                        w_hun * hunger)\n",
        "            from math import exp\n",
        "            sigmoid = lambda x: 1 / (1 + exp(-x))\n",
        "            adjusted_prob = base_prob * sigmoid(influence)\n",
        "            return random.random() < adjusted_prob\n",
        "\n",
        "        dropped_roles = []\n",
        "        kept_tasks_by_role = {}\n",
        "\n",
        "        # 4) Summary header (verbose)\n",
        "        if verbose:\n",
        "            print(f\"🔧 [Dropout] {self.name}: \"\n",
        "                  f\"attention={attention:.2f}, fatigue={fatigue:.2f}, hunger={hunger:.2f}\")\n",
        "\n",
        "        # 5) Drop roles\n",
        "        for role in list(self.roles):\n",
        "            if attention < attention_threshold or adaptive_sample(base_dropout_prob, attention, fatigue, hunger):\n",
        "                dropped_roles.append(role)\n",
        "        self.roles = [r for r in self.roles if r not in dropped_roles]\n",
        "\n",
        "        # 6) Drop tasks per remaining role\n",
        "        for role in self.roles:\n",
        "            relevant = self.get_task_candidates_for_roles(\n",
        "                roles=[role],\n",
        "                prompt=prompt,                       # so you can blend last_score + similarity\n",
        "                top_k=self.meta_parameters.get(\"max_tasks\", 5)\n",
        "            )\n",
        "            kept = [t for t in relevant\n",
        "                    if not adaptive_sample(base_dropout_prob, attention, fatigue, hunger)]\n",
        "            if kept:\n",
        "                kept_tasks_by_role[role] = kept\n",
        "\n",
        "        # 7) Aggregate kept tasks\n",
        "        self.tasks = [t for tasks in kept_tasks_by_role.values() for t in tasks]\n",
        "\n",
        "        # 8) Resolve any role↔task conflicts, quietly\n",
        "        self.detect_and_resolve_role_task_conflict(prompt, verbose=False)\n",
        "\n",
        "        # 9) Compute “static” removals\n",
        "        static_removed_roles = [r for r in original_roles if r not in self.roles]\n",
        "        static_removed_tasks = [t for t in original_tasks if t not in self.tasks]\n",
        "\n",
        "        # 10) Unified summary (verbose)\n",
        "        if verbose:\n",
        "            print(f\"🧠 [Dropout Results] {self.name} → dropped roles: {dropped_roles or '—'}, remaining tasks: {self.tasks or '—'}\")\n",
        "\n",
        "\n",
        "        return dropped_roles, kept_tasks_by_role\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def compute_base_pruning_threshold(self):\n",
        "        \"\"\"\n",
        "        Computes the decayed pruning threshold using exponential decay based on interval progress.\n",
        "        Used as a dynamic threshold for pruning low-performing tasks.\n",
        "        \"\"\"\n",
        "        base_threshold = self.meta_parameters.get(\"pruning_threshold\", 0.4)\n",
        "        decay_rate = self.meta_parameters.get(\"pruning_decay_rate\", 3.0)\n",
        "        progress = self.interval / max(1, self.local_round)\n",
        "\n",
        "        return base_threshold * math.exp(-decay_rate * progress)\n",
        "\n",
        "\n",
        "    def compute_metric_score(self, prompt, metric):\n",
        "        prompt_embedding = get_prompt_embedding(prompt)\n",
        "        metric_embedding = get_metric_embedding(metric)\n",
        "        score = cosine_similarity([prompt_embedding], [metric_embedding])[0][0]\n",
        "        return score\n",
        "\n",
        "\n",
        "    def _compute_pruning(self, prompt):\n",
        "        # We prune only tasks (but there is reactivation), not metrics.\n",
        "\n",
        "        threshold = self.compute_base_pruning_threshold()\n",
        "        pruned_roles = []\n",
        "        pruned_tasks = []\n",
        "\n",
        "\n",
        "\n",
        "        # Example: prune any born_role whose task pool all scores below threshold\n",
        "        for role, tasks in self.role_task_map.items():\n",
        "            scores = [ self.compute_task_score(prompt, t) for t in tasks ]\n",
        "            if scores and max(scores) < threshold:\n",
        "                pruned_roles.append(role)\n",
        "\n",
        "        # Example: prune tasks across all roles below threshold\n",
        "        for task in set(t for tasks in self.role_task_map.values() for t in tasks):\n",
        "            score = self.compute_task_score(prompt, task)\n",
        "            if score < threshold:\n",
        "                pruned_tasks.append(task)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return pruned_roles, pruned_tasks\n",
        "\n",
        "\n",
        "    def apply_pruning(self, prompt, verbose=True):\n",
        "        \"\"\"\n",
        "        Purpose: Deterministically (by score) removes least-efficient tasks/roles, but allows for random reactivation (controlled randomness).\n",
        "        Apply dynamic pruning + reactivation, then bulk‐prune via _compute_pruning,\n",
        "        and purge those roles/tasks from the static born_roles and role_task_map.\n",
        "        Logs per‐task removals; only logs a bulk summary if no per‐task logs occurred.\n",
        "        Key Factors:\n",
        "        Base pruning threshold decays over the local round.\n",
        "        Physiology only comes in for reactivation probability, not for the main pruning event.\n",
        "        Main pruning: If task’s average score < threshold, it is a pruning candidate.\n",
        "        Reactivation: Instead of pruning, the task might get a random “second chance,” with probability modulated by current attention, fatigue, and hunger.\n",
        "        You don’t need reactivated_roles explicitly for now.\n",
        "        Reactivated tasks can restore roles implicitly.\n",
        "        Metrics are not pruned.\n",
        "        \"\"\"\n",
        "        reactivated_tasks = []\n",
        "        locally_pruned = []\n",
        "        pruned_tasks    = []\n",
        "        pruned_roles = []\n",
        "        reactivated_roles = []  # Placeholder for symmetry/future use if needed\n",
        "\n",
        "        # ── Not enough history yet? ──────────────────────────────────────────\n",
        "        if len(self.short_memory) < 3:\n",
        "            return []\n",
        "\n",
        "        # ── 1) Compute threshold & physiology ───────────────────────────────\n",
        "        threshold = self.compute_base_pruning_threshold()\n",
        "        attention = self.compute_attention()\n",
        "        fatigue   = self.compute_fatigue()\n",
        "        hunger    = self.compute_hunger()\n",
        "        base_prob = self.meta_parameters[\"pruning_reactivation_prob\"]\n",
        "\n",
        "        # ── 2) Adaptive sampling helper ─────────────────────────────────────\n",
        "        def adaptive_sample(bp, att, fat, hun):\n",
        "            meta      = self.meta_parameters\n",
        "            w_attn    = meta[\"pruning_attention_weight\"]\n",
        "            w_fatigue = meta[\"pruning_fatigue_weight\"]\n",
        "            w_hunger  = meta[\"pruning_hunger_weight\"]\n",
        "            influence = (-w_attn * att) + (w_fatigue * fat) + (w_hunger * hun)\n",
        "            from math import exp\n",
        "            sigmoid = lambda x: 1 / (1 + exp(-x))\n",
        "            adj_prob = bp * sigmoid(influence)\n",
        "            return random.random() < adj_prob, adj_prob\n",
        "\n",
        "        # ── 3) Per‐task pruning logs ────────────────────────────────────────\n",
        "        removed_tasks = []\n",
        "        per_task_logged = False\n",
        "\n",
        "        for task in list(self.tasks):\n",
        "            if self._prune_condition(task):  # or your own should_prune(task) logic\n",
        "                removed_tasks.append(task)\n",
        "                self.tasks.remove(task)\n",
        "                if verbose:\n",
        "                    print(f\"🗑️ [Prune] {self.name} removed task: {task}\")\n",
        "                    per_task_logged = True\n",
        "\n",
        "        # ── 4) Score‐based pruning + reactivation ───────────────────────────\n",
        "        locally_pruned = []\n",
        "        for task in list(self.tasks):\n",
        "            recent = [m for m in self.short_memory if m.get(\"task\") == task]\n",
        "            avg_score = sum(m.get(\"score\", 0) for m in recent[-3:]) / max(1, len(recent[-3:]))\n",
        "            if avg_score < threshold:\n",
        "                reactivate, p = adaptive_sample(base_prob, attention, fatigue, hunger)\n",
        "                if verbose:\n",
        "                    if reactivate:\n",
        "                        print(f\"♻️ {self.name} reactivated '{task}' (p={p:.3f})\")\n",
        "                        reactivated_tasks.append(task)\n",
        "                        # ✅ Restore role if this reactivated task belonged to a pruned role\n",
        "                        for role, role_tasks in ROLE_TASK_MAP.items():\n",
        "                            if task in role_tasks and role not in self.roles and role in self.born_roles:\n",
        "                                self.roles.append(role)\n",
        "                                if verbose:\n",
        "                                    print(f\"🎭 {self.name} re-added role '{role}' due to reactivated task '{task}'\")\n",
        "                    else:\n",
        "                        print(f\"✂️ {self.name} pruned '{task}' (avg={avg_score:.2f}<thr={threshold:.2f})\")\n",
        "                if not reactivate:\n",
        "                    locally_pruned.append(task)\n",
        "\n",
        "\n",
        "\n",
        "        # remove those pruned by score\n",
        "        self.tasks = [t for t in self.tasks if t not in locally_pruned + pruned_tasks]\n",
        "\n",
        "\n",
        "        for key in locally_pruned + pruned_tasks:\n",
        "            self.task_store.pop(key, None)\n",
        "\n",
        "        # resolve any role/task conflicts silently\n",
        "        self.detect_and_resolve_role_task_conflict(prompt, verbose=False)\n",
        "\n",
        "        # ── 5) Bulk pruning hook ────────────────────────────────────────────\n",
        "        pruned_roles, pruned_tasks = self._compute_pruning(prompt)\n",
        "\n",
        "\n",
        "        # Only print a bulk summary if no per‐task logs occurred\n",
        "        if verbose and not per_task_logged and (pruned_roles or pruned_tasks):\n",
        "            print(f\"🗑️ Bulk pruning → roles: {pruned_roles}, tasks: {pruned_tasks}\")\n",
        "\n",
        "        # ── 6) Update static definitions & purge_log ────────────────────────\n",
        "        before_roles = set(self.born_roles)\n",
        "        before_tasks = {r: ts[:] for r, ts in self.role_task_map.items()}\n",
        "        before_metrics = set(self.metrics)\n",
        "\n",
        "        # remove roles\n",
        "        for r in pruned_roles:\n",
        "            if r in self.born_roles:\n",
        "                self.born_roles.remove(r)\n",
        "            self.role_task_map.pop(r, None)\n",
        "\n",
        "        # remove tasks from all roles\n",
        "        for t in pruned_tasks:\n",
        "            for ts in self.role_task_map.values():\n",
        "                if t in ts:\n",
        "                    ts.remove(t)\n",
        "\n",
        "\n",
        "\n",
        "        # record the diff\n",
        "        after_roles = set(self.born_roles)\n",
        "        after_tasks = {r: ts[:] for r, ts in self.role_task_map.items()}\n",
        "        after_metrics = set(self.metrics)\n",
        "\n",
        "\n",
        "        all_before_tasks = set(self.born_tasks)\n",
        "        all_after_tasks  = {t for task_list in after_tasks.values() for t in task_list}\n",
        "\n",
        "        self.purge_log.append({\n",
        "            \"global_round\":      self.runner.global_round + 1,\n",
        "            \"removed_roles\":     sorted(set(self.born_roles) - after_roles),\n",
        "            \"retained_roles\":    sorted(after_roles),\n",
        "            \"removed_tasks\":     sorted(all_before_tasks - all_after_tasks),\n",
        "            \"retained_tasks\":    sorted(all_after_tasks),\n",
        "            \"retained_metrics\":  sorted(after_metrics),\n",
        "            \"reactivated_tasks\": sorted(reactivated_tasks)\n",
        "        })\n",
        "\n",
        "\n",
        "        # ── 7) Final static-purge summary ───────────────────────────────────\n",
        "        if verbose:\n",
        "            removed_r = sorted(before_roles - after_roles)\n",
        "            retained_r = sorted(after_roles)\n",
        "            removed_t = sorted(all_before - all_after)\n",
        "            retained_t = sorted(all_after)\n",
        "            retained_m = sorted(after_metrics)\n",
        "            print(f\"   ↪️ Metrics retained: {retained_m}\")\n",
        "            print(f\"   ↪️ Static roles removed: {removed_r or '–'}\")\n",
        "            print(f\"   ↪️ Static roles retained: {retained_r}\")\n",
        "            print(f\"   ↪️ Static tasks removed: {removed_t or '–'}\")\n",
        "            print(f\"   ↪️ Static tasks retained: {retained_t}\")\n",
        "            print(f\"   ♻️ Reactivated tasks: {reactivated_tasks or '–'}\")\n",
        "\n",
        "        return removed_tasks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # =======================\n",
        "    # 7.8. **Memory & Redundancy Management**\n",
        "    # =======================\n",
        "\n",
        "    def trigger_memory_based_recovery(self, verbose=True):\n",
        "        \"\"\"\n",
        "        Re‑initialize roles and tasks from memory (most‑similar strategy) and\n",
        "        reset the stagnation counter to escape a stagnant state.\n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            print(f\"🧠 {self.name} triggering memory‑based recovery.\")\n",
        "        self.initialize_from_memory(strategy=\"most_similar\", verbose=verbose)\n",
        "        self.stagnation_counter = 0\n",
        "\n",
        "\n",
        "\n",
        "    def is_redundant_content(self, task, content, memory, threshold=0.85):\n",
        "        \"\"\"\n",
        "        Check if content is redundant with past outputs for the same task.\n",
        "        Returns (True, similarity) if redundancy is detected.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            current_vec = embedding_model.encode(content)    # OK\n",
        "\n",
        "            # ✅ Normalize current_vec\n",
        "            if current_vec is None:\n",
        "                current_vec = np.zeros(384)\n",
        "            elif isinstance(current_vec, list):\n",
        "                current_vec = np.array(current_vec)\n",
        "\n",
        "            for entry in memory:\n",
        "                if entry.get(\"task\") == task:\n",
        "                    past_vec = embedding_model.encode(entry.get(\"output\"))   # OK\n",
        "\n",
        "                    # ✅ Normalize past_vec\n",
        "                    if past_vec is None:\n",
        "                        past_vec = np.zeros(384)\n",
        "                    elif isinstance(past_vec, list):\n",
        "                        past_vec = np.array(past_vec)\n",
        "\n",
        "                    similarity = cosine_similarity([current_vec], [past_vec])[0][0]\n",
        "                    if similarity >= threshold:\n",
        "                        print(f\"♻️ {self.name} found redundant output (sim={similarity:.2f})\")\n",
        "                        return True, similarity\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Redundancy check failed: {e}\")\n",
        "            return False, 0.0\n",
        "\n",
        "        return False, 0.0\n",
        "\n",
        "\n",
        "    def has_already_done(self, task, prompt=None, content=None, verbose=False):\n",
        "        \"\"\"\n",
        "        Check if this task/prompt/content pair has already been executed and stored.\n",
        "        Returns True if found in short_memory.\n",
        "        \"\"\"\n",
        "        # 1) By exact prompt match\n",
        "        if prompt:\n",
        "            for item in self.short_memory:\n",
        "                if isinstance(item, dict) and item.get(\"prompt\") == prompt:\n",
        "                    if task in item.get(\"tasks\", []):\n",
        "                        if verbose:\n",
        "                            print(f\"♻️ {self.name} has already done task '{task}' for this prompt.\")\n",
        "                        return True\n",
        "\n",
        "        # 2) By exact content match\n",
        "        if content:\n",
        "            for item in self.short_memory:\n",
        "                if isinstance(item, dict) and item.get(\"output\") == content:\n",
        "                    if task in item.get(\"tasks\", []):\n",
        "                        if verbose:\n",
        "                            print(f\"♻️ {self.name} has already produced this content for task '{task}'.\")\n",
        "                        return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def remember_task_output(self, task, output, prompt=None):\n",
        "        \"\"\"\n",
        "        Store the output of a specific task in short‐term memory,\n",
        "        avoiding duplicates based on prompt or content.\n",
        "        \"\"\"\n",
        "        # Only remember if not already done\n",
        "        if not self.has_already_done(task, prompt=prompt, content=output):\n",
        "            # Score the output for storage\n",
        "            score, _ = self.score_output(output, prompt=prompt)\n",
        "            self.short_memory.append({\n",
        "                \"task\": task,\n",
        "                \"output\": output,\n",
        "                \"prompt\": prompt,\n",
        "                \"prompt_key\": self.generate_prompt_key(prompt) if prompt else None,\n",
        "                \"timestamp\": time.time(),\n",
        "                \"roles\": list(self.roles),\n",
        "                \"features\": list(self.features),\n",
        "                \"score\": score,\n",
        "                \"cooperation\": self.in_cooperation\n",
        "            })\n",
        "\n",
        "    def generate_prompt_key(self, prompt):\n",
        "        \"\"\"\n",
        "        Generate a normalized prompt key for memory lookup.\n",
        "        Used for redundancy filtering and external DB.\n",
        "        \"\"\"\n",
        "        return prompt.strip().lower().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
        "\n",
        "\n",
        "    # =======================\n",
        "    # 7.9. Cooperation & Sharing\n",
        "    # =======================\n",
        "\n",
        "    def maybe_cooperate(self, verbose=True):\n",
        "        \"\"\"\n",
        "        Decide whether to enter cooperation mode based on attention, fatigue, and hunger.\n",
        "        Uses a sigmoid-influenced probability around a meta-learned baseline.\n",
        "        \"\"\"\n",
        "        attention = self.compute_attention()\n",
        "        fatigue = self.compute_fatigue()\n",
        "        hunger = self.compute_hunger()\n",
        "\n",
        "        meta = self.meta_parameters\n",
        "        w_attention = meta.get(\"cooperation_attention_weight\", 0.2)\n",
        "        w_fatigue   = meta.get(\"cooperation_fatigue_weight\", 0.2)\n",
        "        w_hunger    = meta.get(\"cooperation_hunger_weight\", 0.2)\n",
        "\n",
        "        influence = (-w_attention * attention) + (w_fatigue * fatigue) + (w_hunger * hunger)\n",
        "\n",
        "        from math import exp\n",
        "        sigmoid = lambda x: 1 / (1 + exp(-x))\n",
        "\n",
        "        baseline       = meta.get(\"cooperation_baseline\", 0.5)\n",
        "        coop_randomness = random.uniform(*meta.get(\"cooperation_randomness_range\", (0.05, 0.3)))\n",
        "\n",
        "        coop_chance = min(\n",
        "            max(baseline * sigmoid(influence)\n",
        "                + random.uniform(-coop_randomness, coop_randomness),\n",
        "                0.0),\n",
        "            1.0\n",
        "        )\n",
        "        self.in_cooperation = random.random() < coop_chance\n",
        "\n",
        "        if verbose:\n",
        "            state = \"🫂 joined\" if self.in_cooperation else \"🧍 stayed independent\"\n",
        "            print(f\"🔁 {self.name} {state} (coop chance = {coop_chance:.2f})\")\n",
        "\n",
        "\n",
        "\n",
        "    # =======================\n",
        "    # 7.10. Feature Selection & Capabilities\n",
        "    # =======================\n",
        "\n",
        "    def select_features(self, prompt, embedding_model=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Assign features based on semantic similarity between the prompt and cached feature archetype embeddings.\n",
        "        The 'inline_refinement' feature is triggered adaptively based on physiology and meta-learned weights.\n",
        "        \"\"\"\n",
        "        if embedding_model is None:\n",
        "            embedding_model = self.embedding_model  # fallback or global\n",
        "\n",
        "        # --- Compute embedding for the prompt ---\n",
        "        prompt_emb = embedding_model.encode(prompt)\n",
        "        if prompt_emb is None:\n",
        "            prompt_emb = np.zeros(384)  # adjust dimension if necessary\n",
        "\n",
        "        # --- Compute similarities to cached archetype embeddings ---\n",
        "        feature_scores = {}\n",
        "        for feature, ref_emb in FEATURE_EMBEDDINGS.items():\n",
        "            score = np.dot(prompt_emb, ref_emb) / (np.linalg.norm(prompt_emb) * np.linalg.norm(ref_emb) + 1e-8)\n",
        "            feature_scores[feature] = score\n",
        "\n",
        "        # --- Select features above threshold ---\n",
        "        threshold = self.meta_parameters.get(\"task_similarity_threshold\", 0.4)\n",
        "        self.features = [f for f, s in feature_scores.items() if s >= threshold]\n",
        "        self.features = list(set(self.features))  # Deduplicate\n",
        "\n",
        "\n",
        "        # 🎲 Optional: Role-level entropy (exploration of latent roles)\n",
        "        if phase in (\"inflection\", \"post-inflection\"):\n",
        "            latent_roles = [r for r in POSSIBLE_ROLES if r not in self.roles]\n",
        "            if latent_roles:\n",
        "                import random\n",
        "                new_role = random.choice(latent_roles)\n",
        "                self.roles.append(new_role)\n",
        "                if verbose:\n",
        "                    print(f\"🎲 {self.name} exploring latent role: {new_role}\")\n",
        "\n",
        "\n",
        "        # ── STEP 3: Exploration vs Exploitation Tuning ──────────────────────────────\n",
        "        phase = self.variance_history.get(\"phase\", \"stable\")\n",
        "\n",
        "        if phase in (\"inflection\", \"post-inflection\"):\n",
        "            self.meta_parameters[\"task_sampling_temperature\"] = 1.2  # Encourage exploration\n",
        "            self.meta_parameters[\"top_k_tasks\"] = min(\n",
        "                self.meta_parameters.get(\"top_k_tasks\", 3) + 1, 5\n",
        "            )\n",
        "            self.meta_parameters[\"task_threshold\"] = max(\n",
        "                self.meta_parameters.get(\"task_threshold\", 0.4) - 0.05, 0.3\n",
        "            )\n",
        "            if verbose:\n",
        "                print(f\"🔍 {self.name} exploring more — phase={phase}\")\n",
        "        else:\n",
        "            self.meta_parameters[\"task_sampling_temperature\"] = 0.8  # Exploit/Consolidate\n",
        "            self.meta_parameters[\"top_k_tasks\"] = max(\n",
        "                self.meta_parameters.get(\"top_k_tasks\", 3) - 1, 1\n",
        "            )\n",
        "            self.meta_parameters[\"task_threshold\"] = min(\n",
        "                self.meta_parameters.get(\"task_threshold\", 0.4) + 0.05, 0.6\n",
        "            )\n",
        "            if verbose:\n",
        "                print(f\"♻️ {self.name} consolidating (reuse favored) — phase={phase}\")\n",
        "\n",
        "        # 🔁 Optional: Jollycard injection tuning based on phase\n",
        "        if phase in (\"inflection\", \"post-inflection\"):\n",
        "            self.meta_parameters[\"jollycard_sampling_temperature\"] = min(\n",
        "                self.meta_parameters.get(\"jollycard_sampling_temperature\", 1.0) * 1.05, 2.0\n",
        "            )\n",
        "            self.meta_parameters[\"jollycard_injection_weight\"] = min(\n",
        "                self.meta_parameters.get(\"jollycard_injection_weight\", 0.5) + 0.01, 0.9\n",
        "            )\n",
        "        else:\n",
        "            self.meta_parameters[\"jollycard_sampling_temperature\"] = max(\n",
        "                self.meta_parameters.get(\"jollycard_sampling_temperature\", 1.0) * 0.95, 0.5\n",
        "            )\n",
        "            self.meta_parameters[\"jollycard_injection_weight\"] = max(\n",
        "                self.meta_parameters.get(\"jollycard_injection_weight\", 0.5) - 0.01, 0.1\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        # --- Inline refinement logic (formerly 'post_process') ---\n",
        "        attention = self.compute_attention()\n",
        "        fatigue   = self.compute_fatigue()\n",
        "        hunger    = self.compute_hunger()\n",
        "\n",
        "        meta        = self.meta_parameters\n",
        "        w_attention = meta.get(\"external_attention_weight\", 0.2)\n",
        "        w_fatigue   = meta.get(\"external_fatigue_weight\", 0.2)\n",
        "        w_hunger    = meta.get(\"external_hunger_weight\", 0.2)\n",
        "        base_prob   = meta.get(\"external_call_base_prob\", 0.4)\n",
        "\n",
        "        influence = (-w_attention * attention +\n",
        "                      w_fatigue * fatigue +\n",
        "                      w_hunger * hunger)\n",
        "\n",
        "        from math import exp\n",
        "        sigmoid = lambda x: 1 / (1 + exp(-x))\n",
        "        adjusted_prob = base_prob * sigmoid(influence)\n",
        "\n",
        "        import random\n",
        "        if random.random() < adjusted_prob:\n",
        "            self.features.append(\"inline_refinement\")\n",
        "            self.external_access_count += 1\n",
        "            if verbose:\n",
        "                print(f\"🔌 {self.name} selected 'inline_refinement' feature (p={adjusted_prob:.3f})\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"🧩 {self.name} selected features: {self.features}\")\n",
        "            if hasattr(self, 'relevant_capabilities') and self.relevant_capabilities:\n",
        "                print(f\"🧠 Relevant capabilities activated: {self.relevant_capabilities}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def should_use_web_scraping(self, prompt, verbose=True):\n",
        "        \"\"\"\n",
        "        Agent-level decision: Should I perform web scraping this round?\n",
        "        Returns True if scraping is enabled AND 'web_scraping' is in self.features.\n",
        "        \"\"\"\n",
        "        # Global on/off switch for web scraping\n",
        "        if not getattr(self, \"enable_web_scraping\", True):\n",
        "            decision = False\n",
        "        else:\n",
        "            decision = \"web_scraping\" in getattr(self, \"features\", [])\n",
        "\n",
        "        if verbose:\n",
        "            state = \"✅ will use\" if decision else \"🚫 will skip\"\n",
        "            print(f\"🌐 {self.name} {state} web scraping for prompt: \\\"{prompt[:60]}...\\\"\")\n",
        "\n",
        "        return decision\n",
        "\n",
        "    def perform_web_scraping(self, prompt, verbose=True):\n",
        "        \"\"\"\n",
        "        Full web-scraping pipeline: generate search query, fetch & clean snippets,\n",
        "        summarize, rank, and return a concise Web Context block.\n",
        "        \"\"\"\n",
        "        # 1) Generate search keywords\n",
        "        keywords = make_search_query(prompt)\n",
        "        query    = \" \".join(keywords)\n",
        "        if verbose:\n",
        "            print(f\"🔍 Searching web for: {query}\")\n",
        "\n",
        "        # 2) Fetch & cache top-k results\n",
        "        if not hasattr(self, \"web_cache\"):\n",
        "            self.web_cache = {}\n",
        "        if query in self.web_cache:\n",
        "            hits = self.web_cache[query]\n",
        "        else:\n",
        "            hits = search_api.search(query, top_k=3)\n",
        "            self.web_cache[query] = hits\n",
        "\n",
        "        # 3) Extract, clean & summarize each snippet\n",
        "        contexts = []\n",
        "        for url, snippet in hits:\n",
        "            cleaned = fetch_and_clean(url)\n",
        "            summary = llm_generate(\n",
        "                model_key=self.model_key,\n",
        "                prompt=f\"Summarize in 2 sentences: {cleaned[:500]}\",\n",
        "                temperature=0.2\n",
        "            ).strip()\n",
        "            contexts.append((url, summary))\n",
        "\n",
        "        # 4) Select top-2 by keyword overlap\n",
        "        ranked = sorted(\n",
        "            contexts,\n",
        "            key=lambda us: sum(1 for kw in keywords if kw in us[1].lower()),\n",
        "            reverse=True\n",
        "        )[:2]\n",
        "\n",
        "        # 5) Build Web Context block\n",
        "        block = \"\\n\".join(f\"[Context from {url}]: {summary}\" for url, summary in ranked)\n",
        "        return block\n",
        "\n",
        "\n",
        "    def perform_data_query(self, prompt):\n",
        "        # e.g. parse a SQL or CSV source\n",
        "        return \"…data table or stats…\"\n",
        "\n",
        "    def extract_entities(self, text):\n",
        "        # e.g. spaCy or simple regex NER\n",
        "        return [\"Entity1\", \"Entity2\"]\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        # e.g. call a sentiment model\n",
        "        return {\"polarity\": 0.2, \"subjectivity\": 0.5}\n",
        "\n",
        "    def execute_code_snippet(self, code):\n",
        "        # safely run code in a sandbox and capture stdout\n",
        "        return \"…code output…\"\n",
        "\n",
        "    def summarize_text(self, text):\n",
        "        # call LLM or local summarizer\n",
        "        return llm_generate(self.model_key, f\"Summarize: {text}\", temperature=0.2)\n",
        "\n",
        "    def validate_output(self, text):\n",
        "        # fact-check via LLM or rules\n",
        "        return llm_generate(self.model_key, f\"Fact-check this: {text}\", temperature=0.2)\n",
        "\n",
        "\n",
        "\n",
        "    # =======================\n",
        "    # 7.11. Execution & Action\n",
        "    # =======================\n",
        "\n",
        "    def act(self, mission_prompt, verbose=True):\n",
        "        \"\"\"\n",
        "        End-to-end agent execution:\n",
        "          1. Feature selection & optional web scraping\n",
        "          2. Dynamic model selection\n",
        "          3. Model invocation\n",
        "          4. Output scoring & memory update\n",
        "          5. Meta-parameter updates & round advancement\n",
        "        Returns: (output, score, attention, fatigue, hunger, _, meta_parameters, status)\n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            print(f\"🔁 ACT method triggered for {self.name}\")\n",
        "        self.usage_count += 1\n",
        "\n",
        "        # ── Inject planning hints based on scenario-cluster map ──\n",
        "        if hasattr(self.runner, \"scenario_cluster_map\") and scene in self.runner.scenario_cluster_map:\n",
        "            cluster_hints = self.runner.scenario_cluster_map[scene]\n",
        "            if verbose:\n",
        "                print(f\"🧠 Injected planning hint → scenario '{scene}' linked to clusters: {cluster_hints}\")\n",
        "            self.meta_parameters[\"planning_hints\"] = cluster_hints\n",
        "        else:\n",
        "            self.meta_parameters[\"planning_hints\"] = []\n",
        "\n",
        "        # 1) Feature selection\n",
        "        self.select_features(mission_prompt)\n",
        "\n",
        "        # 2) Web scraping if enabled\n",
        "        scraped_info = None\n",
        "        if self.should_use_web_scraping(mission_prompt, verbose=verbose):\n",
        "            if verbose:\n",
        "                print(f\"🌍 {self.name} is performing enhanced web scraping…\")\n",
        "            web_ctx = self.perform_web_scraping(mission_prompt, verbose)\n",
        "            mission_prompt += f\"\\n\\n[Web Context]:\\n{web_ctx}\"\n",
        "            if verbose:\n",
        "                print(f\"🌐 Context added:\\n{web_ctx[:200]}…\\n\")\n",
        "\n",
        "\n",
        "        # ── PASTE PRE-PROCESSING HERE ───────────────────────────────────\n",
        "        # 2.1) Data query\n",
        "        if \"data_query\" in self.features:\n",
        "            data_ctx = self.perform_data_query(mission_prompt)\n",
        "            if verbose:\n",
        "                print(f\"📈 Data context:\\n{data_ctx}\")\n",
        "            mission_prompt += f\"\\n\\n[Data Context]: {data_ctx}\"\n",
        "\n",
        "        # 2.2) Entity extraction\n",
        "        if \"entity_extraction\" in self.features:\n",
        "            entities = self.extract_entities(mission_prompt)\n",
        "            if verbose:\n",
        "                print(f\"🏷️ Extracted entities: {entities}\")\n",
        "            mission_prompt += f\"\\n\\n[Entities]: {entities}\"\n",
        "\n",
        "        # 2.3) Sentiment analysis (pre-tagging)\n",
        "        if \"sentiment_analysis\" in self.features:\n",
        "            sentiment = self.analyze_sentiment(mission_prompt)\n",
        "            if verbose:\n",
        "                print(f\"😊 Sentiment score: {sentiment}\")\n",
        "            mission_prompt += f\"\\n\\n[Sentiment]: {sentiment}\"\n",
        "\n",
        "        # ── END PRE-PROCESSING ─────────────────────────────────────────\n",
        "\n",
        "        # ── PASTE RETRIEVAL INJECTION HERE ─────────────────────────────────\n",
        "        # 2.x) Retrieval‐augmented context\n",
        "        if \"retrieval\" in self.features:\n",
        "            mem_ctx = self.retrieve_from_memory(mission_prompt, top_k=3)\n",
        "            if verbose:\n",
        "                print(f\"🗄️ {self.name} retrieved memory context:\\n{mem_ctx}\\n\")\n",
        "            mission_prompt += f\"\\n\\n[Memory Context]:\\n{mem_ctx}\"\n",
        "        # ── END RETRIEVAL ────────────────────────────────────────────────\n",
        "\n",
        "        # ── END PRE-PROCESSING ─────────────────────────────────────────\n",
        "\n",
        "\n",
        "        # 3) Dynamic model selection\n",
        "        model_key, self.model = self.select_model()\n",
        "        if verbose:\n",
        "            print(f\"🔄 {self.name} selected model → {model_key}\")\n",
        "\n",
        "\n",
        "        # 4) Core LLM call with inline role injection\n",
        "        try:\n",
        "            # Inject inline roles (pre-output shaping)\n",
        "            shaped_prompt = self.inject_inline_roles_into_prompt(mission_prompt)\n",
        "\n",
        "            if model_key == \"gemini\":\n",
        "                raw_output = self.model.generate_content(shaped_prompt).text\n",
        "            else:\n",
        "                raw_output = (\n",
        "                    self.model.chat.completions.create(\n",
        "                        model=self.model_name,\n",
        "                        messages=[{\"role\": \"user\", \"content\": shaped_prompt}]\n",
        "                    )\n",
        "                    .choices[0]\n",
        "                    .message\n",
        "                    .content\n",
        "                )\n",
        "\n",
        "            self.last_raw_output = raw_output\n",
        "            output = raw_output.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle rate limits by retrying once\n",
        "            if \"429\" in str(e):\n",
        "                if verbose:\n",
        "                    print(\"⚠️ Rate limit, retrying…\")\n",
        "                time.sleep(3)\n",
        "                try:\n",
        "                    shaped_prompt = self.inject_inline_roles_into_prompt(mission_prompt)\n",
        "                    if model_key == \"gemini\":\n",
        "                        raw_output = self.model.generate_content(shaped_prompt).text\n",
        "                    else:\n",
        "                        raw_output = (\n",
        "                            self.model.chat.completions.create(\n",
        "                                model=self.model_name,\n",
        "                                messages=[{\"role\": \"user\", \"content\": shaped_prompt}]\n",
        "                            )\n",
        "                            .choices[0]\n",
        "                            .message\n",
        "                            .content\n",
        "                        )\n",
        "                    self.last_raw_output = raw_output\n",
        "                    output = raw_output.strip()\n",
        "                except Exception as e2:\n",
        "                    output = f\"⚠️ Error after retry: {e2}\"\n",
        "            else:\n",
        "                output = f\"⚠️ Model Error: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "        # 6) Compute internal metrics\n",
        "        attention = self.compute_attention()\n",
        "        fatigue   = self.compute_fatigue()\n",
        "        hunger    = self.compute_hunger()\n",
        "\n",
        "        # append once\n",
        "        self.attention_history.append(attention)\n",
        "        self.fatigue_history.append(fatigue)\n",
        "        self.hunger_history.append(hunger)\n",
        "\n",
        "\n",
        "\n",
        "        # 7) Score & efficiency\n",
        "        score, _ = self.score_output(output, prompt=mission_prompt, verbose=verbose)\n",
        "        efficiency_score = round(\n",
        "            score * attention /\n",
        "            (1 + 0.1*self.interval +\n",
        "                 0.5*self.meaningless_output_counter +\n",
        "                 0.3*self.external_access_count),\n",
        "            3\n",
        "        )\n",
        "\n",
        "        if verbose:\n",
        "            print(\n",
        "                f\"🎯 {self.name} act() → \"\n",
        "                f\"Score={score}, Efficiency={efficiency_score:.3f}, \"\n",
        "                f\"attention={attention:.3f}, fatigue={fatigue:.3f}, hunger={hunger:.3f}\"\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        for task_key in self.tasks:\n",
        "            if task_key in self.task_store:\n",
        "                self.task_store[task_key][\"last_score\"] = score\n",
        "\n",
        "\n",
        "\n",
        "        # 7.1) Stagnation tracking (increment on low performance, reset otherwise)\n",
        "        if score < 0.5 and efficiency_score < 0.5:\n",
        "            self.stagnation_counter += 1\n",
        "            if verbose:\n",
        "                print(f\"⚠️ {self.name} stagnation_counter → {self.stagnation_counter}\")\n",
        "        else:\n",
        "            if self.stagnation_counter > 0 and verbose:\n",
        "                print(f\"✅ {self.name} recovered from stagnation.\")\n",
        "            self.stagnation_counter = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # ── STEP 1: Injection Control ────────────────────────────────\n",
        "        allow_injection = False\n",
        "        phase = getattr(self, \"variance_history\", {}).get(\"phase\", \"stable\")\n",
        "        reactivation = getattr(self, \"variance_history\", {}).get(\"reactivation_flag\", False)\n",
        "\n",
        "        if phase in (\"inflection\", \"post-inflection\") or reactivation:\n",
        "            allow_injection = True\n",
        "\n",
        "        if not allow_injection:\n",
        "            print(f\"🚫 {self.name} skipping short memory injection — phase={phase}, reactivation={reactivation}\")\n",
        "            return  # 🚫 Block the rest of this method\n",
        "        else:\n",
        "            print(f\"✅ {self.name} injection allowed — phase={phase}, reactivation={reactivation}\")\n",
        "\n",
        "        # 7.2) Memory updates\n",
        "        entry = {\n",
        "            \"prompt\": mission_prompt,\n",
        "            \"prompt_key\": self.generate_prompt_key(mission_prompt),\n",
        "            \"output\": output,\n",
        "            \"score\": score,\n",
        "            \"embedding\": get_prompt_embedding(mission_prompt),\n",
        "            \"features\": list(self.features),\n",
        "            \"roles\": list(self.roles),\n",
        "            \"cooperation\": self.in_cooperation,\n",
        "            \"timestamp\": time.time()\n",
        "        }\n",
        "\n",
        "\n",
        "        self.short_memory.append(entry)\n",
        "        if \"web_scraping\" in self.features and scraped_info:\n",
        "            self.short_memory.append({\n",
        "                **entry,\n",
        "                \"task\": \"web_scraping\",\n",
        "                \"source\": \"web\",\n",
        "                \"output\": scraped_info\n",
        "            })\n",
        "        self.last_output = output\n",
        "\n",
        "        # 8) Meta-learning & housekeeping\n",
        "        self.update_meta_parameters(attention, fatigue, hunger)\n",
        "        self.interval += 1\n",
        "        for task in self.tasks:\n",
        "            self.remember_task_output(task, output, prompt=mission_prompt)\n",
        "\n",
        "\n",
        "\n",
        "        # (Placeholder meaningfulness = 1.0 for now)\n",
        "        self.update_meta_parameters(\n",
        "            meaningfulness_score=1.0,\n",
        "            efficiency_score=efficiency_score,\n",
        "            attention_score=attention,\n",
        "            verbose=verbose\n",
        "        )\n",
        "        self.local_round += 1\n",
        "\n",
        "        return output, score, attention, fatigue, hunger, \"\", dict(self.meta_parameters), \"generated\"\n",
        "\n",
        "    # === Dispatcher already present ===\n",
        "    def perform_inline_module(self, role, output):\n",
        "        if role == \"Grapher\":\n",
        "            return self.insert_causal_graph(output)\n",
        "        elif role == \"Statisticien\":\n",
        "            return self.enhance_statistical_inference(output)\n",
        "        elif role == \"Twin_Digitalizer\":\n",
        "            return self.generate_digital_twin_annotation(output)\n",
        "        elif role == \"Simulator\":\n",
        "            return self.append_simulation_block(output)\n",
        "        elif role == \"Refiner\":\n",
        "            return self.inline_refine_output(output)\n",
        "        elif role == \"Validator\":\n",
        "            return self.inline_validate_output(output)\n",
        "        elif role == \"Executor\":\n",
        "            return self.symbolic_execute_plan(output)\n",
        "        elif role == \"Analyst\":\n",
        "            return self.enrich_with_contextual_analysis(output)\n",
        "        elif role == \"Strategist\":\n",
        "            return self.add_high_level_strategy(output)\n",
        "        elif role == \"Builder\":\n",
        "            return self.sketch_solution_architecture(output)\n",
        "        elif role == \"Scout\":\n",
        "            return self.add_exploration_paths(output)\n",
        "        else:\n",
        "            return output  # fallback\n",
        "\n",
        "\n",
        "    def add_exploration_paths(self, output):\n",
        "        return output + \"\\n\\n🧭 Exploration Paths: [Alternative directions or domains suggested]\"\n",
        "\n",
        "    def sketch_solution_architecture(self, output):\n",
        "        return output + \"\\n\\n🏗 Architecture: [Functional building blocks proposed]\"\n",
        "\n",
        "    def add_high_level_strategy(self, output):\n",
        "        return output + \"\\n\\n♟️ Strategic Layer: [Long-term goal alignment and tradeoffs considered]\"\n",
        "\n",
        "    def enrich_with_contextual_analysis(self, output):\n",
        "        return output + \"\\n\\n🔍 Contextual Insight: [Environmental, temporal, or geopolitical context]\"\n",
        "\n",
        "    def symbolic_execute_plan(self, output):\n",
        "        return output + \"\\n\\n🛠 Execution Plan: [Next steps derived from proposal]\"\n",
        "\n",
        "    def inline_refine_output(self, output):\n",
        "        return output + \"\\n\\n🔧 Inline Refinement: improved clarity, structure, and flow.\"\n",
        "\n",
        "    def inline_validate_output(self, output):\n",
        "        return output + \"\\n\\n🧪 Inline Validation: internally consistent and fact-aligned.\"\n",
        "\n",
        "    def insert_causal_graph(self, output):\n",
        "        state = self.get_recent_state_summary()\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        if state[\"fatigue\"] and state[\"attention\"]:\n",
        "            G.add_edge(\"fatigue\", \"attention\")\n",
        "        if state[\"attention\"] and state[\"efficiency\"]:\n",
        "            G.add_edge(\"attention\", \"efficiency\")\n",
        "\n",
        "        # Represent graph as text\n",
        "        graph_text = \"\\n\".join(f\"{u} → {v}\" for u, v in G.edges)\n",
        "        self.last_graph_text = graph_text\n",
        "\n",
        "        # ✅ Embed graph and store it\n",
        "        emb_vec = None\n",
        "        if hasattr(self, \"embedding_model\"):\n",
        "            emb_input = \"CausalGraph: \" + graph_text\n",
        "            emb_vec = self.embedding_model.encode(emb_input)\n",
        "\n",
        "        if emb_vec is not None:\n",
        "            self.last_graph_embedding = np.array(emb_vec)\n",
        "\n",
        "            # 🔁 Lookup similar causal graphs\n",
        "            similar = find_similar_graphs(self.last_graph_embedding)\n",
        "            self.last_graph_similarity = similar  # for reuse or validation scoring\n",
        "\n",
        "            # (Optional debug print)\n",
        "            if similar:\n",
        "                print(f\"🔁 {self.name} found similar causal graphs:\")\n",
        "                for key, dist in similar:\n",
        "                    print(f\"   → {key} (dist={dist:.2f})\")\n",
        "\n",
        "        return output + \"\\n\\n📈 Causal Graph (agent-state):\\n\" + graph_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_recent_state_summary(self, window: int = 5):\n",
        "        \"\"\"\n",
        "        Returns recent agent state summaries for inline modules.\n",
        "        Includes efficiency, attention, fatigue, hunger, and short-term scores.\n",
        "        \"\"\"\n",
        "        recent = self.interval_score_log[-window:]\n",
        "\n",
        "        summary = {\n",
        "            \"efficiency\": [entry.get(\"efficiency\", 0) for entry in recent],\n",
        "            \"meaningfulness\": [entry.get(\"meaningfulness\", 0) for entry in recent],\n",
        "            \"attention\": [entry.get(\"attention\", 0) for entry in recent],\n",
        "            \"fatigue\": [entry.get(\"fatigue\", 0) for entry in recent],\n",
        "            \"hunger\": [entry.get(\"hunger\", 0) for entry in recent],\n",
        "            \"timestamps\": [entry.get(\"interval\") for entry in recent],\n",
        "        }\n",
        "        return summary\n",
        "\n",
        "\n",
        "    def enhance_statistical_inference(self, output):\n",
        "        state = self.get_recent_state_summary()\n",
        "        eff_scores = state[\"efficiency\"]\n",
        "        att_scores = state[\"attention\"]\n",
        "\n",
        "        if eff_scores:\n",
        "            eff_mean = np.mean(eff_scores)\n",
        "            eff_std = np.std(eff_scores)\n",
        "            att_mean = np.mean(att_scores)\n",
        "            report = (\n",
        "                f\"Efficiency μ={eff_mean:.2f}, σ={eff_std:.2f} | \"\n",
        "                f\"Avg attention={att_mean:.2f}\"\n",
        "            )\n",
        "        else:\n",
        "            report = \"No recent efficiency data available.\"\n",
        "\n",
        "        return output + \"\\n\\n📊 Statistical Summary:\\n\" + report\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate_digital_twin_annotation(self, output):\n",
        "        state = {\n",
        "            \"system\": \"energy grid\",\n",
        "            \"inputs\": {\"solar\": 0.6, \"wind\": 0.3, \"thermal\": 0.1},\n",
        "            \"load\": 92.5,\n",
        "            \"status\": \"stable\"\n",
        "        }\n",
        "        twin_repr = \"\\n\".join(f\"{k}: {v}\" for k, v in state.items())\n",
        "        return output + \"\\n\\n🧿 Digital Twin:\\n\" + twin_repr\n",
        "\n",
        "\n",
        "    def append_simulation_block(self, output):\n",
        "        years = np.arange(2025, 2035)\n",
        "        forecast = 100 * np.exp(-0.05 * (years - 2025))\n",
        "        forecast_text = \"\\n\".join(f\"{y}: {v:.1f}\" for y, v in zip(years, forecast))\n",
        "        return output + \"\\n\\n🧪 Simulation Result:\\n\" + forecast_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def highlight_text_differences(self, verbose=True):\n",
        "        \"\"\"\n",
        "        Compare the last raw output and final output, highlighting changes.\n",
        "        Uses difflib and optionally prints differences in Markdown.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'last_raw_output') or not hasattr(self, 'last_output'):\n",
        "            if verbose:\n",
        "                print(\"⚠️ No outputs available for comparison.\")\n",
        "            return\n",
        "\n",
        "        raw   = self.last_raw_output\n",
        "        final = self.last_output\n",
        "\n",
        "        matcher = difflib.SequenceMatcher(None, raw, final)\n",
        "        highlighted = []\n",
        "\n",
        "        for opcode, a0, a1, b0, b1 in matcher.get_opcodes():\n",
        "            if opcode == 'equal':\n",
        "                highlighted.append(raw[a0:a1])\n",
        "            elif opcode == 'insert':\n",
        "                highlighted.append(\n",
        "                    f\"<span style='background-color:#d4edda;'>[[INSERT → {final[b0:b1]}]]</span>\")\n",
        "            elif opcode == 'delete':\n",
        "                highlighted.append(\n",
        "                    f\"<span style='background-color:#f8d7da;'>[[DELETE → {raw[a0:a1]}]]</span>\")\n",
        "            elif opcode == 'replace':\n",
        "                highlighted.append(\n",
        "                    f\"<span style='background-color:#fff3cd;'>[[REPLACE {raw[a0:a1]} → {final[b0:b1]}]]</span>\")\n",
        "\n",
        "        diff_text = ''.join(highlighted)\n",
        "\n",
        "        if diff_text != raw:\n",
        "            if verbose:\n",
        "                display(Markdown(\n",
        "                    f\"🧩 **Differences between raw and final output:**\\n\\n{diff_text}\"))\n",
        "        elif verbose:\n",
        "            print(\"✅ No textual differences detected between raw and final output.\")\n",
        "\n",
        "\n",
        "\n",
        "    def reset_local_state(self):\n",
        "        \"\"\"\n",
        "        Soft wipe – called whenever the agent starts a *new local mission*.\n",
        "        • Clears short‑term memory, counters and physiology curves.\n",
        "        • Leaves long‑term memory and meta‑weights untouched.\n",
        "        \"\"\"\n",
        "\n",
        "        # 1)  Clear memories and histories  ──────────────────────────────\n",
        "        self.attention_history.clear()\n",
        "        self.fatigue_history.clear()\n",
        "        self.hunger_history.clear()\n",
        "\n",
        "        self.short_memory.clear()\n",
        "\n",
        "        # 2)  Reset counters / timers  ───────────────────────────────────\n",
        "        self.interval              = 1          # NEW  (start interval count at 1)\n",
        "        self.usage_count                = 0\n",
        "        self.meaningless_output_counter = 0\n",
        "        self.start_time                 = time.time()\n",
        "\n",
        "        # 3)  Clear last outputs / foresight  ────────────────────────────\n",
        "        self.last_output            = \"\"\n",
        "        self.last_raw_output        = \"\"\n",
        "        self.last_foresight         = None\n",
        "        self.last_foresight_signals = {}\n",
        "\n",
        "        # seed the very first sample for the *new* local round\n",
        "        self.seed_first_phys_sample()\n",
        "\n",
        "\n",
        "\n",
        "        # 4)  Notify runner (if any)  ────────────────────────────────────\n",
        "        if hasattr(self, \"runner\"):\n",
        "            self.runner.clear_penalty_for_agent(self)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def reset_meaningless_counter(self):\n",
        "        \"\"\"\n",
        "        Resets the counter for meaningless or generic outputs.\n",
        "        Used to avoid unfair penalties after validation or reset.\n",
        "        \"\"\"\n",
        "        self.meaningless_output_counter = 0\n",
        "\n",
        "\n",
        "\n",
        "    def reset_global_state(self):\n",
        "        \"\"\"\n",
        "        Reset the agent’s overall state:\n",
        "          - Clears both local state and strategic assignments\n",
        "          - Resets roles to their initial born_roles\n",
        "          - Clears tasks/features and meta-weights\n",
        "          - Resets round counters and timers\n",
        "        \"\"\"\n",
        "        # First, clear local/session data\n",
        "        self.reset_local_state()\n",
        "\n",
        "        # Revert to initial strategic parameters\n",
        "        self.roles = list(self.born_roles) if hasattr(self, \"born_roles\") else []\n",
        "        self.tasks = []       # Do NOT reseed tasks here\n",
        "        self.features = []\n",
        "        self.in_cooperation = False\n",
        "\n",
        "        # Reset timing and round counters\n",
        "        self.local_round = 1\n",
        "        self.interval = 1\n",
        "        self.start_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "    def share_memory_with(self, cooperating_agents):\n",
        "        \"\"\"\n",
        "        Share and merge short‐ and long‐term memory with peer agents.\n",
        "        Deduplicates entries and applies a slight fatigue/hunger relief.\n",
        "        \"\"\"\n",
        "        # 1) Merge short_memory and long_memory from peers\n",
        "        for other in cooperating_agents:\n",
        "            if other is not self:\n",
        "                # Append short‐term memories\n",
        "                self.short_memory.extend(other.short_memory)\n",
        "                # Merge long‐term entries, keeping higher‑scored items\n",
        "                for key, value in other.long_memory.items():\n",
        "                    if key not in self.long_memory or value.get(\"score\", 0) > self.long_memory[key].get(\"score\", 0):\n",
        "                        self.long_memory[key] = value\n",
        "\n",
        "        # 2) Deduplicate short_memory by (prompt, output)\n",
        "        seen = set()\n",
        "        deduped = []\n",
        "        for entry in self.short_memory:\n",
        "            key = (entry.get(\"prompt\"), entry.get(\"output\"))\n",
        "            if key not in seen:\n",
        "                deduped.append(entry)\n",
        "                seen.add(key)\n",
        "        self.short_memory = deduped\n",
        "\n",
        "        print(f\"🔗 {self.name} shared memory with peers. Short_memory size: {len(self.short_memory)}\")\n",
        "\n",
        "        # 3) Cooperation eases fatigue & hunger slightly\n",
        "        if self.fatigue_history:\n",
        "            self.fatigue_history[-1] = max(self.fatigue_history[-1] - 0.1, 0.0)\n",
        "        if self.hunger_history:\n",
        "            self.hunger_history[-1] = max(self.hunger_history[-1] - 0.1, 0.0)\n",
        "\n",
        "\n",
        "    # =======================\n",
        "    # 7.12. Foresight & Planning\n",
        "    # =======================\n",
        "\n",
        "    def analyze_foresight_signals(self, prompt, output, current_path, stored_entry, verbose=True):\n",
        "        \"\"\"\n",
        "        Compute foresight signals for planning:\n",
        "          - prompt_similarity: cosine similarity between current prompt and stored prompt\n",
        "          - output_similarity: cosine similarity between current output and stored output\n",
        "          - path_difference: count of differences in roles, tasks, features, cooperation\n",
        "        \"\"\"\n",
        "        prompt_sim = 0.0\n",
        "        output_sim = 0.0\n",
        "        path_diff = 0\n",
        "\n",
        "        # Prompt similarity\n",
        "        try:\n",
        "            vec1 = embedding_model.encode(prompt) or np.zeros(EMB_DIM)\n",
        "            vec2 = embedding_model.encode(stored_entry.get(\"original_prompt\", prompt)) or np.zeros(EMB_DIM)\n",
        "            prompt_sim = cosine_similarity([vec1], [vec2])[0][0]\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Foresight prompt similarity failed: {e}\")\n",
        "\n",
        "        # Output similarity\n",
        "        try:\n",
        "            vec1 = embedding_model.encode(output) or np.zeros(EMB_DIM)\n",
        "            vec2 = embedding_model.encode(stored_entry.get(\"output\", output)) or np.zeros(EMB_DIM)\n",
        "            output_sim = cosine_similarity([vec1], [vec2])[0][0]\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Foresight output similarity failed: {e}\")\n",
        "\n",
        "        # Path comparison\n",
        "        stored_path = {\n",
        "            \"roles\": set(stored_entry.get(\"roles\", [])),\n",
        "            \"tasks\": set(stored_entry.get(\"tasks\", [])),\n",
        "            \"features\": set(stored_entry.get(\"features\", [])),\n",
        "            \"cooperation\": stored_entry.get(\"cooperation\", False)\n",
        "        }\n",
        "        curr_path = {\n",
        "            \"roles\": set(current_path.get(\"roles\", [])),\n",
        "            \"tasks\": set(current_path.get(\"tasks\", [])),\n",
        "            \"features\": set(current_path.get(\"features\", [])),\n",
        "            \"cooperation\": current_path.get(\"cooperation\", False)\n",
        "        }\n",
        "        path_diff = sum([\n",
        "            curr_path[\"roles\"] != stored_path[\"roles\"],\n",
        "            curr_path[\"tasks\"] != stored_path[\"tasks\"],\n",
        "            curr_path[\"features\"] != stored_path[\"features\"],\n",
        "            curr_path[\"cooperation\"] != stored_path[\"cooperation\"]\n",
        "        ])\n",
        "\n",
        "        # Initialize similarity history if missing\n",
        "        if not hasattr(self, \"variance_history\"):\n",
        "            self.variance_history = {\n",
        "                \"uvr_similarity_short\": deque(maxlen=5),\n",
        "                \"uvr_similarity_reference\": deque(maxlen=10),\n",
        "                \"reactivation_flag\": False\n",
        "            }\n",
        "\n",
        "        if not hasattr(self, \"prompt_similarity_history\"):\n",
        "            self.prompt_similarity_history = []\n",
        "            self.output_similarity_history = []\n",
        "            self.path_difference_history = []\n",
        "\n",
        "        self.prompt_similarity_history.append(prompt_sim)\n",
        "        self.output_similarity_history.append(output_sim)\n",
        "        self.path_difference_history.append(path_diff)\n",
        "\n",
        "        # Compute UVR status per component\n",
        "        min_window = self.meta_parameters.get(\"uvr_min_window\", 5)\n",
        "        inflection_threshold = self.meta_parameters.get(\"uvr_inflection_threshold\", 1.5)\n",
        "\n",
        "        prompt_uvr, var_prompt_recent, var_prompt_prior, _ = compute_uvr_status(self.prompt_similarity_history, min_window, inflection_threshold)\n",
        "        output_uvr, var_output_recent, var_output_prior, _ = compute_uvr_status(self.output_similarity_history, min_window, inflection_threshold)\n",
        "        path_uvr, var_path_recent, var_path_prior, _ = compute_uvr_status(self.path_difference_history, min_window, inflection_threshold)\n",
        "\n",
        "        if prompt_uvr:\n",
        "            print(f\"⚠️ Prompt UVR — var spike: {var_prompt_recent:.3f} vs {var_prompt_prior:.3f}\")\n",
        "        if output_uvr:\n",
        "            print(f\"⚠️ Output UVR — var spike: {var_output_recent:.3f} vs {var_output_prior:.3f}\")\n",
        "        if path_uvr:\n",
        "            print(f\"⚠️ Path UVR — var spike: {var_path_recent:.3f} vs {var_path_prior:.3f}\")\n",
        "\n",
        "        # Physiology and graph similarities\n",
        "        graph_sim = 1.0 - min(self.last_graph_similarity[0][1], 1.0) if hasattr(self, \"last_graph_similarity\") and self.last_graph_similarity else 0.0\n",
        "\n",
        "        path_vec = np.array([\n",
        "            len(current_path[\"roles\"] & stored_path[\"roles\"]),\n",
        "            len(current_path[\"tasks\"] & stored_path[\"tasks\"]),\n",
        "            len(current_path[\"features\"] & stored_path[\"features\"]),\n",
        "            float(current_path[\"cooperation\"] == stored_path[\"cooperation\"])\n",
        "        ]) / 4.0\n",
        "        path_sim = cosine_similarity([path_vec], [path_vec])[0][0]\n",
        "\n",
        "        physio_vec = np.array([\n",
        "            self.attention_history[-1] if self.attention_history else 0.5,\n",
        "            self.fatigue_history[-1] if self.fatigue_history else 0.5,\n",
        "            self.hunger_history[-1] if self.hunger_history else 0.5\n",
        "        ])\n",
        "        prior_physio_vec = np.array([\n",
        "            stored_entry.get(\"attention_history\", 0.5),\n",
        "            stored_entry.get(\"fatigue_history\", 0.5),\n",
        "            stored_entry.get(\"hunger_history\", 0.5)\n",
        "        ])\n",
        "        physio_sim = cosine_similarity([physio_vec], [prior_physio_vec])[0][0]\n",
        "\n",
        "        # Compute composite UVR similarity\n",
        "        w1 = self.meta_parameters.get(\"uvr_weight_prompt\", 0.2)\n",
        "        w2 = self.meta_parameters.get(\"uvr_weight_output\", 0.2)\n",
        "        w3 = self.meta_parameters.get(\"uvr_weight_graph\", 0.2)\n",
        "        w4 = self.meta_parameters.get(\"uvr_weight_path\", 0.2)\n",
        "        w5 = self.meta_parameters.get(\"uvr_weight_physio\", 0.2)\n",
        "\n",
        "        UVR_similarity = (\n",
        "            w1 * prompt_sim +\n",
        "            w2 * output_sim +\n",
        "            w3 * graph_sim +\n",
        "            w4 * path_sim +\n",
        "            w5 * physio_sim\n",
        "        )\n",
        "\n",
        "        # Dual-window variance tracking\n",
        "        self.variance_history[\"uvr_similarity_short\"].append(UVR_similarity)\n",
        "        if len(self.variance_history[\"uvr_similarity_short\"]) == self.variance_history[\"uvr_similarity_short\"].maxlen:\n",
        "            for val in self.variance_history[\"uvr_similarity_short\"]:\n",
        "                self.variance_history[\"uvr_similarity_reference\"].append(val)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"🧠 {self.name} short variance → {list(self.variance_history['uvr_similarity_short'])}\")\n",
        "            print(f\"📘 {self.name} ref variance → {list(self.variance_history['uvr_similarity_reference'])}\")\n",
        "\n",
        "        # Compute delta and novelty\n",
        "        var_short = np.var(list(self.variance_history[\"uvr_similarity_short\"]))\n",
        "        var_long = np.var(list(self.variance_history[\"uvr_similarity_reference\"]))\n",
        "        delta_var = var_short - var_long\n",
        "        novelty_score = 1.0 - UVR_similarity\n",
        "\n",
        "\n",
        "\n",
        "        # ── STEP 6: Classify inflection phase based on short vs. reference variance ──\n",
        "        phase_margin = self.meta_parameters.get(\"inflection_phase_margin\", 0.03)\n",
        "\n",
        "        if var_long < 1e-5:\n",
        "            inflection_phase = \"undetermined\"\n",
        "        else:\n",
        "            ratio = var_short / var_long\n",
        "\n",
        "            if abs(ratio - 1.0) < phase_margin:\n",
        "                inflection_phase = \"inflection\"\n",
        "            elif ratio > 1.0 + phase_margin:\n",
        "                inflection_phase = \"post-inflection\"\n",
        "            elif ratio < 1.0 - phase_margin:\n",
        "                inflection_phase = \"pre-inflection\"\n",
        "            else:\n",
        "                inflection_phase = \"stable\"\n",
        "\n",
        "        self.variance_history[\"phase\"] = inflection_phase\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"📊 {self.name} inflection phase → {inflection_phase}\")\n",
        "\n",
        "\n",
        "        delta_thresh = self.meta_parameters.get(\"uvr_reactivation_delta_threshold\", 0.05)\n",
        "        novelty_thresh = self.meta_parameters.get(\"uvr_reactivation_novelty_threshold\", 0.3)\n",
        "        should_reactivate = delta_var > delta_thresh and novelty_score > novelty_thresh\n",
        "        self.variance_history[\"reactivation_flag\"] = should_reactivate\n",
        "\n",
        "        if should_reactivate:\n",
        "            print(f\"🔄 Reactivation triggered → ΔVar={delta_var:.3f}, Novelty={novelty_score:.3f}\")\n",
        "\n",
        "        # ── PHASE 1: Spike Logging ──────────────────────────────\n",
        "        if not hasattr(self, \"signal_spike_log\"):\n",
        "            self.signal_spike_log = deque(maxlen=100)\n",
        "\n",
        "        self.signal_spike_log.append({\n",
        "            \"timestamp\": time.time(),\n",
        "            \"uvr_similarity\": round(UVR_similarity, 3),\n",
        "            \"output_similarity_variance\": round(var_output_recent, 4),\n",
        "            \"prompt_similarity\": round(prompt_sim, 4),\n",
        "            \"scenario\": getattr(self, \"last_scenario\", \"unknown\"),\n",
        "            \"roles\": self.roles[:],\n",
        "            \"tasks\": self.tasks[:],\n",
        "            \"features\": self.features[:],\n",
        "            \"inline_activated_roles\": self.inline_activated_roles[:],\n",
        "            \"reactivation_flag\": True,\n",
        "            \"inflection_phase\": self.variance_history.get(\"phase\", \"unknown\")\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "        # Optional: single-track UVR variance for logging\n",
        "        if not hasattr(self, \"uvr_similarity_history\"):\n",
        "            self.uvr_similarity_history = []\n",
        "        self.uvr_similarity_history.append(UVR_similarity)\n",
        "        uvr_triggered, var_recent, var_prior, var_ratio = compute_uvr_status(\n",
        "            self.uvr_similarity_history, min_window, inflection_threshold\n",
        "        )\n",
        "        if uvr_triggered:\n",
        "            print(f\"⚠️ UVR triggered — Similarity variance spike (ratio={var_ratio:.2f})\")\n",
        "\n",
        "        return {\n",
        "            \"prompt_similarity\": round(prompt_sim, 3),\n",
        "            \"output_similarity\": round(output_sim, 3),\n",
        "            \"path_difference\": path_diff,\n",
        "            \"uvr_similarity\": round(UVR_similarity, 3),\n",
        "            \"output_similarity_variance\": var_output_recent\n",
        "        }\n",
        "\n",
        "        # ── STEP 5: Log contextual delta variance for later meta-learning or clustering ──\n",
        "        if not hasattr(self, \"variance_context_log\"):\n",
        "            self.variance_context_log = []\n",
        "\n",
        "        self.variance_context_log.append({\n",
        "            \"agent\": self.name,\n",
        "            \"delta_var\": round(delta_var, 4),\n",
        "            \"novelty_score\": round(novelty_score, 4),\n",
        "            \"uvr_similarity\": round(UVR_similarity, 4),\n",
        "            \"roles\": list(curr_path[\"roles\"]),\n",
        "            \"tasks\": list(curr_path[\"tasks\"]),\n",
        "            \"features\": list(curr_path[\"features\"]),\n",
        "            \"cooperation\": curr_path[\"cooperation\"],\n",
        "            \"scenario\": (stored_entry.get(\"scenario\") or \"unknown\"),\n",
        "            \"timestamp\": time.time()\n",
        "        })\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"🧾 {self.name} logged delta variance context for meta-analysis.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def compute_foresight(self, prompt, path_reuse, similarity_score, external_db=None):\n",
        "        \"\"\"\n",
        "        Determine a foresight scenario based on:\n",
        "          - Reuse flag (convergent paths)\n",
        "          - Similarity scores\n",
        "          - Foresight signals (prompt/output similarity, path differences)\n",
        "        Returns a scenario label (e.g., Black Swan, Grey Rhino).\n",
        "        \"\"\"\n",
        "        # Build key and current context path\n",
        "        prompt_key = self.generate_prompt_key(prompt)\n",
        "        current_path = {\n",
        "            \"tasks\": set(self.tasks),\n",
        "            \"features\": set(self.features),\n",
        "            \"roles\": set(self.roles),\n",
        "            \"cooperation\": self.in_cooperation\n",
        "        }\n",
        "\n",
        "        # Retrieve stored context (local or external)\n",
        "        stored = self.long_memory.get(prompt_key)\n",
        "        if not stored and external_db:\n",
        "            stored = external_db.get(prompt_key)\n",
        "\n",
        "        # Increment external usage if relevant\n",
        "        if stored and external_db and prompt_key in external_db:\n",
        "            external_db[prompt_key][\"usage_count\"] = external_db[prompt_key].get(\"usage_count\", 0) + 1\n",
        "\n",
        "        # Analyze signals or default values\n",
        "        if stored:\n",
        "            signals = self.analyze_foresight_signals(prompt, self.last_output, current_path, stored)\n",
        "        else:\n",
        "            signals = {\"prompt_similarity\": 0.0, \"output_similarity\": 0.0, \"path_difference\": 4}\n",
        "\n",
        "        self.last_foresight_signals = signals\n",
        "\n",
        "        # ✅ Classification logic — causal override FIRST\n",
        "        if hasattr(self, \"last_graph_similarity\") and self.last_graph_similarity:\n",
        "            most_similar_key, min_dist = min(self.last_graph_similarity, key=lambda x: x[1])\n",
        "            if min_dist < 0.25:\n",
        "                scenario = \"✅ Convergent Paths (Causal)\"\n",
        "            elif min_dist < 0.4:\n",
        "                scenario = \"🌫️ Grey Swan (Causal)\"\n",
        "            elif min_dist < 0.6:\n",
        "                scenario = \"🐉 Wild Card (Causal)\"\n",
        "            else:\n",
        "                scenario = \"🕳️ Black Swan (Causal)\"\n",
        "            self.last_foresight = scenario\n",
        "            return scenario\n",
        "\n",
        "\n",
        "        # Classification logic\n",
        "        if path_reuse:\n",
        "            scenario = \"✅ Convergent Paths\"\n",
        "        elif similarity_score < 0.3 and signals[\"path_difference\"] >= 3:\n",
        "            scenario = \"🧩 Cascading Discontinuity\"\n",
        "        elif similarity_score < 0.5 and signals[\"output_similarity\"] > 0.9:\n",
        "            scenario = \"🪂 Tipping Point\"\n",
        "        elif signals[\"path_difference\"] >= 2 or signals[\"output_similarity\"] < 0.5:\n",
        "            scenario = \"🐉 Wild Card\"\n",
        "        elif similarity_score > 0.8 and signals[\"output_similarity\"] > 0.8:\n",
        "            scenario = \"🧭 White Swan\"\n",
        "        elif similarity_score > 0.6 and signals[\"output_similarity\"] > 0.6:\n",
        "            scenario = \"🌫️ Grey Swan\"\n",
        "        elif similarity_score > 0.4:\n",
        "            scenario = \"🤝 Grey Rhino\"\n",
        "        else:\n",
        "            scenario = \"🕳️ Black Swan\"\n",
        "\n",
        "        self.last_foresight = scenario\n",
        "        return scenario\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def assign_cluster_tag(self, cluster):\n",
        "        \"\"\"\n",
        "        Assigns a semantic label to a signal cluster.\n",
        "        This helps interpret root causes behind volatility spikes.\n",
        "        \"\"\"\n",
        "        avg_var = np.mean([s[\"output_similarity_variance\"] for s in cluster])\n",
        "        common_roles = set.intersection(*[set(s[\"roles\"]) for s in cluster])\n",
        "        common_scenarios = set(s[\"scenario\"] for s in cluster)\n",
        "\n",
        "        # Rule-based tag assignment\n",
        "        if avg_var > 0.06 and \"Black Swan\" in common_scenarios:\n",
        "            tag = \"instability (foresight shock)\"\n",
        "        elif avg_var > 0.06 and \"Tipping Point\" in common_scenarios:\n",
        "            tag = \"strategy shift (critical phase)\"\n",
        "        elif avg_var < 0.02:\n",
        "            tag = \"residual drift\"\n",
        "        else:\n",
        "            tag = \"adaptive turbulence\"\n",
        "\n",
        "        # Assign to each spike in cluster\n",
        "        for spike in cluster:\n",
        "            spike[\"cluster_tag\"] = tag\n",
        "            spike[\"cluster_roles\"] = list(common_roles)\n",
        "\n",
        "\n",
        "    def build_scenario_cluster_map(self):\n",
        "        \"\"\"\n",
        "        Constructs a map of foresight scenarios to the types of variance clusters\n",
        "        they've co-occurred with.\n",
        "        \"\"\"\n",
        "        scenario_cluster_map = {}\n",
        "\n",
        "        # Aggregate all clusters from agents\n",
        "        all_spikes = []\n",
        "        for ag in self.agents:\n",
        "            all_spikes.extend(getattr(ag, \"signal_spike_log\", []))\n",
        "\n",
        "        for spike in all_spikes:\n",
        "            scen = spike.get(\"scenario\", \"Unknown\")\n",
        "            tag = spike.get(\"cluster_tag\", None)\n",
        "            if tag is None:\n",
        "                continue  # Skip untagged\n",
        "\n",
        "            if scen not in scenario_cluster_map:\n",
        "                scenario_cluster_map[scen] = []\n",
        "\n",
        "            scenario_cluster_map[scen].append(tag)\n",
        "\n",
        "        # Deduplicate\n",
        "        for scen, tags in scenario_cluster_map.items():\n",
        "            scenario_cluster_map[scen] = list(set(tags))\n",
        "\n",
        "        self.scenario_cluster_map = scenario_cluster_map\n",
        "\n",
        "        print(\"📌 Scenario–Cluster Co-Occurrence Map:\")\n",
        "        for scen, tags in scenario_cluster_map.items():\n",
        "            print(f\"  - {scen}: {tags}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # ============================================================\n",
        "    # 📘 PROMPT_KEY vs SEMANTIC REUSE — DESIGN GUIDELINES\n",
        "    # ============================================================\n",
        "\n",
        "    \"\"\"\n",
        "    IMPORTANT DESIGN NOTE:\n",
        "\n",
        "    This system uses TWO DIFFERENT mechanisms to handle prompts:\n",
        "\n",
        "    —————————————————————————————\n",
        "    📌 1. prompt_key → (generate_prompt_key)\n",
        "\n",
        "    - Purpose → For EXTERNAL DB and LONG MEMORY storage/retrieval\n",
        "    - How → Uses normalized (lowercase, no space/newline) prompt text\n",
        "    - Use cases →\n",
        "        - Quickly check if the EXACT same prompt was previously stored\n",
        "        - Retrieve stored mission results (local memory or external database)\n",
        "        - Avoid exact duplicates (redundancy filtering)\n",
        "\n",
        "    - NOT semantic → purely based on raw text format\n",
        "\n",
        "    —————————————————————————————\n",
        "    📌 2. Semantic Similarity → (similarity_to_db + reuse_similarity_threshold)\n",
        "\n",
        "    - Purpose → For mission REUSE detection (similarity, not identity)\n",
        "    - How → Uses embeddings + cosine similarity\n",
        "    - Use cases →\n",
        "        - Detect if new prompt is semantically similar to past prompts\n",
        "        - Decide if mission should be treated as NEW or CONTINUATION\n",
        "        - Adaptive (meta-learned threshold) → allows exploration vs convergence\n",
        "\n",
        "    - Fully semantic → uses embeddings for meaning-based comparison\n",
        "\n",
        "    —————————————————————————————\n",
        "    ✅ Summary\n",
        "\n",
        "    prompt_key → storage + lookup (exact match / fast redundancy filter)\n",
        "    semantic similarity → mission reuse detection (meta-learned + meaning-based)\n",
        "\n",
        "    THESE TWO SHOULD NOT BE CONFUSED.\n",
        "    They work together, but have distinct roles.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # ============================================================\n",
        "    # 📘 PROMPT FLOW DIAGRAM — FROM INPUT TO REUSE / STORAGE / FORESIGHT\n",
        "    # ============================================================\n",
        "\n",
        "    \"\"\"\n",
        "    Diagram — How prompts flow through the system:\n",
        "\n",
        "    [ USER PROMPT ]\n",
        "       ↓\n",
        "    (similarity_to_db → semantic similarity → meta-learned reuse threshold → YES/NO reuse decision)\n",
        "\n",
        "       ↓                                        ↓\n",
        "    [ Same mission → CONTINUE global round ]   [ New mission → RESET global round ]\n",
        "\n",
        "       ↓\n",
        "    [ ACT + local validation + result generation ]\n",
        "       ↓\n",
        "    [ Store in SHORT MEMORY → (prompt, output, score) ]    → (uses raw prompt)\n",
        "       ↓\n",
        "    [ Store in LONG MEMORY → (prompt_key, entry) ]         → (uses generate_prompt_key)\n",
        "\n",
        "       ↓\n",
        "    [ If validated as GLOBAL best → store in EXTERNAL DB → (prompt_key, entry) ]\n",
        "\n",
        "       ↓\n",
        "    [ Later missions → use similarity_to_db + semantic similarity → compare against EXTERNAL DB ]\n",
        "\n",
        "       ↓\n",
        "    [ When foresight analysis is needed → retrieve via prompt_key → compare semantic similarity (embedding) to stored prompt/output → classify scenario (Black Swan, Grey Rhino, etc.) ]\n",
        "    \"\"\"\n",
        "\n",
        "    # ============================================================\n",
        "    # ==================== 🧭 CELL 7.13 — SIMILARITY TO EXTERNAL DB ===\n",
        "    # ============================================================\n",
        "\n",
        "    def similarity_to_long_memory_db(self, prompt):\n",
        "        \"\"\"\n",
        "        Compute the highest semantic similarity between *prompt* and any\n",
        "        stored prompt in the external DB. Returns 0 if the DB is empty.\n",
        "        \"\"\"\n",
        "        db = getattr(self, \"long_memory\", None)\n",
        "        if not db:                        # no DB or empty DB\n",
        "            return 0.0\n",
        "\n",
        "        # Embedding for the new prompt\n",
        "        prompt_vec = embedding_model.encode(prompt)\n",
        "        if prompt_vec is None:\n",
        "            prompt_vec = np.zeros(EMB_DIM)\n",
        "        elif isinstance(prompt_vec, list):\n",
        "            prompt_vec = np.array(prompt_vec)\n",
        "\n",
        "        highest_similarity = 0.0\n",
        "\n",
        "        # Iterate over stored prompts\n",
        "        for key, entry in db.items():\n",
        "            stored_prompt = entry.get(\"prompt\", \"\")\n",
        "            stored_vec = embedding_model.encode(stored_prompt)\n",
        "            if stored_vec is None:\n",
        "                stored_vec = np.zeros(EMB_DIM)\n",
        "            elif isinstance(stored_vec, list):\n",
        "                stored_vec = np.array(stored_vec)\n",
        "\n",
        "            sim = cosine_similarity([prompt_vec], [stored_vec])[0][0]\n",
        "            highest_similarity = max(highest_similarity, sim)\n",
        "\n",
        "        return highest_similarity\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PGIZ_4H20yKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ==================== 🏁 CELL 8 — MULTI-AGENT ROUNDS =========\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import math\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "\n",
        "# Global counter for rounds\n",
        "GLOBAL_ROUND = 1\n",
        "\n",
        "def run_agents_round(\n",
        "    agents,\n",
        "    mission_prompt,\n",
        "    verbose: bool = False,\n",
        "    new_mission: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Executes one global round for a list of agents on the same mission prompt.\n",
        "    - new_mission=True  → Reset GLOBAL_ROUND to 1\n",
        "    - new_mission=False → Continue from last GLOBAL_ROUND\n",
        "    Resets each agent’s local_round and interval via initialize_from_prompt,\n",
        "    and provides a runner context for clear_penalty and purge_log.\n",
        "    \"\"\"\n",
        "    global GLOBAL_ROUND\n",
        "\n",
        "    # 1) Reset global counter if this is a brand-new mission\n",
        "    if new_mission:\n",
        "        GLOBAL_ROUND = 1\n",
        "\n",
        "    # 2) Provide runner context (for agent.runner references)\n",
        "    # runner_context = SimpleNamespace(global_round=GLOBAL_ROUND)\n",
        "\n",
        "    # Update the real runner’s global round counter\n",
        "    runner.global_round = GLOBAL_ROUND\n",
        "\n",
        "    # 3) Announce this round\n",
        "    print(f\"\\n🌍 GLOBAL ROUND {GLOBAL_ROUND} — Mission: {mission_prompt}\\n\")\n",
        "\n",
        "    # 4) Initialize or reset each agent’s counters & runner link\n",
        "    for agent in agents:\n",
        "        # agent.runner = runner_context\n",
        "        agent.initialize_from_prompt(\n",
        "            mission_prompt,\n",
        "            global_round=GLOBAL_ROUND,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "    # 5) Execute each agent’s act() and collect results\n",
        "    records = []\n",
        "    for agent in agents:\n",
        "        output, score, attention, fatigue, hunger, _, meta_parameters, status = agent.act(\n",
        "            mission_prompt, verbose=verbose\n",
        "        )\n",
        "        records.append({\n",
        "            \"Agent\": agent.name,\n",
        "            \"Roles\": \", \".join(agent.roles),\n",
        "            \"Tasks\": \", \".join(agent.tasks),\n",
        "            \"Score\": score,\n",
        "            \"Attention\": attention,\n",
        "            \"Fatigue\": fatigue,\n",
        "            \"Hunger\": hunger,\n",
        "            \"Meta_StrategyFit\": round(meta_parameters.get(\"strategy_fit\", 0.0), 3),\n",
        "            \"Meta_TaskCoupling\": round(meta_parameters.get(\"task_feature_coupling\", 0.0), 3),\n",
        "            \"ShortMem\": len(agent.short_memory),\n",
        "            \"LongMem\": len(agent.long_memory),\n",
        "            \"ExternalCalls\": agent.external_access_count,\n",
        "            \"Status\": status\n",
        "        })\n",
        "\n",
        "    # 6) Display summary table\n",
        "    df = pd.DataFrame(records)\n",
        "    display(\n",
        "        df.style\n",
        "          .set_caption(f\"📊 Summary: Global Round {GLOBAL_ROUND}\")\n",
        "          .format(precision=3)\n",
        "    )\n",
        "\n",
        "    # 7) Advance the global counter for next call\n",
        "    GLOBAL_ROUND += 1\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yYmQuUxWNiV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ==================== 🛡️ CELL 9 — REDUNDANCY FILTER ==========\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "class RedundancyFilter:\n",
        "\n",
        "    @staticmethod\n",
        "    def is_prompt_redundant(prompt, external_db, agent, reuse_threshold=None):\n",
        "        \"\"\"\n",
        "        Returns True if the prompt is redundant (based on semantic similarity).\n",
        "        Also bumps usage count if found redundant → for smarter decay handling.\n",
        "        \"\"\"\n",
        "        if not external_db:\n",
        "            return False\n",
        "\n",
        "        prompt_vec = embedding_model.encode(prompt)    # OK\n",
        "\n",
        "        # ✅ Normalize embedding\n",
        "        if prompt_vec is None:\n",
        "            prompt_vec = np.zeros(384)\n",
        "        elif isinstance(prompt_vec, list):\n",
        "            prompt_vec = np.array(prompt_vec)\n",
        "\n",
        "        highest_similarity = 0.0\n",
        "        most_similar_key = None\n",
        "\n",
        "        for key, entry in external_db.items():\n",
        "            stored_prompt = entry.get(\"original_prompt\", \"\")\n",
        "            stored_vec = embedding_model.encode(stored_prompt)    # OK\n",
        "\n",
        "            # ✅ Normalize embedding\n",
        "            if stored_vec is None:\n",
        "                stored_vec = np.zeros(384)\n",
        "            elif isinstance(stored_vec, list):\n",
        "                stored_vec = np.array(stored_vec)\n",
        "\n",
        "            sim = cosine_similarity([prompt_vec], [stored_vec])[0][0]\n",
        "\n",
        "            if sim > highest_similarity:\n",
        "                highest_similarity = sim\n",
        "                most_similar_key = key\n",
        "\n",
        "        # Use agent meta-parameter threshold if not explicitly provided\n",
        "        if reuse_threshold is None:\n",
        "            reuse_threshold = agent.meta_parameters.get(\"reuse_similarity_threshold\", 0.75)\n",
        "\n",
        "        if highest_similarity >= reuse_threshold:\n",
        "            print(f\"🛑 [FILTER] Prompt-level redundant: similarity={highest_similarity:.3f} >= threshold={reuse_threshold:.3f}\")\n",
        "\n",
        "            # ✅ Bump usage count → prevents fast decay\n",
        "            if most_similar_key:\n",
        "                external_db[most_similar_key][\"usage_count\"] += 1\n",
        "\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def is_task_redundant(agent, task, prompt=None, output=None, role=None, role_aware=False):\n",
        "        \"\"\"\n",
        "        Check if *task* has already been executed by *agent* under the same prompt/output.\n",
        "        If role_aware=True, require that the same role was involved.\n",
        "        \"\"\"\n",
        "        for item in agent.short_memory:\n",
        "            if item.get(\"task\") != task:\n",
        "                continue\n",
        "\n",
        "            # Role‑aware filtering: skip if the stored entry wasn’t done under the same role\n",
        "            if role_aware and role and role not in item.get(\"roles\", []):\n",
        "                continue\n",
        "\n",
        "            # Redundancy by prompt or by exact output\n",
        "            if prompt and item.get(\"prompt\") == prompt:\n",
        "                print(f\"♻️ [FILTER] Task‑level redundant on prompt: {task}\")\n",
        "                return True\n",
        "            if output and item.get(\"output\") == output:\n",
        "                print(f\"♻️ [FILTER] Task‑level redundant on output: {task}\")\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def is_content_redundant(agent, task, prompt, cooperating_agents, threshold=0.85):\n",
        "        \"\"\"\n",
        "        Return True if *agent* recently produced content for *task/prompt*\n",
        "        that is semantically very similar to content already produced by any\n",
        "        peer in *cooperating_agents*.\n",
        "        \"\"\"\n",
        "        # 1. Find the latest output by this agent for the task+prompt\n",
        "        curr_entry = next(\n",
        "            (e for e in reversed(agent.short_memory)\n",
        "             if e.get(\"task\") == task and e.get(\"prompt\") == prompt),\n",
        "            None\n",
        "        )\n",
        "        if not curr_entry:\n",
        "            return False\n",
        "\n",
        "        curr_vec = embedding_model.encode(curr_entry[\"output\"])\n",
        "        if curr_vec is None:\n",
        "            return False\n",
        "        if isinstance(curr_vec, list):\n",
        "            curr_vec = np.array(curr_vec)\n",
        "\n",
        "        # 2. Compare with peers’ outputs for the same task\n",
        "        for peer in cooperating_agents:\n",
        "            if peer is agent:\n",
        "                continue\n",
        "            for e in peer.short_memory:\n",
        "                if e.get(\"task\") != task:\n",
        "                    continue\n",
        "                peer_vec = embedding_model.encode(e[\"output\"])\n",
        "                if peer_vec is None:\n",
        "                    continue\n",
        "                if isinstance(peer_vec, list):\n",
        "                    peer_vec = np.array(peer_vec)\n",
        "\n",
        "                sim = cosine_similarity([curr_vec], [peer_vec])[0][0]\n",
        "                if sim >= threshold:\n",
        "                    print(f\"♻️ [FILTER] Content redundant between {agent.name} and {peer.name} \"\n",
        "                          f\"(task={task}, sim={sim:.2f})\")\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def is_graph_redundant(graph_embedding, graph_index, graph_key_map, reuse_threshold=0.25):\n",
        "        \"\"\"\n",
        "        Returns True if the graph embedding is close enough to a past one\n",
        "        to be considered redundant. Uses FAISS nearest neighbor search.\n",
        "\n",
        "        - `graph_embedding`: np.ndarray of current causal graph\n",
        "        - `graph_index`: FAISS index (already built)\n",
        "        - `graph_key_map`: mapping from FAISS idx → mission key\n",
        "        - `reuse_threshold`: L2 distance threshold to flag as redundant\n",
        "        \"\"\"\n",
        "\n",
        "        if graph_index.ntotal == 0:\n",
        "            return False  # No prior graphs\n",
        "\n",
        "        if graph_embedding is None or not isinstance(graph_embedding, np.ndarray):\n",
        "            return False\n",
        "\n",
        "        D, I = graph_index.search(np.array([graph_embedding]), k=1)\n",
        "        dist = D[0][0]\n",
        "\n",
        "        if dist < reuse_threshold:\n",
        "            matched_key = graph_key_map.get(I[0][0], \"[unknown]\")\n",
        "            print(f\"🛑 [FILTER] Graph-level redundant: dist={dist:.3f} < threshold={reuse_threshold:.3f}\")\n",
        "            print(f\"    Most similar graph mission key: {matched_key}\")\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4x7N-1X06Zc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ==================== 🚦 CELL 10 — FORESIGHT BEHAVIOR & COUNTERS =========\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "\n",
        "# 📜 Global foresight behavior map\n",
        "FORESIGHT_BEHAVIOR_MAP = {\n",
        "    \"✅ Convergent Paths\": {\n",
        "        \"retrieval\": \"M_global\",\n",
        "        \"randomness\": \"none\",\n",
        "        \"response\": \"reuse_best_path\"\n",
        "    },\n",
        "    \"🤝 Grey Rhino\": {\n",
        "        \"retrieval\": \"M_long\",\n",
        "        \"randomness\": \"low\",\n",
        "        \"response\": \"force_attention_reset\"\n",
        "    },\n",
        "    \"🧭 White Swan\": {\n",
        "        \"retrieval\": \"M_long_nearest_neighbor\",\n",
        "        \"randomness\": \"low_moderate\",\n",
        "        \"response\": \"prepare_adjust_weights\"\n",
        "    },\n",
        "    \"🌫️ Grey Swan\": {\n",
        "        \"retrieval\": \"partial_match_shadow\",\n",
        "        \"randomness\": \"moderate\",\n",
        "        \"response\": \"probe_with_low_weight\"\n",
        "    },\n",
        "    \"🐉 Wild Card\": {\n",
        "        \"retrieval\": \"low_confidence_match\",\n",
        "        \"randomness\": \"high\",\n",
        "        \"response\": \"promote_exploration\"\n",
        "    },\n",
        "    \"🕳️ Black Swan\": {\n",
        "        \"retrieval\": \"none_blank_slate\",\n",
        "        \"randomness\": \"maximum\",\n",
        "        \"response\": \"activate_high_entropy\"\n",
        "    },\n",
        "    \"🧩 Cascading Discontinuity\": {\n",
        "        \"retrieval\": \"cascade_segment\",\n",
        "        \"randomness\": \"varies\",\n",
        "        \"response\": \"activate_chain_analysis\"\n",
        "    },\n",
        "    \"🪂 Tipping Point\": {\n",
        "        \"retrieval\": \"fragile_prior_path\",\n",
        "        \"randomness\": \"moderate_high\",\n",
        "        \"response\": \"add_redundancy\"\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def normalize_foresight_key(scenario: str) -> str:\n",
        "    \"\"\"\n",
        "    Strips causal suffix or trailing metadata to match base FORESIGHT_BEHAVIOR_MAP keys.\n",
        "    \"\"\"\n",
        "    return scenario.split(\" (\")[0].strip()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def update_decision_counter(agent_name, interval_counters):\n",
        "    \"\"\"\n",
        "    Increment or initialize the cooperation decision counter for an agent.\n",
        "    \"\"\"\n",
        "    current = interval_counters.get(agent_name, 0)\n",
        "    interval_counters[agent_name] = current + 1 if current >= 1 else 1\n",
        "    return interval_counters[agent_name]\n",
        "\n",
        "def update_interval_counter(agent_name, interval_counters, active=True):\n",
        "    \"\"\"\n",
        "    Increment the interval counter for an agent if active;\n",
        "    otherwise leave it unchanged.\n",
        "    \"\"\"\n",
        "    if active:\n",
        "        interval_counters[agent_name] = interval_counters.get(agent_name, 0) + 1\n",
        "    return interval_counters[agent_name]\n"
      ],
      "metadata": {
        "id": "zoiLcby_PK3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 11. RUNNER CLASS\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "import time\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# 11.1. Prompt Clustering\n",
        "# --------------------\n",
        "\n",
        "class EnhancedStrategicRunner:\n",
        "\n",
        "\n",
        "    def process_mission_prompt(self, mission_prompt, verbose=False):\n",
        "        \"\"\"\n",
        "        Normalize and preprocess the mission prompt.\n",
        "        Compute and cache prompt key and embedding for this round.\n",
        "\n",
        "        Side effects:\n",
        "        - Updates self.last_mission_prompt\n",
        "        - Updates self.last_mission_key\n",
        "        - Updates self.last_mission_embedding\n",
        "        \"\"\"\n",
        "\n",
        "        # ── 1. Normalize prompt for key and comparison ──\n",
        "        normalized_prompt = mission_prompt.strip().lower().replace(\"\\n\", \" \").strip()\n",
        "        self.last_mission_prompt = mission_prompt\n",
        "        self.last_mission_key = self.generate_prompt_key(normalized_prompt)\n",
        "\n",
        "        # ── 2. Compute and cache the embedding ──\n",
        "        raw_emb = embedding_model.encode(mission_prompt)\n",
        "        if raw_emb is None:\n",
        "            emb = np.zeros(EMB_DIM)\n",
        "        elif isinstance(raw_emb, list):\n",
        "            emb = np.array(raw_emb)\n",
        "        else:\n",
        "            emb = raw_emb\n",
        "        self.last_mission_embedding = emb\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"📌 Processed mission prompt: {mission_prompt[:80]}...\")\n",
        "            print(f\"   ↪️ Key: {self.last_mission_key[:12]}... | Embedding dim: {emb.shape[0]}\")\n",
        "\n",
        "        return self.last_mission_key, self.last_mission_embedding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def cluster_prompts(self, n_clusters=3):\n",
        "        \"\"\"\n",
        "        Cluster stored prompts in external_db using KMeans on precomputed embeddings.\n",
        "        Prints and returns a dict mapping cluster labels to lists of prompt strings.\n",
        "        \"\"\"\n",
        "        keys = list(self.external_db.keys())\n",
        "        prompts = []\n",
        "        embeddings = []\n",
        "\n",
        "        # Gather embeddings and corresponding prompts\n",
        "        for k in keys:\n",
        "            entry = self.external_db[k]\n",
        "            if \"embedding\" in entry:\n",
        "                embeddings.append(entry[\"embedding\"])\n",
        "                prompts.append(entry.get(\"original_prompt\", k))\n",
        "\n",
        "        # Guard against too few data points\n",
        "        if len(embeddings) < n_clusters:\n",
        "            print(f\"⚠️ Not enough prompts to form {n_clusters} clusters \"\n",
        "                  f\"(only {len(embeddings)} available).\")\n",
        "            return {}\n",
        "\n",
        "        # Perform clustering\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "        # Organize prompts by cluster\n",
        "        clusters = {i: [] for i in range(n_clusters)}\n",
        "        for idx, label in enumerate(labels):\n",
        "            clusters[label].append(prompts[idx])\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\n📊 Prompt Clusters (semantic similarity):\")\n",
        "        for label, members in clusters.items():\n",
        "            print(f\"🔹 Cluster {label + 1}:\")\n",
        "            for p in members:\n",
        "                print(f\"  - {p[:80]}\")\n",
        "        return clusters\n",
        "\n",
        "\n",
        "    def score_prompt(prompt_text):\n",
        "        \"\"\"\n",
        "        Score a prompt based on length, structure, keyword richness, etc.\n",
        "        Can be adjusted based on your scoring philosophy.\n",
        "        \"\"\"\n",
        "        if not prompt_text.strip():\n",
        "            return 0.0\n",
        "\n",
        "        base = 0.3\n",
        "        word_bonus = 0.01 * len(prompt_text.split())      # e.g. 10 words → +0.10\n",
        "        symbol_penalty = 0.1 if \"???\" in prompt_text else 0.0\n",
        "\n",
        "        return min(base + word_bonus - symbol_penalty, 1.0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update_prompt_clusters(self, new_prompt, new_embedding, recluster_threshold=5):\n",
        "        \"\"\"\n",
        "        Update prompt clusters with a new prompt and embedding.\n",
        "        Track best-scored representative prompt per cluster.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"prompt_clusters\"):\n",
        "            self.prompt_clusters = []\n",
        "\n",
        "        added_to_existing = False\n",
        "        prompt_score = score_prompt(new_prompt)  # ← Use your defined scoring function\n",
        "\n",
        "        # Try to assign to closest cluster\n",
        "        for cluster in self.prompt_clusters:\n",
        "            centroid = cluster[\"centroid\"]\n",
        "            similarity = cosine_similarity([new_embedding], [centroid])[0][0]\n",
        "            if similarity > 0.85:\n",
        "                cluster[\"prompts\"].append(new_prompt)\n",
        "\n",
        "                # Update best prompt if applicable\n",
        "                if \"best_score\" not in cluster or prompt_score > cluster[\"best_score\"]:\n",
        "                    cluster[\"best_prompt\"] = new_prompt\n",
        "                    cluster[\"best_score\"] = prompt_score\n",
        "\n",
        "                added_to_existing = True\n",
        "                break\n",
        "\n",
        "        # If not close to any existing, create a new cluster\n",
        "        if not added_to_existing:\n",
        "            self.prompt_clusters.append({\n",
        "                \"prompts\": [new_prompt],\n",
        "                \"centroid\": new_embedding,\n",
        "                \"best_prompt\": new_prompt,\n",
        "                \"best_score\": prompt_score\n",
        "            })\n",
        "\n",
        "        if len(self.prompt_clusters) >= recluster_threshold:\n",
        "            self.recluster_prompt_centroids()\n",
        "\n",
        "        self.save_external_db()\n",
        "\n",
        "\n",
        "    def recluster_prompt_centroids(self, n_clusters=3):\n",
        "        \"\"\"\n",
        "        Recluster all stored prompts based on their embeddings and rebuild prompt_clusters.\n",
        "        Tracks the best-scored prompt per cluster using score_prompt().\n",
        "        \"\"\"\n",
        "        if not self.external_db:\n",
        "            return\n",
        "\n",
        "        prompts = []\n",
        "        embeddings = []\n",
        "\n",
        "        for k, entry in self.external_db.items():\n",
        "            if k == \"__meta__\":\n",
        "                continue\n",
        "            if \"embedding\" in entry:\n",
        "                embeddings.append(entry[\"embedding\"])\n",
        "                prompts.append(entry.get(\"original_prompt\", k))\n",
        "\n",
        "        if len(embeddings) < n_clusters:\n",
        "            print(f\"⚠️ Not enough data to recluster ({len(embeddings)} prompts < {n_clusters} clusters).\")\n",
        "            return\n",
        "\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "        new_clusters = {i: {\"prompts\": [], \"centroid\": None} for i in range(n_clusters)}\n",
        "\n",
        "        for idx, label in enumerate(labels):\n",
        "            new_clusters[label][\"prompts\"].append(prompts[idx])\n",
        "\n",
        "        for i in range(n_clusters):\n",
        "            cluster_prompts = new_clusters[i][\"prompts\"]\n",
        "            cluster_embeddings = [\n",
        "                embeddings[idx] for idx in range(len(labels)) if labels[idx] == i\n",
        "            ]\n",
        "            if cluster_embeddings:\n",
        "                centroid = np.mean(cluster_embeddings, axis=0)\n",
        "                new_clusters[i][\"centroid\"] = centroid\n",
        "\n",
        "                # Identify best-scored prompt\n",
        "                best_prompt = None\n",
        "                best_score = -1\n",
        "                for p in cluster_prompts:\n",
        "                    score = score_prompt(p)\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_prompt = p\n",
        "\n",
        "                new_clusters[i][\"best_prompt\"] = best_prompt\n",
        "                new_clusters[i][\"best_score\"] = best_score\n",
        "\n",
        "        self.prompt_clusters = list(new_clusters.values())\n",
        "        print(f\"🔄 Reclustered prompts into {n_clusters} clusters.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # --------------------\n",
        "    # 11.2. Runner Initialization\n",
        "    # --------------------\n",
        "\n",
        "    def __init__(self, agents, external_db=None):\n",
        "        self.agents = agents\n",
        "        self.prompt_clusters = []\n",
        "\n",
        "        self.external_db = external_db or {}\n",
        "\n",
        "        # ── AUTO-LOAD prompt clusters (stored on disk) ──\n",
        "        self.prompt_clusters = []\n",
        "        self.output_clusters = []  # Stores clustered outputs with centroids and metadata\n",
        "\n",
        "\n",
        "\n",
        "        # ── Global prompt & mission tracking ─────────────────────\n",
        "        self.last_mission_prompt = None       # the last full prompt seen\n",
        "        self.global_mission_key   = None      # hash key of best global output\n",
        "        self.global_output        = None      # the best global output text\n",
        "        self.global_round        = 1         # global round counter\n",
        "        self.global_time_log      = []        # timestamps of global validations\n",
        "\n",
        "        # ── Metrics & scenarios logs ────────────────────────────\n",
        "        self.scenario_log        = []\n",
        "        self.mission_metrics_log = []\n",
        "\n",
        "        # ── Per-agent state trackers ────────────────────────────\n",
        "        self.coop_decision_counters = {agent.name: 1     for agent in agents}\n",
        "        self.local_validated_agents  = {agent.name: False for agent in agents}\n",
        "\n",
        "        # ── Initialize each agent’s own local counters & logs ──\n",
        "        for agent in self.agents:\n",
        "            agent.local_time_log     = []\n",
        "            agent.local_interval_log = []\n",
        "            agent.local_round       = 1\n",
        "            agent.interval      = 1\n",
        "\n",
        "        # ── Penalty & decay ─────────────────────────────────────\n",
        "        self.penalty_tracker = {}  # maps prompt_key → (current_penalty, decay_counter)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # -----------------------------\n",
        "    # 11.3. Execution Loop & Orchestration\n",
        "    # -----------------------------\n",
        "\n",
        "    def maybe_update_stagnation_counter(self, agent, meaningfulness_score, efficiency_score):\n",
        "        \"\"\"\n",
        "        Updates the stagnation counter for the agent if both efficiency and meaningfulness are low.\n",
        "        Returns True if stagnation is detected, False otherwise.\n",
        "        \"\"\"\n",
        "        stagnation_detected = False\n",
        "\n",
        "        if efficiency_score < 0.5 and meaningfulness_score < 0.5:\n",
        "            agent.stagnation_counter += 1\n",
        "            stagnation_detected = True\n",
        "            print(f\"⚠️ {agent.name} stagnation counter incremented → {agent.stagnation_counter}\")\n",
        "        else:\n",
        "            if agent.stagnation_counter > 0:\n",
        "                print(f\"✅ {agent.name} recovered from stagnation.\")\n",
        "            agent.stagnation_counter = 0\n",
        "\n",
        "        return stagnation_detected\n",
        "\n",
        "\n",
        "    def execute(self, mission_prompt, verbose=False):\n",
        "        promote = False\n",
        "\n",
        "        # ✅ Step 0: Process and cache prompt before execution\n",
        "        prompt_key, embedding = self.process_mission_prompt(mission_prompt, verbose=True)\n",
        "\n",
        "\n",
        "        # ── 1. Early sanity check for a garbled prompt ──\n",
        "        meaningless, sim = is_meaningless_output(mission_prompt, mission_prompt)\n",
        "        if meaningless:\n",
        "            print(f\"⚠️ Your prompt appears meaningless (sim={sim:.2f}). Please provide a new mission prompt.\")\n",
        "            return False\n",
        "\n",
        "        # ── 2. Global-prompt similarity & new-vs-same mission logic ──\n",
        "        reactivated = any(getattr(agent, \"variance_history\", {}).get(\"reactivation_flag\", False) for agent in self.agents)\n",
        "        phase_set = {getattr(agent, \"variance_history\", {}).get(\"phase\", \"stable\") for agent in self.agents}\n",
        "\n",
        "        if reactivated or \"inflection\" in phase_set or \"post-inflection\" in phase_set:\n",
        "            print(\"🔁 Reuse filtering triggered — environment volatile or novel.\")\n",
        "            best_similarity = 0.0     # Pretend nothing is similar\n",
        "            threshold = 1.1           # Effectively disable reuse threshold\n",
        "        else:\n",
        "            best_similarity = self.runner_similarity_to_db(mission_prompt)\n",
        "            threshold = (\n",
        "                self.agents[0].meta_parameters.get(\"reuse_similarity_threshold\", 0.75)\n",
        "                if self.agents else 0.75\n",
        "            )\n",
        "\n",
        "        print(f\"📈 Best similarity found: {best_similarity:.4f} vs reuse threshold: {threshold:.4f}\")\n",
        "        is_new_prompt = best_similarity < threshold\n",
        "\n",
        "        if best_similarity >= threshold:\n",
        "            print(\"🔁 Similar mission prompt detected — continuing global round.\")\n",
        "        else:\n",
        "            print(\"🆕 New mission prompt detected — resetting global counter.\")\n",
        "            self.global_round = 1\n",
        "            self.global_mission_key = None\n",
        "            self.global_time_log.clear()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # ── 3. Central Counter Reset (Stage 3 setup) ──\n",
        "        MEANING_THRESHOLD = 0.65\n",
        "        self.meaning_validated = {a.name: False for a in self.agents}\n",
        "        self.local_validated_agents = {a.name: False for a in self.agents}\n",
        "\n",
        "        for ag in self.agents:\n",
        "            ag.local_round        = 1\n",
        "            ag.interval           = 1\n",
        "            self.coop_decision_counters[ag.name] = 1\n",
        "            ag.local_time_log     = []\n",
        "            ag.local_interval_log = []\n",
        "\n",
        "        print(\"\\n---- Interval 1 (mission start) ----\")\n",
        "        for ag in self.agents:\n",
        "            print(f\" • {ag.name}: local_round={ag.local_round}, interval={ag.interval}\")\n",
        "\n",
        "        # 4. Metric selection (now per-agent, adaptive)\n",
        "        for ag in self.agents:\n",
        "            ag.assign_metrics_from_prompt(\n",
        "                prompt=mission_prompt,\n",
        "                top_k=3,\n",
        "                verbose=verbose\n",
        "            )\n",
        "\n",
        "        # Optionally: aggregate all metrics for mission-level log\n",
        "        from collections import Counter\n",
        "        all_metrics = [m for ag in self.agents for m in ag.metrics]\n",
        "        mission_metrics = [m for m, _ in Counter(all_metrics).most_common(3)]\n",
        "\n",
        "        # ── 5. SEED INITIAL ROLES & TASKS for Interval 1 ──\n",
        "        for ag in self.agents:\n",
        "            ag.assign_roles_from_prompt(\n",
        "                prompt=self.last_mission_prompt,\n",
        "                top_k=3,\n",
        "                verbose=True\n",
        "            )\n",
        "            ag.reevaluate_roles(\n",
        "                self.last_mission_prompt,\n",
        "                efficiency_threshold=ag.meta_parameters[\"strategy_fit\"],\n",
        "                verbose=True\n",
        "            )\n",
        "            ag.role_task_map = {}\n",
        "            for role in ag.roles:\n",
        "                ag.role_task_map[role] = ROLE_TASK_MAP.get(role, []).copy()\n",
        "            ag.assign_tasks_from_roles_multi_round(\n",
        "                prompt=self.last_mission_prompt,\n",
        "                global_round=self.global_round,\n",
        "                allow_jollycard=True,\n",
        "                verbose=True\n",
        "            )\n",
        "\n",
        "        print(\"\\n🔩 Seeded initial roles/tasks for Interval 1\")\n",
        "\n",
        "        for ag in self.agents:\n",
        "            print(f\" • {ag.name}: roles={ag.roles}, tasks={ag.tasks}\")\n",
        "            print(\"\\n📦 Dynamic task_store contents after initial seed (per agent):\")\n",
        "            for ag2 in self.agents:\n",
        "                print(f\" • {ag2.name}.task_store:\")\n",
        "                if not ag2.task_store:\n",
        "                    print(\"     (empty)\")\n",
        "                else:\n",
        "                    for key, entry in ag2.task_store.items():\n",
        "                        desc = entry.get(\"desc\", \"<no desc>\")\n",
        "                        score = entry.get(\"last_score\", 0.0)\n",
        "                        print(f\"     • {key!r}: last_score={score:.3f}, desc='{desc[:40]}…'\")\n",
        "\n",
        "        # ── 6. Reset per-mission counters & initialize Stage-flags ──\n",
        "        # (This is somewhat redundant with above, but ensures fresh state.)\n",
        "        self.meaning_validated = {agent.name: False for agent in self.agents}\n",
        "        self.local_validated_agents = {agent.name: False for agent in self.agents}\n",
        "        for agent in self.agents:\n",
        "            agent.local_round        = 1\n",
        "            agent.interval           = 1\n",
        "            self.coop_decision_counters[agent.name] = 1\n",
        "            agent.local_interval_log = []\n",
        "\n",
        "        # ── 7. Main agent loop — repeat until at least one local validation ──\n",
        "        local_validated = []\n",
        "        interval_cycles = 0\n",
        "\n",
        "        while not local_validated:\n",
        "            interval_cycles += 1\n",
        "            # ✅ Reset rerouting flags for this interval\n",
        "            for agent in self.agents:\n",
        "                agent.has_been_rerouted = False\n",
        "\n",
        "            if interval_cycles > 10:\n",
        "                print(\"⚠️  Aborting after 10 unvalidated local cycles.\")\n",
        "                break\n",
        "\n",
        "            current_iv = self.agents[0].interval\n",
        "            print(f\"\\n==== Interval {current_iv} ====\")\n",
        "\n",
        "\n",
        "            meaningful_found_this_interval = False\n",
        "\n",
        "\n",
        "            # ── Start-of-interval physiology snapshot (x = k–1)\n",
        "            for ag in self.agents:\n",
        "                ag.attention_history.append(ag.compute_attention())\n",
        "                ag.fatigue_history.append(ag.compute_fatigue())\n",
        "                ag.hunger_history.append(ag.compute_hunger())\n",
        "\n",
        "            local_outputs = []\n",
        "            scenario_tally = {}\n",
        "\n",
        "            # ── 7.1. Loop over each agent, in full (repeat until at least one local validation)\n",
        "            for agent in self.agents:\n",
        "                # A-1) Role/task logic\n",
        "                if agent.interval == 1:\n",
        "                    print(f\"🔒 {agent.name} retains roles/tasks: {agent.roles} → {agent.tasks}\")\n",
        "                else:\n",
        "                    agent.assign_roles_from_prompt(\n",
        "                        prompt=mission_prompt,\n",
        "                        top_k=3,\n",
        "                        verbose=True\n",
        "                    )\n",
        "                    agent.reevaluate_roles(\n",
        "                        prompt=mission_prompt,\n",
        "                        efficiency_threshold=agent.meta_parameters[\"strategy_fit\"],\n",
        "                        verbose=True\n",
        "                    )\n",
        "                    agent.assign_tasks_from_roles_multi_round(\n",
        "                        prompt=mission_prompt,\n",
        "                        global_round=self.global_round + 1,\n",
        "                        allow_jollycard=True,\n",
        "                        verbose=True\n",
        "                    )\n",
        "                    print(f\"🔁 {agent.name} reassigned → {agent.roles} → {agent.tasks}\")\n",
        "\n",
        "                # A-2) Fallback if still no roles/tasks\n",
        "                if not agent.roles or not agent.tasks:\n",
        "                    print(f\"⚠️ {agent.name} has no roles/tasks → fallback.\")\n",
        "                    agent.fallback_choose_role_tasks(POSSIBLE_ROLES, POSSIBLE_TASKS, verbose=True)\n",
        "                    print(f\"🔄 {agent.name} fallback → {agent.roles} → {agent.tasks}\")\n",
        "\n",
        "                # A-3) Persist role_task_map pruning from before\n",
        "                if hasattr(agent, \"role_task_map\"):\n",
        "                    for role, tasks in agent.role_task_map.items():\n",
        "                        agent.role_task_map[role] = [t for t in tasks if t in agent.tasks]\n",
        "\n",
        "\n",
        "                # A-4) Redundancy filters & skip\n",
        "                redundancy_reason = None\n",
        "\n",
        "                if (\n",
        "                    agent.in_cooperation and any(\n",
        "                        RedundancyFilter.is_content_redundant(agent, t, mission_prompt, self.agents)\n",
        "                        for t in agent.tasks)\n",
        "                ):\n",
        "                    redundancy_reason = \"content-level redundancy\"\n",
        "\n",
        "                elif any(\n",
        "                    RedundancyFilter.is_task_redundant(\n",
        "                        agent, t, prompt=mission_prompt,\n",
        "                        role=(agent.roles[0] if agent.roles else None),\n",
        "                        role_aware=True)\n",
        "                    for t in agent.tasks\n",
        "                ):\n",
        "                    redundancy_reason = \"task-level redundancy\"\n",
        "\n",
        "                elif RedundancyFilter.is_prompt_redundant(mission_prompt, self.external_db, agent):\n",
        "                    redundancy_reason = \"prompt-level redundancy\"\n",
        "\n",
        "                elif hasattr(agent, \"last_graph_embedding\") and RedundancyFilter.is_graph_redundant(\n",
        "                    agent.last_graph_embedding, graph_index, graph_key_map\n",
        "                ):\n",
        "                    redundancy_reason = \"causal graph redundancy\"\n",
        "\n",
        "                if redundancy_reason:\n",
        "                    print(f\"🛑 {agent.name} skipped due to {redundancy_reason}.\")\n",
        "                    continue\n",
        "\n",
        "                # Skip if no roles or tasks\n",
        "                if not agent.roles or not agent.tasks:\n",
        "                    print(f\"⏩ {agent.name} skipped act() — no roles/tasks.\")\n",
        "                    agent.increment_local_round(reason=\"validated\")\n",
        "                    continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # A-5) ACT → get output, raw physio (x = k-1 start)\n",
        "                output, score, attention, fatigue, hunger, *_ = agent.act(\n",
        "                    mission_prompt, verbose=True\n",
        "                )\n",
        "\n",
        "\n",
        "                # A-5.5) Inline gating based on internal metrics — PRE-LLM CALL\n",
        "                metrics = agent.compute_internal_metrics(mission_prompt, output=None)\n",
        "\n",
        "\n",
        "                # ── STEP 5: Inline Role Gating Sensitivity ──────────────────────────────\n",
        "                phase = getattr(agent, \"variance_history\", {}).get(\"phase\", \"stable\")\n",
        "                reactivation = getattr(agent, \"variance_history\", {}).get(\"reactivation_flag\", False)\n",
        "\n",
        "                # Use base thresholds as fallback\n",
        "                base_thresholds = agent.meta_parameters.get(\"inline_thresholds\", {})\n",
        "                adjusted_thresholds = {}\n",
        "\n",
        "                for role, base in base_thresholds.items():\n",
        "                    if phase in (\"inflection\", \"post-inflection\") or reactivation:\n",
        "                        # Soften threshold → encourage activation\n",
        "                        adjusted = max(base * 0.85, 0.1)\n",
        "                    elif phase in (\"stable\", \"pre-inflection\"):\n",
        "                        # Tighten threshold → conserve resources\n",
        "                        adjusted = min(base * 1.15, 0.9)\n",
        "                    else:\n",
        "                        adjusted = base\n",
        "                    adjusted_thresholds[role] = round(adjusted, 3)\n",
        "\n",
        "                # Store or override\n",
        "                agent.meta_parameters[\"inline_thresholds_adaptive\"] = adjusted_thresholds\n",
        "\n",
        "\n",
        "                # Gate roles using the adjusted thresholds\n",
        "                inline_roles = gate_inline_roles(\n",
        "                    agent,\n",
        "                    metrics,\n",
        "                    adjusted_thresholds,\n",
        "                    ROLE_METRIC_MAP\n",
        "                )\n",
        "\n",
        "\n",
        "                agent.inline_activated_roles = inline_roles\n",
        "\n",
        "\n",
        "                if inline_roles:  # only store if something actually activated\n",
        "                    agent.update_long_memory(\n",
        "                        prompt=mission_prompt,\n",
        "                        output=output,\n",
        "                        score=score,\n",
        "                        signals=signals,\n",
        "                        prompt_key=self.generate_prompt_key(mission_prompt),\n",
        "                        verbose=False\n",
        "\n",
        "\n",
        "\n",
        "                # Modify prompt inline based on active roles\n",
        "                # Modify prompt inline based on active roles\n",
        "                role_instructions = {\n",
        "                    \"Refiner\":  \"Refine your response carefully.\",\n",
        "                    \"Validator\": \"Validate your answer as you generate it.\",\n",
        "                    \"Executor\": \"Include logic or code execution details.\"\n",
        "                }\n",
        "\n",
        "                prefix_lines = [role_instructions[r] for r in inline_roles if r in role_instructions]\n",
        "                prefix_block = \"\\n\".join(prefix_lines)\n",
        "\n",
        "                final_prompt = f\"{prefix_block}\\n\\nOriginal prompt:\\n\\n{mission_prompt}\" if prefix_block else mission_prompt\n",
        "\n",
        "                if verbose:\n",
        "                    print(f\"🎯 {agent.name} inline roles active → {inline_roles}\")\n",
        "                    print(f\"📝 Modified prompt:\\n{final_prompt[:300]}...\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # A-6) Generate the output (single LLM call, now with inline role effects)\n",
        "\n",
        "                # Start from defaults\n",
        "                llm_kwargs = {\n",
        "                    \"temperature\": 0.5,\n",
        "                    \"top_p\": 1.0\n",
        "                }\n",
        "\n",
        "                # Apply inline role overrides conservatively\n",
        "                for role in inline_roles:\n",
        "                    if role in INLINE_ROLE_BEHAVIOR:\n",
        "                        for k, v in INLINE_ROLE_BEHAVIOR[role].items():\n",
        "                            llm_kwargs[k] = min(llm_kwargs.get(k, 1.0), v)\n",
        "\n",
        "                # Call LLM\n",
        "                output = llm_generate(\n",
        "                    model_key=agent.model_key,\n",
        "                    prompt=final_prompt,\n",
        "                    **llm_kwargs\n",
        "                ).strip()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # ✅ Optional: Reassign tasks based on updated roles\n",
        "                agent.detect_and_resolve_role_task_conflict(mission_prompt, verbose=True)\n",
        "\n",
        "\n",
        "                # Append end-of-interval physiology snapshot (x = k)\n",
        "                agent.attention_history.append(attention)\n",
        "                agent.fatigue_history.append(fatigue)\n",
        "                agent.hunger_history.append(hunger)\n",
        "\n",
        "\n",
        "\n",
        "                # Compute true efficiency\n",
        "                efficiency_score = round(\n",
        "                    score * attention /\n",
        "                    (1 + 0.1 * agent.interval\n",
        "                      + 0.5 * agent.meaningless_output_counter\n",
        "                      + 0.3 * agent.external_access_count),\n",
        "                    3\n",
        "                )\n",
        "                print(\n",
        "                    f\"🚀 {agent.name} act() → \"\n",
        "                    f\"Score={score:.3f}, Eff={efficiency_score:.3f}, \"\n",
        "                    f\"att={attention:.3f}, fat={fatigue:.3f}, hung={hunger:.3f}\"\n",
        "                )\n",
        "\n",
        "                # --- Log interval scores and state for this agent ---\n",
        "                agent.interval_score_log.append({\n",
        "                    \"efficiency\":      efficiency_score,\n",
        "                    \"meaningfulness\":  score,\n",
        "                    \"attention\":       attention,\n",
        "                    \"fatigue\":         fatigue,\n",
        "                    \"hunger\":          hunger,\n",
        "                    \"scenario\":        scene,  # scenario label from compute_foresight()\n",
        "                    \"roles\":           agent.roles[:],        # copy current roles\n",
        "                    \"tasks\":           agent.tasks[:],        # copy current tasks\n",
        "                    \"features\":        agent.features[:],     # copy current features\n",
        "                    \"cooperation\":     agent.in_cooperation,  # True/False\n",
        "                    \"interval\":        agent.interval,        # optional: interval counter\n",
        "                    \"external_access\": agent.external_access_count,  # optional: how many external calls this interval\n",
        "                    \"uvr_similarity\": signals.get(\"uvr_similarity\"),\n",
        "                    \"output_similarity_variance\": signals.get(\"output_similarity_variance\")\n",
        "                })\n",
        "\n",
        "\n",
        "                # Dropout & pruning\n",
        "                agent.apply_dropout(mission_prompt, verbose=True)\n",
        "                agent.apply_pruning(mission_prompt, verbose=True)\n",
        "                if hasattr(agent, \"role_task_map\"):\n",
        "                    for role, tasks in agent.role_task_map.items():\n",
        "                        agent.role_task_map[role] = [t for t in tasks if t in agent.tasks]\n",
        "\n",
        "                # Mid-interval physiology snapshot (x = k-0.5)\n",
        "                att_mid  = agent.compute_attention()\n",
        "                fat_mid  = agent.compute_fatigue()\n",
        "                hung_mid = agent.compute_hunger()\n",
        "                agent.attention_history.append(att_mid)\n",
        "                agent.fatigue_history.append(fat_mid)\n",
        "                agent.hunger_history.append(hung_mid)\n",
        "                if verbose:\n",
        "                    print(f\"[MID] {agent.name}  att={att_mid:.3f}, fat={fat_mid:.3f}, hung={hung_mid:.3f}\")\n",
        "\n",
        "                # Dropout & pruning again (for mid-interval)\n",
        "                agent.apply_dropout(mission_prompt, verbose=True)\n",
        "                agent.apply_pruning(mission_prompt, verbose=True)\n",
        "                if hasattr(agent, \"role_task_map\"):\n",
        "                    for role, tasks in agent.role_task_map.items():\n",
        "                        agent.role_task_map[role] = [t for t in tasks if t in agent.tasks]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # Stagnation tracking & recovery\n",
        "                stagnated = self.maybe_update_stagnation_counter(\n",
        "                    agent,\n",
        "                    meaningfulness_score=1.0,  # TODO: real meaningfulness if needed\n",
        "                    efficiency_score=efficiency_score\n",
        "                )\n",
        "                self.maybe_trigger_recovery(agent)\n",
        "\n",
        "                # Stage 1: Let agent decide if this interval is meaningful\n",
        "                if not self.meaning_validated[agent.name]:\n",
        "                    validated = agent.evaluate_interval(score, threshold=MEANING_THRESHOLD, verbose=True)\n",
        "                    if validated:\n",
        "                        self.meaning_validated[agent.name] = True\n",
        "                        meaningful_found_this_interval = True\n",
        "\n",
        "\n",
        "                # Immediate global promotion for new prompts\n",
        "                if is_new_prompt:\n",
        "                    promote = True\n",
        "                    best = {\n",
        "                        \"agent\":      agent,\n",
        "                        \"score\":      score,\n",
        "                        \"efficiency\": efficiency_score\n",
        "                    }\n",
        "                    print(f\"⏹️ {agent.name} is first to validate → ending global loop early.\")\n",
        "\n",
        "                    # Store in external_db\n",
        "                    raw_emb = embedding_model.encode(self.last_mission_prompt)\n",
        "                    if raw_emb is None:\n",
        "                        emb = np.zeros(EMB_DIM)\n",
        "                    elif isinstance(raw_emb, list):\n",
        "                        emb = np.array(raw_emb)\n",
        "                    else:\n",
        "                        emb = raw_emb\n",
        "                    # ── STEP 1: Injection Control for external DB ───────────────────────────────\n",
        "                    allow_injection = False\n",
        "                    phase = getattr(agent, \"variance_history\", {}).get(\"phase\", \"stable\")\n",
        "                    reactivation = getattr(agent, \"variance_history\", {}).get(\"reactivation_flag\", False)\n",
        "\n",
        "                    if phase in (\"inflection\", \"post-inflection\") or reactivation:\n",
        "                        allow_injection = True\n",
        "\n",
        "                    if not allow_injection:\n",
        "                        print(f\"🚫 {agent.name} skipping external DB injection — phase={phase}, reactivation={reactivation}\")\n",
        "                    else:\n",
        "                        print(f\"✅ {agent.name} external DB injection allowed — phase={phase}, reactivation={reactivation}\")\n",
        "\n",
        "                if allow_injection:\n",
        "\n",
        "                    prompt_key = self.last_mission_key\n",
        "\n",
        "\n",
        "                    self.external_db[prompt_key] = {\n",
        "                        \"output\": output,\n",
        "                        \"score\": score,\n",
        "                        \"metrics\": mission_metrics,\n",
        "                        \"embedding\": emb if isinstance(emb, np.ndarray) else np.array(emb),\n",
        "                        \"prompt_key\": prompt_key,\n",
        "                        \"original_prompt\": mission_prompt,\n",
        "                        \"timestamp\": time.time(),\n",
        "                        \"usage_count\": 0,\n",
        "\n",
        "                        # Causal graph\n",
        "                        \"graph_embedding\": (\n",
        "                            agent.last_graph_embedding.tolist()\n",
        "                            if hasattr(agent, \"last_graph_embedding\") else None\n",
        "                        ),\n",
        "                        \"graph_text\": (\n",
        "                            agent.last_graph_text\n",
        "                            if hasattr(agent, \"last_graph_text\") else None\n",
        "                        ),\n",
        "\n",
        "                        # Agent state snapshot\n",
        "                        \"attention\": attention,\n",
        "                        \"fatigue\": fatigue,\n",
        "                        \"hunger\": hunger,\n",
        "                        \"roles\": agent.roles[:],\n",
        "                        \"tasks\": agent.tasks[:],\n",
        "                        \"features\": agent.features[:],\n",
        "                        \"inline_activated_roles\": agent.inline_activated_roles[:],\n",
        "\n",
        "                        # Reasoning context\n",
        "                        \"role_task_map\": deepcopy(agent.role_task_map),\n",
        "                        \"role_metric_map\": deepcopy(agent.role_metric_map),\n",
        "\n",
        "                        \"uvr_similarity\": signals.get(\"uvr_similarity\"),\n",
        "                        \"output_similarity_variance\": signals.get(\"output_similarity_variance\")\n",
        "                    }\n",
        "\n",
        "\n",
        "\n",
        "                    # ✅ Save to disk\n",
        "                    self.save_external_db(\"external_db.json\")\n",
        "\n",
        "\n",
        "                    self.global_time_log.append(time.time())\n",
        "                    print(f\"\\n🥇 GLOBAL PROMOTED (g={self.global_round})\")\n",
        "                    plot_agent_physiology(self)\n",
        "                    plot_real_timelines_with_shading(self)\n",
        "                    self.global_round += 1\n",
        "                    self.reset_all()\n",
        "                    return True\n",
        "\n",
        "\n",
        "                # Stage 2: Local mission validation (delegated to agent)\n",
        "                if self.meaning_validated[agent.name] and not self.local_validated_agents[agent.name]:\n",
        "                    validated = agent.evaluate_local_mission(\n",
        "                        mission_key=self.last_mission_prompt,\n",
        "                        score=score,\n",
        "                        output=output,\n",
        "                        signals=signals,\n",
        "                        prompt=mission_prompt,\n",
        "                        verbose=verbose\n",
        "                    )\n",
        "                    if validated:\n",
        "                        self.local_validated_agents[agent.name] = True\n",
        "                        # Cluster the validated output\n",
        "                        self.cluster_output(\n",
        "                            output_text=output,\n",
        "                            task=agent.tasks[0] if agent.tasks else None,\n",
        "                            prompt_key=self.last_mission_prompt,\n",
        "                            score=score,  # ✅ score from evaluate_local_mission\n",
        "                            verbose=verbose\n",
        "                        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                        print(f\"\\n📈 {agent.name} physiology this local round:\")\n",
        "                        plot_agent_physiology(self, agents=[agent])\n",
        "                        break\n",
        "\n",
        "                # Tally & metrics collection\n",
        "                scene = agent.compute_foresight(\n",
        "                    mission_prompt,\n",
        "                    path_reuse=False,\n",
        "                    similarity_score=score,\n",
        "                    external_db=self.external_db\n",
        "                )\n",
        "\n",
        "                agent.signal_spike_log.append({\n",
        "                    \"timestamp\": time.time(),\n",
        "                    \"agent\": agent.name,\n",
        "                    \"roles\": agent.roles[:],\n",
        "                    \"output_similarity_variance\": signals.get(\"output_similarity_variance\"),\n",
        "                    \"inflection_phase\": getattr(agent, \"variance_history\", {}).get(\"phase\", \"unknown\"),\n",
        "                    \"reactivation_flag\": getattr(agent, \"variance_history\", {}).get(\"reactivation_flag\", False),\n",
        "                    \"scenario\": scene  # ← Add the foresight scenario label here\n",
        "                })\n",
        "\n",
        "                # ── SIGNAL-BASED REROUTING TRIGGER ──────────────────────\n",
        "                latest_signal = agent.signal_spike_log[-1] if agent.signal_spike_log else None\n",
        "\n",
        "\n",
        "                if latest_signal and agent.is_signal_clear(latest_signal):\n",
        "\n",
        "                    if agent.has_been_rerouted:\n",
        "                        print(f\"⚠️  {agent.name} has already rerouted this interval → skipping.\")\n",
        "                    else:\n",
        "                        print(f\"🔁 {agent.name} detected a clear rerouting signal → triggering strategy reset.\")\n",
        "                        agent.has_been_rerouted = True\n",
        "\n",
        "                        severity = \"severe\" if latest_signal.get(\"cluster_tag\") == \"strategy collapse\" else \"moderate\"\n",
        "                        agent.reroute_strategy(severity=severity, verbose=True)\n",
        "                        agent.reset_local_state()\n",
        "\n",
        "                    continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                scenario_tally[scene] = scenario_tally.get(scene, 0) + 1\n",
        "                local_outputs.append({\n",
        "                    \"agent\": agent,\n",
        "                    \"output\": output,\n",
        "                    \"score\": score,\n",
        "                    \"attention\": attention,\n",
        "                    \"fatigue\": fatigue,\n",
        "                    \"hunger\": hunger,\n",
        "                    \"scenario\": scene\n",
        "                })\n",
        "\n",
        "\n",
        "            if not meaningful_found_this_interval:\n",
        "                old_thresh = MEANING_THRESHOLD\n",
        "                MEANING_THRESHOLD = max(MEANING_THRESHOLD - 0.05, 0.1)\n",
        "                print(f\"⬇️  MEANING_THRESHOLD reduced: {old_thresh:.2f} → {MEANING_THRESHOLD:.2f}\")\n",
        "\n",
        "\n",
        "            # BREAK OUT if someone already validated\n",
        "            if any(self.local_validated_agents.values()):\n",
        "                break\n",
        "\n",
        "            # 4.2) Print scenario tally & handle cooperation\n",
        "            if not is_new_prompt and not promote:\n",
        "                candidates = [r for r in local_outputs if r[\"score\"] >= MEANING_THRESHOLD]\n",
        "                if candidates:\n",
        "                    old = self.external_db[self.global_mission_key]\n",
        "                    old_mean = old[\"metrics\"][\"meaningfulness\"]\n",
        "                    old_eff  = old[\"metrics\"][\"efficiency\"]\n",
        "                    valid = [\n",
        "                        r for r in candidates\n",
        "                        if (r[\"score\"] > old_mean and r[\"efficiency\"] >= old_eff)\n",
        "                          or (r[\"score\"] == old_mean and r[\"efficiency\"] > old_eff)\n",
        "                    ]\n",
        "                    if valid:\n",
        "                        best = max(valid, key=lambda r: (r[\"score\"], r[\"efficiency\"]))\n",
        "                        promote = True\n",
        "                        print(f\"⏹️ {best['agent'].name} wins repeat‐prompt promotion at interval {current_iv}.\")\n",
        "                        break\n",
        "\n",
        "            if not promote:\n",
        "                print(\"\\n📊 Scenario tally across agents:\")\n",
        "                for scen, cnt in scenario_tally.items():\n",
        "                    print(f\" - {scen}: {cnt}\")\n",
        "                self.handle_cooperation(scenario_tally)\n",
        "\n",
        "            # ── (Optional) Show UVR spikes after this round ──\n",
        "            for agent in self.agents:\n",
        "                if hasattr(agent, \"signal_spike_log\") and agent.signal_spike_log:\n",
        "                    print(f\"\\n📌 Spike log for {agent.name}:\")\n",
        "                    for spike in agent.signal_spike_log:\n",
        "                        print(f\"  ⏱️ [{spike['timestamp']:.0f}] Phase={spike['inflection_phase']}, \"\n",
        "                              f\"Roles={spike['roles']}, ΔVar={spike['output_similarity_variance']:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "            # 4.3) Log scenario tally\n",
        "            self.scenario_log.append({\n",
        "                \"global_round\": self.global_round + 1,\n",
        "                \"scenario_tally\": dict(scenario_tally)\n",
        "            })\n",
        "\n",
        "\n",
        "            # 4.3b) Run spike clustering and scenario-cluster correlation\n",
        "            self.cluster_signal_spikes()           # Step 2\n",
        "            self.build_scenario_cluster_map()      # Step 3\n",
        "\n",
        "            # 4.3c) Save scenario–cluster co-occurrence map\n",
        "            self.save_scenario_cluster_map(\"scenario_cluster_map.json\")\n",
        "\n",
        "\n",
        "            # 4.4) Local validation check & logging\n",
        "            local_validated = []\n",
        "            for entry in local_outputs:\n",
        "                agent, sc = entry[\"agent\"], entry[\"score\"]\n",
        "                if sc >= MEANING_THRESHOLD:\n",
        "                    local_validated.append(agent)\n",
        "                    self.local_validated_agents[agent.name] = True\n",
        "                    print(f\"✅ {agent.name} validated locally (score={sc:.2f})\")\n",
        "                else:\n",
        "                    self.local_validated_agents[agent.name] = False\n",
        "\n",
        "            for entry in local_outputs:\n",
        "                ag = entry[\"agent\"]\n",
        "                self.mission_metrics_log.append({\n",
        "                    \"global_round\": self.global_round + 1,\n",
        "                    \"agent\":        ag.name,\n",
        "                    \"score\":        entry[\"score\"],\n",
        "                    \"validated\":    self.local_validated_agents[ag.name],\n",
        "                    \"metrics\":      mission_metrics,\n",
        "                    \"scenario\":     entry[\"scenario\"],\n",
        "                    \"timestamp\":    time.time()\n",
        "                })\n",
        "\n",
        "        # ── end while not local_validated ──\n",
        "\n",
        "        # --- LOCAL ROUND END: Update short-term meta-parameters and logs for EACH AGENT ---\n",
        "        for agent in self.agents:\n",
        "            intervals = getattr(agent, \"interval_score_log\", [])\n",
        "            if intervals:  # Only if there was at least one interval\n",
        "                avg_eff = np.mean([x[\"efficiency\"] for x in intervals])\n",
        "                avg_mean = np.mean([x[\"meaningfulness\"] for x in intervals])\n",
        "                avg_attn = np.mean([x[\"attention\"] for x in intervals])\n",
        "\n",
        "                # Get the mode scenario label for the local round\n",
        "                scenario_mode = get_local_round_scenario_mode(agent.local_round_score_log)\n",
        "                foresight_signal = {\"scenario\": scenario_mode}\n",
        "\n",
        "                # Update short-term meta-parameters\n",
        "                agent.update_short_term_meta_parameters(\n",
        "                    foresight_signal=foresight_signal,\n",
        "                    avg_efficiency_short_term=avg_eff,\n",
        "                    avg_meaningfulness_short_term=avg_mean,\n",
        "                    attention_score=avg_attn,\n",
        "                    interval_score_log=agent.interval_score_log\n",
        "                )\n",
        "\n",
        "                # Log/clear interval scores for this local round\n",
        "                if not hasattr(agent, \"local_round_score_log\"):\n",
        "                    agent.local_round_score_log = []\n",
        "                agent.local_round_score_log.append(agent.interval_score_log[:])\n",
        "                agent.interval_score_log.clear()\n",
        "\n",
        "\n",
        "\n",
        "        # 5) Global Mission Promotion\n",
        "        validated = [\n",
        "            r for r in local_outputs\n",
        "            if self.local_validated_agents.get(r[\"agent\"].name, False)\n",
        "        ]\n",
        "        if validated:\n",
        "            best = max(validated, key=lambda r: r[\"score\"])\n",
        "            agent, score = best[\"agent\"], best[\"score\"]\n",
        "            if agent.stagnation_counter >= agent.meta_parameters.get(\n",
        "                    \"stagnation_recovery_threshold\", 5):\n",
        "                print(f\"⚡ {agent.name} stagnated → forcing global promotion.\")\n",
        "                promote = True\n",
        "            else:\n",
        "                key = self.global_mission_key\n",
        "                old = self.external_db[key][\"score\"] if key else -float(\"inf\")\n",
        "                if self.global_round == 1 or not key or agent.wants_global_promotion(score, old):\n",
        "                    promote = True\n",
        "\n",
        "        if promote:\n",
        "\n",
        "            emb = self.last_mission_embedding\n",
        "            prompt_key = self.last_mission_key\n",
        "\n",
        "\n",
        "\n",
        "            self.external_db[prompt_key] = {\n",
        "                \"output\":          best[\"output\"],\n",
        "                \"score\":           best[\"score\"],\n",
        "                \"metrics\":         mission_metrics,\n",
        "                \"embedding\":       emb,\n",
        "                \"prompt_key\": prompt_key,\n",
        "                \"original_prompt\": mission_prompt,\n",
        "                \"timestamp\":       time.time(),\n",
        "                \"usage_count\":     0,\n",
        "\n",
        "                # Agent state\n",
        "                \"attention\": agent.attention_history[-1] if agent.attention_history else None,\n",
        "                \"fatigue\": agent.fatigue_history[-1] if agent.fatigue_history else None,\n",
        "                \"hunger\": agent.hunger_history[-1] if agent.hunger_history else None,\n",
        "                \"roles\": agent.roles[:],\n",
        "                \"tasks\": agent.tasks[:],\n",
        "                \"features\": agent.features[:],\n",
        "                \"inline_activated_roles\": agent.inline_activated_roles[:],\n",
        "\n",
        "                # Causal graph info\n",
        "                \"graph_embedding\": agent.last_graph_embedding.tolist()\n",
        "                    if hasattr(agent, \"last_graph_embedding\") else None,\n",
        "                \"graph_text\": agent.last_graph_text\n",
        "                    if hasattr(agent, \"last_graph_text\") else None,\n",
        "\n",
        "                # Reasoning maps\n",
        "                \"role_task_map\": deepcopy(agent.role_task_map),\n",
        "                \"role_metric_map\": deepcopy(agent.role_metric_map)\n",
        "            }\n",
        "\n",
        "            # 🔁 Update semantic prompt clusters (canonical injection only)\n",
        "            self.update_prompt_clusters(\n",
        "                new_prompt=mission_prompt,\n",
        "                new_embedding=emb\n",
        "            )\n",
        "\n",
        "            # ✅ Auto-save updated clusters\n",
        "            self.save_prompt_clusters(\"prompt_clusters.json\")\n",
        "\n",
        "            # ✅ Save to disk\n",
        "            self.save_external_db(\"external_db.json\")\n",
        "\n",
        "            self.global_time_log.append(time.time())\n",
        "            print(f\"\\n🥇 GLOBAL PROMOTED (g={self.global_round})\")\n",
        "            self.global_round += 1\n",
        "\n",
        "            # --- Long-term meta-parameter updates for all agents ---\n",
        "            global_avg_eff = np.mean([\n",
        "                np.mean([x[\"efficiency\"] for x in getattr(agent, \"local_round_score_log\", [[]])[-1]])\n",
        "                if getattr(agent, \"local_round_score_log\", []) else 0.5\n",
        "                for agent in self.agents\n",
        "            ])\n",
        "            global_avg_mean = np.mean([\n",
        "                np.mean([x[\"meaningfulness\"] for x in getattr(agent, \"local_round_score_log\", [[]])[-1]])\n",
        "                if getattr(agent, \"local_round_score_log\", []) else 0.5\n",
        "                for agent in self.agents\n",
        "            ])\n",
        "\n",
        "            global_avg_attn = np.mean([\n",
        "                np.mean([x[\"attention\"] for x in getattr(agent, \"local_round_score_log\", [[]])[-1]])\n",
        "                if getattr(agent, \"local_round_score_log\", []) else 0.5\n",
        "                for agent in self.agents\n",
        "            ])\n",
        "\n",
        "\n",
        "            global_avg_fatigue = np.mean([\n",
        "                np.mean([x.get(\"fatigue\", 0.5) for x in getattr(agent, \"local_round_score_log\", [[]])[-1]])\n",
        "                if getattr(agent, \"local_round_score_log\", []) else 0.5\n",
        "                for agent in self.agents\n",
        "            ])\n",
        "\n",
        "            global_avg_hunger = np.mean([\n",
        "                np.mean([x.get(\"hunger\", 0.5) for x in getattr(agent, \"local_round_score_log\", [[]])[-1]])\n",
        "                if getattr(agent, \"local_round_score_log\", []) else 0.5\n",
        "                for agent in self.agents\n",
        "            ])\n",
        "\n",
        "\n",
        "\n",
        "            for agent in self.agents:\n",
        "                # --- Compute scenario mode for the global round\n",
        "                global_scenario_mode = get_global_round_scenario_mode(agent.local_round_score_log)\n",
        "                foresight_signal = {\"scenario\": global_scenario_mode}\n",
        "\n",
        "                agent.update_long_term_meta_parameters(\n",
        "                    foresight_signal=foresight_signal,\n",
        "                    avg_efficiency_long_term=global_avg_eff,\n",
        "                    avg_meaningfulness_long_term=global_avg_mean,\n",
        "                    global_avg_attn=global_avg_attn,\n",
        "                    global_avg_fatigue=global_avg_fatigue,\n",
        "                    global_avg_hunger=global_avg_hunger,\n",
        "                    verbose=True\n",
        "                )\n",
        "\n",
        "\n",
        "            plot_agent_physiology(self)\n",
        "            plot_real_timelines_with_shading(self)\n",
        "            for agent in self.agents:\n",
        "                agent.local_round        = 1\n",
        "                gent.interval           = 1\n",
        "                agent.local_time_log     = []\n",
        "                agent.local_interval_log = []\n",
        "                agent.stagnation_counter = 0\n",
        "            self.reset_all()\n",
        "\n",
        "        # 6) Abortion: no agent passed Stage 2 within the max intervals\n",
        "        if not local_validated:\n",
        "            print(\"🔴 No agent completed Stage 2 within the interval limit → aborting round.\")\n",
        "            self.global_round += 1\n",
        "\n",
        "            self.reset_all()\n",
        "\n",
        "        return promote\n",
        "\n",
        "\n",
        "    # --------------------\n",
        "    # 11.4. Stagnation Recovery\n",
        "    # --------------------\n",
        "\n",
        "    def maybe_trigger_recovery(self, agent):\n",
        "        \"\"\"\n",
        "        If an agent’s stagnation counter exceeds the recovery threshold,\n",
        "        trigger memory-based recovery to unstick it.\n",
        "        \"\"\"\n",
        "        # Fetch per-agent recovery threshold (default to 5 if not defined)\n",
        "        recovery_threshold = agent.meta_parameters.get(\"stagnation_recovery_threshold\", 5)\n",
        "\n",
        "        if agent.stagnation_counter >= recovery_threshold:\n",
        "            print(f\"🚨 {agent.name} is stuck → triggering memory-based recovery.\")\n",
        "            agent.trigger_memory_based_recovery(verbose=True)\n",
        "            agent.stagnation_counter = 0  # Reset after triggering recovery\n",
        "\n",
        "\n",
        "\n",
        "    # --------------------\n",
        "    # 11.5. Agent Timeline Plotting\n",
        "    # --------------------\n",
        "\n",
        "    def plot_agent_timelines(self):\n",
        "        \"\"\"\n",
        "        ASCII‐bar showing exactly which discrete intervals each agent validated in.\n",
        "        \"\"\"\n",
        "        print(\"\\n🕒 Agent Timelines Based on Local Validation Speed:\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # find the farthest interval any agent ever validated at\n",
        "        max_interval = max(\n",
        "            (max(agent.local_interval_log) for agent in self.agents if agent.local_interval_log),\n",
        "            default=0\n",
        "        )\n",
        "\n",
        "        for agent in self.agents:\n",
        "            line = \"\"\n",
        "            for k in range(1, max_interval + 1):\n",
        "                line += \"▓\" if k in agent.local_interval_log else \"░\"\n",
        "            print(f\"🤖 {agent.name}: {line or '(no validations yet)'}\")\n",
        "\n",
        "        print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "    # --------------------\n",
        "    # 11.6. Agent Snapshot Reporting\n",
        "    # --------------------\n",
        "    def print_agent_snapshot(self):\n",
        "        \"\"\"\n",
        "        Print a quick snapshot of each agent’s roles, tasks,\n",
        "        local round, interval counter, and recent validations.\n",
        "        \"\"\"\n",
        "        print(\"\\n🧠 Agent Snapshot:\")\n",
        "        print(\"=\" * 80)\n",
        "        for agent in self.agents:\n",
        "            print(f\"\\n🤖 {agent.name}\")\n",
        "            print(f\"  🎭 Roles:     {', '.join(agent.roles) if agent.roles else 'None'}\")\n",
        "            print(f\"  🧪 Tasks:     {', '.join(agent.tasks) if agent.tasks else 'None'}\")\n",
        "            print(f\"  🔢 Local round:   {agent.local_round}\")\n",
        "            last_interval = agent.local_interval_log[-1] if agent.local_interval_log else 0\n",
        "            print(f\"  ⏳ Last validated interval: {last_interval}\")\n",
        "\n",
        "            # How many times this agent validated locally\n",
        "            val_count = len(getattr(agent, 'local_time_log', []))\n",
        "            print(f\"  ✅ Local validations: {val_count}\")\n",
        "            # Show up to last 3 validation timestamps\n",
        "            for idx, ts in enumerate(agent.local_time_log[-3:], start=1):\n",
        "                print(f\"    {idx}. {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(ts))}\")\n",
        "            print(f\"  📚 Short mem entries: {len(agent.short_memory)}\")\n",
        "            # Show last 3 memory entries\n",
        "            for idx, entry in enumerate(agent.short_memory[-3:], start=1):\n",
        "                snip = entry.get(\"prompt\", \"\")[:60]\n",
        "                roles = entry.get(\"roles\", [])\n",
        "                roles = \", \".join(roles) if isinstance(roles, list) else roles\n",
        "                print(f\"    {idx}. [Roles: {roles or 'None'}] Prompt: {snip}...\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "\n",
        "\n",
        "    def plot_scenario_cluster_timeline(self):\n",
        "        \"\"\"\n",
        "        Plots global timeline showing scenario-cluster evolution.\n",
        "        \"\"\"\n",
        "        timestamps = []\n",
        "        scenario_labels = []\n",
        "        cluster_tags = []\n",
        "\n",
        "        # Collect all signals with both scenario + cluster tag\n",
        "        for agent in self.agents:\n",
        "            for spike in getattr(agent, \"signal_spike_log\", []):\n",
        "                if \"scenario\" in spike and \"cluster_tag\" in spike:\n",
        "                    timestamps.append(spike[\"timestamp\"])\n",
        "                    scenario_labels.append(spike[\"scenario\"])\n",
        "                    cluster_tags.append(spike[\"cluster_tag\"])\n",
        "\n",
        "        if not timestamps:\n",
        "            print(\"⚠️ No annotated scenario-cluster signals to plot.\")\n",
        "            return\n",
        "\n",
        "        # Normalize timestamps\n",
        "        min_t = min(timestamps)\n",
        "        norm_times = [t - min_t for t in timestamps]\n",
        "\n",
        "        # Encode categories to ints\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        scen_enc = LabelEncoder()\n",
        "        clus_enc = LabelEncoder()\n",
        "\n",
        "        y1 = scen_enc.fit_transform(scenario_labels)\n",
        "        y2 = clus_enc.fit_transform(cluster_tags)\n",
        "\n",
        "        cmap1 = cm.get_cmap(\"tab10\", len(scen_enc.classes_))\n",
        "        cmap2 = cm.get_cmap(\"tab20\", len(clus_enc.classes_))\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # Plot scenario labels\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.scatter(norm_times, y1, c=y1, cmap=cmap1, s=40)\n",
        "        plt.yticks(range(len(scen_enc.classes_)), scen_enc.classes_)\n",
        "        plt.xlabel(\"Time (s since first spike)\")\n",
        "        plt.title(\"⏳ Scenario Evolution\")\n",
        "\n",
        "        # Plot cluster tags\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.scatter(norm_times, y2, c=y2, cmap=cmap2, s=40)\n",
        "        plt.yticks(range(len(clus_enc.classes_)), clus_enc.classes_)\n",
        "        plt.xlabel(\"Time (s since first spike)\")\n",
        "        plt.title(\"🧠 Signal Cluster Evolution\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    # --------------------\n",
        "    # 11.7. Foresight Response\n",
        "    # --------------------\n",
        "    def respond_to_foresight(self, agent, foresight_scenario):\n",
        "        \"\"\"\n",
        "        Apply mapped behaviors for a given foresight scenario to the specified agent.\n",
        "        \"\"\"\n",
        "        behavior = FORESIGHT_BEHAVIOR_MAP.get(normalize_foresight_key(foresight_scenario))\n",
        "\n",
        "        if not behavior:\n",
        "            print(f\"⚠️ No mapped behavior for foresight scenario: {foresight_scenario}\")\n",
        "            return\n",
        "\n",
        "        response_type = behavior[\"response\"]\n",
        "        origin = \"causal\" if \"(Causal)\" in foresight_scenario else \"semantic\"\n",
        "\n",
        "        if response_type == \"reuse_best_path\":\n",
        "            print(f\"🔁 {agent.name} reusing best global path.\")\n",
        "            # No special action yet (already retrieved best)\n",
        "        elif response_type == \"force_attention_reset\":\n",
        "            print(f\"⚡ {agent.name} forcing attention reset.\")\n",
        "            agent.attention_history = [1.0]  # Full attention restored\n",
        "        elif response_type == \"prepare_adjust_weights\":\n",
        "            print(f\"🛠️ {agent.name} preparing by adjusting feature weights.\")\n",
        "            agent.meta_parameters[\"task_feature_coupling\"] = min(\n",
        "                agent.meta_parameters[\"task_feature_coupling\"] + 0.1, 1.0\n",
        "            )\n",
        "        elif response_type == \"probe_with_low_weight\":\n",
        "            print(f\"🧪 {agent.name} probing fringe path.\")\n",
        "            agent.meta_parameters[\"strategy_fit\"] = max(\n",
        "                agent.meta_parameters[\"strategy_fit\"] - 0.1, 0.1\n",
        "            )\n",
        "        elif response_type == \"promote_exploration\":\n",
        "            print(f\"🚀 {agent.name} promoting high-priority exploration.\")\n",
        "            agent.meta_parameters[\"strategy_fit\"] = min(\n",
        "                agent.meta_parameters[\"strategy_fit\"] + 0.2, 1.0\n",
        "            )\n",
        "        elif response_type == \"activate_high_entropy\":\n",
        "            print(f\"🎲 {agent.name} entering high-entropy exploration mode.\")\n",
        "            agent.in_cooperation = False  # Agent goes solo to explore\n",
        "        elif response_type == \"activate_chain_analysis\":\n",
        "            print(f\"🔗 {agent.name} activating chain analysis mode.\")\n",
        "            # You could flag for cascade-specific behavior later\n",
        "        elif response_type == \"add_redundancy\":\n",
        "            print(f\"🛡️ {agent.name} adding redundancy buffers.\")\n",
        "            agent.meta_parameters[\"task_feature_coupling\"] = min(\n",
        "                agent.meta_parameters[\"task_feature_coupling\"] + 0.15, 1.0\n",
        "            )\n",
        "\n",
        "    # --------------------\n",
        "    # 11.8. Metric Selection Simulation\n",
        "    # --------------------\n",
        "\n",
        "    def meta_learned_metric_selection(self, mission_prompt):\n",
        "        \"\"\"\n",
        "        Select metrics based on prompt similarity and adaptive threshold (no per-metric weights).\n",
        "        \"\"\"\n",
        "        prompt_vec = self.embedding_model.encode(mission_prompt)\n",
        "        sims = {}\n",
        "        for m, ref_vec in self.metric_reference_embeddings.items():\n",
        "            sim = cosine_similarity([prompt_vec], [ref_vec])[0][0]\n",
        "            sims[m] = sim\n",
        "\n",
        "        # Select metrics above adaptive threshold\n",
        "        thresh = self.meta_parameters.get(\"metric_selection_threshold\", 0.5)\n",
        "        selected_metrics = [m for m, score in sims.items() if score > thresh]\n",
        "\n",
        "        # Optionally cap at top_k_metrics\n",
        "        top_k = self.meta_parameters.get(\"top_k_metrics\", 3)\n",
        "        if len(selected_metrics) > top_k:\n",
        "            selected_metrics = sorted(selected_metrics, key=lambda m: sims[m], reverse=True)[:top_k]\n",
        "        # Fallback to most similar if none pass threshold\n",
        "        if not selected_metrics:\n",
        "            selected_metrics = [max(sims, key=sims.get)]\n",
        "        return selected_metrics\n",
        "\n",
        "\n",
        "\n",
        "    # --------------------\n",
        "    # 11.9. External Database Similarity\n",
        "    # --------------------\n",
        "\n",
        "    # --------------------\n",
        "    # 11.9. Runner-side Database Similarity\n",
        "    # --------------------\n",
        "    def runner_similarity_to_db(self, prompt):\n",
        "        \"\"\"\n",
        "        Compute semantic similarity of a given prompt to entries in runner.external_db.\n",
        "        Returns the highest similarity found.\n",
        "        \"\"\"\n",
        "        if not self.external_db:\n",
        "            return 0.0\n",
        "\n",
        "        vec = embedding_model.encode(prompt)\n",
        "        if vec is None:\n",
        "            vec = np.zeros(EMB_DIM)\n",
        "        elif isinstance(vec, list):\n",
        "            vec = np.array(vec)\n",
        "\n",
        "        best = 0.0\n",
        "        for entry in self.external_db.values():\n",
        "            stored = entry.get(\"embedding\")\n",
        "            if stored is None:\n",
        "                continue\n",
        "            if isinstance(stored, list):\n",
        "                stored = np.array(stored)\n",
        "            sim = cosine_similarity([vec], [stored])[0][0]\n",
        "            best = max(best, sim)\n",
        "        return best\n",
        "\n",
        "\n",
        "\n",
        "    # --------------------\n",
        "    # 11.10. Cooperation Handling\n",
        "    # --------------------\n",
        "    def handle_cooperation(self, scenario_tally):\n",
        "        \"\"\"\n",
        "        If multiple agents encounter critical foresight scenarios in the same round,\n",
        "        switch all agents into cooperation mode.\n",
        "        \"\"\"\n",
        "        critical_cases = [\"Grey Rhino\", \"Wild Card\", \"Black Swan\", \"Cascading Discontinuity\"]\n",
        "        if any(scenario in scenario_tally and scenario_tally[scenario] >= 2 for scenario in critical_cases):\n",
        "            print(\"🤝 Triggering cooperative mode across agents.\\n\")\n",
        "            for agent in self.agents:\n",
        "                agent.in_cooperation = True\n",
        "\n",
        "    # --------------------\n",
        "    # 11.11. Full System Reset\n",
        "    # --------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def reset_all(self):\n",
        "        \"\"\"\n",
        "        Reset the entire multi-agent system state:\n",
        "          - Reset each agent’s global state\n",
        "          - Re-seed tasks from born_roles using multi-round assignment\n",
        "          - Clear runner-level counters and trackers (preserving global logs)\n",
        "        \"\"\"\n",
        "        print(\"🔁 Full system reset (fatigue, hunger, memory, meta-weights).\\n\")\n",
        "\n",
        "        # 1) Reset each agent\n",
        "        for agent in self.agents:\n",
        "            # keep physiology for plotting (and reset all other state)\n",
        "            agent.reset_global_state()\n",
        "\n",
        "\n",
        "            print(f\"🔄 {agent.name} reset — roles now: {agent.roles}\")\n",
        "\n",
        "            # Re-seed tasks from Born Roles after reset using assign_tasks_from_roles_multi_round DONE don t comment out assing!!!!!!\n",
        "            if agent.roles and self.last_mission_prompt:\n",
        "                agent.assign_tasks_from_roles_multi_round(\n",
        "                    prompt=self.last_mission_prompt,\n",
        "                    global_round=self.global_round + 1,  # continue global round count after validation\n",
        "                    allow_jollycard=True,  # after validation → jollycard becomes available again\n",
        "                    verbose=True\n",
        "                )\n",
        "                print(f\"🔧 {agent.name} re-seeded tasks after reset: {agent.tasks}\")\n",
        "\n",
        "\n",
        "\n",
        "        # cooperation / penalty trackers\n",
        "        self.coop_decision_counters   = {a.name: 1     for a in self.agents}\n",
        "        self.penalty_tracker.clear()\n",
        "\n",
        "\n",
        "    # --------------------\n",
        "    # 11.12. Penalty Clear Helper\n",
        "    # --------------------\n",
        "    def clear_penalty_for_agent(self, agent):\n",
        "        \"\"\"\n",
        "        Remove any penalty‑tracker entries whose keys begin with the agent’s\n",
        "        prompt‑key prefix (first 8 chars). Prevents carry‑over after a reset.\n",
        "        \"\"\"\n",
        "        prefix = agent.generate_prompt_key(\"\")[:8]   # ← safe 8‑char prefix\n",
        "        keys_to_clear = [k for k in self.penalty_tracker if k.startswith(prefix)]\n",
        "        for k in keys_to_clear:\n",
        "            del self.penalty_tracker[k]\n",
        "\n",
        "\n",
        "\n",
        "    # =====================================\n",
        "    # 11.3 External Memory and Decay Logic\n",
        "    # =====================================\n",
        "\n",
        "    def apply_external_db_decay(self):\n",
        "        \"\"\"\n",
        "        Applies decay to the external DB based on time and usage.\n",
        "        Removes entries with decayed scores below the minimum threshold.\n",
        "        \"\"\"\n",
        "        SECONDS_PER_MONTH = 30 * 24 * 3600\n",
        "\n",
        "        if not self.agents:\n",
        "            print(\"⚠️ No agents found for meta-parameters.\")\n",
        "            return\n",
        "\n",
        "        agent_meta = self.agents[0].meta_parameters\n",
        "\n",
        "        lambda_time = agent_meta.get(\"lambda_time\", 0.5)\n",
        "        lambda_usage = agent_meta.get(\"lambda_usage\", 0.3)\n",
        "        external_db_decay_randomness = agent_meta.get(\"external_db_decay_randomness\", 0.1)\n",
        "        MINIMUM_SCORE = agent_meta.get(\"external_decay_score_threshold\", 0.2)  # ✅ Meta-learned threshold\n",
        "\n",
        "        # Apply noise\n",
        "        lambda_time *= 1 + random.uniform(-external_db_decay_randomness, external_db_decay_randomness)\n",
        "        lambda_usage *= 1 + random.uniform(-external_db_decay_randomness, external_db_decay_randomness)\n",
        "\n",
        "        print(f\"🎲 Applied decay randomness: λ_time={lambda_time:.3f}, λ_usage={lambda_usage:.3f}\")\n",
        "\n",
        "        if not self.external_db:\n",
        "            print(\"\\n📉 External DB is empty. Skipping decay step.\\n\")\n",
        "            return\n",
        "\n",
        "        current_time = time.time()\n",
        "        for key, entry in list(self.external_db.items()):\n",
        "            timestamp = entry.get(\"timestamp\", current_time)\n",
        "            usage = entry.get(\"usage_count\", 0)\n",
        "\n",
        "            months_elapsed = (current_time - timestamp) / SECONDS_PER_MONTH\n",
        "            time_decay = math.exp(-lambda_time * months_elapsed)\n",
        "            usage_decay = math.exp(-lambda_usage * usage)\n",
        "            combined_decay = time_decay * usage_decay\n",
        "\n",
        "            decayed_score = entry[\"score\"] * combined_decay\n",
        "\n",
        "            if decayed_score < MINIMUM_SCORE:\n",
        "                print(f\"🗑️ Score too low, removing: {key[:10]}... | score={decayed_score:.3f}\")\n",
        "                del self.external_db[key]\n",
        "            else:\n",
        "                entry[\"score\"] = decayed_score\n",
        "                print(f\"♻️ Gradual decay applied to {key[:10]}... | new_score={decayed_score:.3f}\")\n",
        "\n",
        "        remaining = list(self.external_db.values())\n",
        "        if remaining:\n",
        "            total = len(remaining)\n",
        "            avg_score = sum(e[\"score\"] for e in remaining) / total\n",
        "            oldest = max((current_time - e[\"timestamp\"]) / SECONDS_PER_MONTH for e in remaining)\n",
        "\n",
        "            print(f\"\\n📊 Decay Report:\")\n",
        "            print(f\"🧠 Entries remaining: {total}\")\n",
        "            print(f\"⭐ Avg. score: {avg_score:.3f}\")\n",
        "            print(f\"📅 Oldest entry age: {oldest:.2f} months\")\n",
        "        else:\n",
        "            print(\"\\n📉 All entries decayed. External DB is empty.\")\n",
        "\n",
        "        print(\"\\n✅ External DB decay step completed.\\n\")\n",
        "\n",
        "    def apply_long_memory_decay(self):\n",
        "        \"\"\"\n",
        "        Applies decay to long memory based on time and usage-like proxy (if available).\n",
        "        Uses meta-parameters to decay entries and purge low-score items.\n",
        "        \"\"\"\n",
        "        SECONDS_PER_MONTH = 30 * 24 * 3600\n",
        "\n",
        "        meta = self.meta_parameters if hasattr(self, \"meta_parameters\") else {}\n",
        "        lambda_time = meta.get(\"lambda_time\", 0.5)\n",
        "        lambda_usage = meta.get(\"lambda_usage\", 0.3)\n",
        "        decay_noise = meta.get(\"external_db_decay_randomness\", 0.1)\n",
        "        MIN_SCORE = meta.get(\"external_decay_score_threshold\", 0.2)\n",
        "\n",
        "        lambda_time *= 1 + random.uniform(-decay_noise, decay_noise)\n",
        "        lambda_usage *= 1 + random.uniform(-decay_noise, decay_noise)\n",
        "\n",
        "        current_time = time.time()\n",
        "        removed = 0\n",
        "        for k in list(self.long_memory.keys()):\n",
        "            entry = self.long_memory[k]\n",
        "            score = entry.get(\"score\", 0.0)\n",
        "            timestamp = entry.get(\"timestamp\", current_time)\n",
        "            usage = entry.get(\"usage_count\", 1)  # fallback to 1 for long memory\n",
        "\n",
        "            months_elapsed = (current_time - timestamp) / SECONDS_PER_MONTH\n",
        "            time_decay = math.exp(-lambda_time * months_elapsed)\n",
        "            usage_decay = math.exp(-lambda_usage * usage)\n",
        "            decayed_score = score * time_decay * usage_decay\n",
        "\n",
        "            if decayed_score < MIN_SCORE:\n",
        "                del self.long_memory[k]\n",
        "                removed += 1\n",
        "            else:\n",
        "                entry[\"score\"] = decayed_score\n",
        "\n",
        "        print(f\"♻️ {self.name}: Decayed long memory → removed {removed}, remaining {len(self.long_memory)}\")\n",
        "\n",
        "\n",
        "    def save_external_db(self, path=\"external_db.json\"):\n",
        "        # Convert all embeddings to lists\n",
        "        serializable_db = {\n",
        "            k: {\n",
        "                **v,\n",
        "                \"embedding\": v[\"embedding\"].tolist() if isinstance(v[\"embedding\"], np.ndarray) else v[\"embedding\"],\n",
        "                \"graph_embedding\": v.get(\"graph_embedding\", []).tolist() if isinstance(v.get(\"graph_embedding\"), np.ndarray) else v.get(\"graph_embedding\")\n",
        "            }\n",
        "            for k, v in self.external_db.items()\n",
        "        }\n",
        "\n",
        "        # Add prompt_clusters to __meta__ if available\n",
        "        if hasattr(self, \"prompt_clusters\"):\n",
        "            serializable_db[\"__meta__\"] = {\n",
        "                \"prompt_clusters\": [\n",
        "                    {\n",
        "                        \"prompts\": cluster[\"prompts\"],\n",
        "                        \"centroid\": (\n",
        "                            cluster[\"centroid\"].tolist()\n",
        "                            if isinstance(cluster[\"centroid\"], np.ndarray)\n",
        "                            else cluster[\"centroid\"]\n",
        "                        ),\n",
        "                        \"best_prompt\": cluster.get(\"best_prompt\", None),\n",
        "                        \"best_score\": cluster.get(\"best_score\", None)\n",
        "                    }\n",
        "                    for cluster in self.prompt_clusters\n",
        "                ]\n",
        "            }\n",
        "\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump(serializable_db, f)\n",
        "\n",
        "\n",
        "    def load_external_db(self, path=\"external_db.json\"):\n",
        "        try:\n",
        "            with open(path, \"r\") as f:\n",
        "                raw_db = json.load(f)\n",
        "\n",
        "            # Extract meta section (if any)\n",
        "            meta = raw_db.pop(\"__meta__\", {})\n",
        "\n",
        "            # Load prompt clusters\n",
        "            if \"prompt_clusters\" in meta:\n",
        "                self.prompt_clusters = []\n",
        "                for cluster in meta[\"prompt_clusters\"]:\n",
        "                    new_cluster = {\n",
        "                        \"prompts\": cluster.get(\"prompts\", []),\n",
        "                        \"centroid\": np.array(cluster[\"centroid\"]) if isinstance(cluster[\"centroid\"], list) else cluster[\"centroid\"],\n",
        "                        \"best_prompt\": cluster.get(\"best_prompt\"),\n",
        "                        \"best_score\": cluster.get(\"best_score\")\n",
        "                    }\n",
        "                    self.prompt_clusters.append(new_cluster)\n",
        "            else:\n",
        "                self.prompt_clusters = []\n",
        "\n",
        "            # Restore external_db\n",
        "            restored_db = {}\n",
        "            for k, v in raw_db.items():\n",
        "                restored_entry = dict(v)\n",
        "                if isinstance(v.get(\"embedding\"), list):\n",
        "                    restored_entry[\"embedding\"] = np.array(v[\"embedding\"])\n",
        "                if isinstance(v.get(\"graph_embedding\"), list):\n",
        "                    restored_entry[\"graph_embedding\"] = np.array(v[\"graph_embedding\"])\n",
        "                restored_db[k] = restored_entry\n",
        "\n",
        "            self.external_db = restored_db\n",
        "            print(f\"✅ Loaded external DB from {path} ({len(restored_db)} entries + {len(self.prompt_clusters)} clusters)\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"⚠️ No existing external DB file found at {path} — starting fresh.\")\n",
        "            self.external_db = {}\n",
        "            self.prompt_clusters = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AWIh1drTQQxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ==================== 📊 CELL 12 — DIAGNOSTIC HELPER + PRE/POST MISSION CHECK ===\n",
        "# ============================================================\n",
        "# ✅ Unified diagnostics helper class and pre/post mission checks.\n",
        "#    - Replaces previous Cell 15 and Cell 16\n",
        "#    - Provides easy-to-use diagnostic commands:\n",
        "#         diag.pre_mission_check()\n",
        "#         diag.post_mission_check_and_compare()\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "class DiagnosticHelper:\n",
        "\n",
        "    def __init__(self, runner):\n",
        "        self.runner = runner\n",
        "        self.pre_external_count = None\n",
        "        self.pre_long_memory = None\n",
        "\n",
        "    def pre_mission_check(self):\n",
        "        print(\"\\n======================\")\n",
        "        print(\"📦 EXTERNAL DB STATUS (Pre-mission)\")\n",
        "        print(\"======================\")\n",
        "\n",
        "        external_count = len(self.runner.external_db)\n",
        "        print(f\"→ External DB entries: {external_count}\")\n",
        "\n",
        "        if external_count == 0:\n",
        "            print(\"⚠️ External DB is currently empty.\")\n",
        "        else:\n",
        "            for key, entry in self.runner.external_db.items():\n",
        "                score = entry.get(\"score\", \"N/A\")\n",
        "                prompt = entry.get(\"prompt\") or entry.get(\"original_prompt\") or \"N/A\"\n",
        "                timestamp = entry.get(\"timestamp\", \"N/A\")\n",
        "                prompt_display = prompt[:50] + \"...\" if isinstance(prompt, str) else \"N/A\"\n",
        "                print(f\"- Key: {key} | Score: {score} | Prompt: {prompt_display} | Timestamp: {timestamp}\")\n",
        "\n",
        "        self.pre_external_count = external_count\n",
        "        self.pre_long_memory = {agent.name: len(agent.long_memory) for agent in self.runner.agents}\n",
        "\n",
        "    def post_mission_check_and_compare(self):\n",
        "        print(\"\\n======================\")\n",
        "        print(\"📦 EXTERNAL DB STATUS (Post-mission)\")\n",
        "        print(\"======================\")\n",
        "\n",
        "        external_count = len(self.runner.external_db)\n",
        "        print(f\"→ External DB entries: {external_count}\")\n",
        "\n",
        "        if external_count == 0:\n",
        "            print(\"⚠️ External DB is currently empty.\")\n",
        "        else:\n",
        "            for key, entry in self.runner.external_db.items():\n",
        "                score = entry.get(\"score\", \"N/A\")\n",
        "                prompt = entry.get(\"prompt\") or entry.get(\"original_prompt\") or \"N/A\"\n",
        "                timestamp = entry.get(\"timestamp\", \"N/A\")\n",
        "                prompt_display = prompt[:50] + \"...\" if isinstance(prompt, str) else \"N/A\"\n",
        "                print(f\"- Key: {key} | Score: {score} | Prompt: {prompt_display} | Timestamp: {timestamp}\")\n",
        "\n",
        "        post_external_count = external_count\n",
        "        post_long_memory = {agent.name: len(agent.long_memory) for agent in self.runner.agents}\n",
        "\n",
        "        print(\"\\n======================\")\n",
        "        print(\"📌 COMPARISON SUMMARY\")\n",
        "        print(\"======================\")\n",
        "\n",
        "        # Compare External DB\n",
        "        if self.pre_external_count == post_external_count:\n",
        "            print(f\"📦 External DB → SAME (entries: {post_external_count})\")\n",
        "        elif post_external_count > self.pre_external_count:\n",
        "            print(f\"📦 External DB → + ADDED (was {self.pre_external_count}, now {post_external_count})\")\n",
        "        else:\n",
        "            print(f\"📦 External DB → - REMOVED (was {self.pre_external_count}, now {post_external_count})\")\n",
        "\n",
        "        # Compare Agent Memories\n",
        "        print(\"\\n🧠 AGENT MEMORY CHANGES:\")\n",
        "\n",
        "        for agent_name in self.pre_long_memory.keys() | post_long_memory.keys():\n",
        "            pre_count = self.pre_long_memory.get(agent_name, 0)\n",
        "            post_count = post_long_memory.get(agent_name, 0)\n",
        "\n",
        "            if pre_count == post_count:\n",
        "                print(f\" - {agent_name}: SAME (entries: {post_count})\")\n",
        "            elif post_count > pre_count:\n",
        "                print(f\" - {agent_name}: + ADDED (was {pre_count}, now {post_count})\")\n",
        "            else:\n",
        "                print(f\" - {agent_name}: - REMOVED (was {pre_count}, now {post_count})\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nqVmICKY7a_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ==================== 🚦 CELL 13 — AGENT & RUNNER CREATION =\n",
        "# ============================================================\n",
        "\n",
        "# Utility for consistent task seeding from roles\n",
        "\n",
        "\n",
        "def seed_tasks_from_roles(agent, mission_prompt=None):\n",
        "    # agent.assign_tasks_from_roles_multi_round(\n",
        "    #     prompt=mission_prompt or agent.runner.last_mission_prompt,\n",
        "    #     global_round=agent.runner.global_round + 1,\n",
        "    #     allow_jollycard=True,\n",
        "    #     verbose=True\n",
        "    # )\n",
        "    pass\n",
        "\n",
        "# 0) Make an “empty” runner\n",
        "runner = EnhancedStrategicRunner(agents=[])\n",
        "\n",
        "# ✅ Load previously stored DB from disk\n",
        "runner.load_external_db(\"external_db.json\")\n",
        "\n",
        "# 1) Build agents with Born Roles + Hybrid Task Seeding\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "model_keys = list(available_models.keys())  # e.g. [\"gemini\",\"openai\",\"claude\",…]\n",
        "agents = []\n",
        "\n",
        "for i in range(3):\n",
        "    name = f\"Node_{i}\"\n",
        "    born = BORN_ROLES.get(name, [])\n",
        "    # pick one of the loaded model keys at random:\n",
        "    chosen_key = random.choice(model_keys)\n",
        "    agent = StrategicAgent(\n",
        "        name=name,\n",
        "        model_key=chosen_key,    # <-- pass the model key, not a file path\n",
        "        born_roles=born,\n",
        "        runner=runner\n",
        "    )\n",
        "    agent.load_long_memory(\"long_memory.json\")  # ✅ LOAD at startup\n",
        "\n",
        "    agents.append(agent)\n",
        "\n",
        "# 2) Now that agents exist, give them to runner\n",
        "runner.agents = agents\n",
        "# (re-initialize any runner caches if needed—often not required)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ✅ DIAGNOSTIC HELPER BINDING → Add to runner after creation\n",
        "runner.diag = DiagnosticHelper(runner)\n",
        "\n"
      ],
      "metadata": {
        "id": "OKJBSLxV7yj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# =============== CELL 14 — RUNNER VISUALIZATION =============\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "def plot_real_timelines_with_shading(runner, *, figsize=(14, 3)):\n",
        "    \"\"\"\n",
        "    ASCII‑style validation bars but with Matplotlib shading.\n",
        "    • One bar per agent.\n",
        "    • Blue segments  = intervals that *did* validate locally\n",
        "    • White segments = intervals that didn't\n",
        "    \"\"\"\n",
        "    agents = runner.agents\n",
        "    if not agents:\n",
        "        print(\"⟐ No agents to plot.\")\n",
        "        return\n",
        "\n",
        "    # length = longest interval processed by any agent this global round\n",
        "    max_x = max((a.interval for a in agents), default=0)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
        "\n",
        "    y_ticks = []\n",
        "    y_labels = []\n",
        "    for idx, ag in enumerate(agents, start=1):\n",
        "        y = idx       # vertical slot for this agent\n",
        "        y_ticks.append(y)\n",
        "        y_labels.append(ag.name)\n",
        "\n",
        "        # build a 0/1 mask per interval\n",
        "        validated_mask = [0]*(max_x+1)\n",
        "        for iv in ag.local_interval_log:\n",
        "            if iv <= max_x:\n",
        "                validated_mask[iv] = 1\n",
        "\n",
        "        # draw background bar\n",
        "        ax.barh(y, max_x, left=0, height=0.6, color=\"lightgrey\", alpha=0.5)\n",
        "\n",
        "        # overlay validated intervals\n",
        "        for k, flag in enumerate(validated_mask, start=0):\n",
        "            if flag:\n",
        "                ax.barh(y, 1, left=k, height=0.6, color=\"skyblue\", alpha=0.9)\n",
        "\n",
        "    ax.set_xlabel(\"Interval #\")\n",
        "    ax.set_xlim(0, max_x + 1e-3)      # avoid identical low/high limits\n",
        "    ax.set_xticks(range(0, max_x+1))\n",
        "\n",
        "    ax.set_yticks(y_ticks)\n",
        "    ax.set_yticklabels(y_labels)\n",
        "    ax.set_title(\"Agent local‑validation timeline\")\n",
        "    ax.grid(axis=\"x\", alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R3f1Yt4ZDHvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# =============== 🚀 CELL 15 — FULL RUN & DIAGNOSTICS ========\n",
        "# ============================================================\n",
        "\n",
        "# 1) Define (or re‑define) the mission prompt\n",
        "mission_prompt = \"Derive the closed-form solution for the Black-Scholes European call option pricing model and implement it in Python.\"\n",
        "runner.last_mission_prompt = mission_prompt          # let agents see the prompt if needed\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 🔍 PRE‑MISSION SNAPSHOT\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "runner.diag.pre_mission_check()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 🚀 MAIN EXECUTION\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "promoted = runner.execute(mission_prompt, verbose=True)\n",
        "print(\"\\n🚦 Promoted this round?\", promoted)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 📸 POST‑MISSION SNAPSHOT & COMPARISON\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "runner.diag.post_mission_check_and_compare()\n",
        "runner.print_agent_snapshot()\n",
        "\n",
        "# ♻️ Apply decay to the external DB\n",
        "runner.apply_external_db_decay()\n",
        "\n",
        "# ♻️ Apply decay to each agent’s long memory\n",
        "for agent in runner.agents:\n",
        "    agent.apply_long_memory_decay()\n",
        "    agent.save_long_memory(f\"{agent.name}_long_memory.json\")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 🗑️  STATIC‑DEFINITION PURGE LOG\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "print(\"\\n🗑️  Static‑Definition Purge Log:\")\n",
        "print(\"=\"*80)\n",
        "for agent in runner.agents:\n",
        "    if not agent.purge_log:\n",
        "        continue\n",
        "    print(f\"\\n🔍 {agent.name} purge history:\")\n",
        "    for entry in agent.purge_log:\n",
        "        gr   = entry[\"global_round\"]\n",
        "        remR = entry[\"removed_roles\"]\n",
        "        retR = entry[\"retained_roles\"]\n",
        "        remT = entry[\"removed_tasks\"]\n",
        "        retT = entry[\"retained_tasks\"]\n",
        "        print(f\" • After global round {gr}:\")\n",
        "        print(f\"    • Roles removed:   {remR or '–'}\")\n",
        "        print(f\"    • Roles retained:  {retR}\")\n",
        "        print(f\"    • Tasks removed:   {remT or '–'}\")\n",
        "        print(f\"    • Tasks retained:  {retT}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 🩺 PHYSIOLOGY ARRAYS (debug print)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "print(\"\\n── Physiology Histories ──\")\n",
        "for agent in runner.agents:\n",
        "    print(f\"{agent.name}:\",\n",
        "          f\"attention={agent.attention_history},\",\n",
        "          f\"fatigue={agent.fatigue_history},\",\n",
        "          f\"hunger={agent.hunger_history}\")\n",
        "    print(f\"{agent.name} length →\", len(agent.attention_history))\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 📊 VISUALS\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "runner.plot_agent_timelines()                 # ASCII timeline\n",
        "runner.plot_scenario_cluster_timeline()\n",
        "plot_real_timelines_with_shading(runner)      # Matplotlib timeline (with validation shading)\n",
        "plot_agent_physiology(runner)                 # Attention / Fatigue / Hunger charts\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XNgxVRPL-CB0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ccde9719-2339-42f5-b6c2-6b33d174c8bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================\n",
            "📦 EXTERNAL DB STATUS (Pre-mission)\n",
            "======================\n",
            "→ External DB entries: 0\n",
            "⚠️ External DB is currently empty.\n",
            "📈 Best similarity found: 0.0000 vs reuse threshold: 0.7500\n",
            "🆕 New mission prompt detected — resetting global counter.\n",
            "\n",
            "---- Interval 1 (mission start) ----\n",
            " • Node_0: local_round=1, interval=1\n",
            " • Node_1: local_round=1, interval=1\n",
            " • Node_2: local_round=1, interval=1\n",
            "🔍 Node_0 top_3 role candidates (pre‑threshold): ['Mathematician', 'Logician', 'Builder']\n",
            "🔍 Node_0 meaningful roles (score ≥ 0.7): []\n",
            "⚠️ Node_0 assigning top role despite threshold: Mathematician\n",
            "🎲 Node_0 randomly assigned role from top_1: ['Mathematician']\n",
            "✅ Node_0 kept roles: ['Mathematician']\n",
            "⏳ Restricting to single role (global round 1): ['Mathematician']\n",
            "🔍 Node_0 seeded tasks from store (top_5): ['compute', 'mathematicize']\n",
            "🧠 Node_0 assigned tasks: ['compute', 'mathematicize']\n",
            "🔍 Node_1 top_3 role candidates (pre‑threshold): ['Mathematician', 'Logician', 'Builder']\n",
            "🔍 Node_1 meaningful roles (score ≥ 0.7): []\n",
            "⚠️ Node_1 assigning top role despite threshold: Mathematician\n",
            "🎲 Node_1 randomly assigned role from top_1: ['Mathematician']\n",
            "✅ Node_1 kept roles: ['Mathematician']\n",
            "⏳ Restricting to single role (global round 1): ['Mathematician']\n",
            "🔍 Node_1 seeded tasks from store (top_5): ['compute', 'mathematicize']\n",
            "🧠 Node_1 assigned tasks: ['compute', 'mathematicize']\n",
            "🔍 Node_2 top_3 role candidates (pre‑threshold): ['Mathematician', 'Logician', 'Builder']\n",
            "🔍 Node_2 meaningful roles (score ≥ 0.7): []\n",
            "⚠️ Node_2 assigning top role despite threshold: Mathematician\n",
            "🎲 Node_2 randomly assigned role from top_1: ['Mathematician']\n",
            "✅ Node_2 kept roles: ['Mathematician']\n",
            "⏳ Restricting to single role (global round 1): ['Mathematician']\n",
            "🔍 Node_2 seeded tasks from store (top_5): ['compute', 'mathematicize']\n",
            "🧠 Node_2 assigned tasks: ['compute', 'mathematicize']\n",
            "\n",
            "🔩 Seeded initial roles/tasks for Interval 1\n",
            " • Node_0: roles=['Mathematician'], tasks=['compute', 'mathematicize']\n",
            "\n",
            "📦 Dynamic task_store contents after initial seed (per agent):\n",
            " • Node_0.task_store:\n",
            "     • 'analyze': last_score=0.000, desc='Perform deep analysis of the content or …'\n",
            "     • 'retrieve': last_score=0.000, desc='Fetch information from memory or externa…'\n",
            "     • 'synthesize': last_score=0.000, desc='Combine multiple inputs into a coherent …'\n",
            "     • 'summarize': last_score=0.000, desc='Produce a concise summary of the key poi…'\n",
            "     • 'compute': last_score=0.000, desc='Perform numerical or logical calculation…'\n",
            "     • 'deduct': last_score=0.000, desc='Draw logical conclusions from given fact…'\n",
            "     • 'infer': last_score=0.000, desc='Make inferences based on context and evi…'\n",
            "     • 'plan': last_score=0.000, desc='Lay out a step-by-step strategy or roadm…'\n",
            "     • 'model': last_score=0.000, desc='Build a structured model or simulation.…'\n",
            "     • 'code': last_score=0.000, desc='Write or generate executable code.…'\n",
            "     • 'verify': last_score=0.000, desc='Check facts or validate data against tru…'\n",
            "     • 'substantiate': last_score=0.000, desc='Provide evidence or references to suppor…'\n",
            "     • 'ground': last_score=0.000, desc='Anchor abstract reasoning in concrete ex…'\n",
            "     • 'design': last_score=0.000, desc='Architect a system, workflow, or high-le…'\n",
            "     • 'debug': last_score=0.000, desc='Identify and fix errors in code or logic…'\n",
            "     • 'optimize': last_score=0.000, desc='Improve performance or efficiency of a s…'\n",
            "     • 'critique': last_score=0.000, desc='Evaluate an argument or design and sugge…'\n",
            "     • 'visualize': last_score=0.000, desc='Generate or describe a chart, diagram, o…'\n",
            "     • 'reverse-engineer': last_score=0.000, desc='Deconstruct a system or process to under…'\n",
            "     • 'forecast': last_score=0.000, desc='Predict future trends or outcomes based …'\n",
            "     • 'mathematicize': last_score=0.000, desc='Formalize an idea in mathematical terms …'\n",
            "     • 'logicalize': last_score=0.000, desc='Apply formal logic to structure argument…'\n",
            "     • 'argument': last_score=0.000, desc='Construct or analyze a persuasive argume…'\n",
            "     • 'factualize': last_score=0.000, desc='Convert a statement into factual, eviden…'\n",
            "     • 'rationalize': last_score=0.000, desc='Explain the reasoning or logic behind a …'\n",
            "     • 'create': last_score=0.000, desc='Generate new content or ideas from scrat…'\n",
            "     • 'idealize': last_score=0.000, desc='Envision the perfect or optimal version …'\n",
            "     • 'abstractize': last_score=0.000, desc='Extract the underlying essence or abstra…'\n",
            "     • 'generalize': last_score=0.000, desc='Form broad principles or patterns from s…'\n",
            "     • 'specify': last_score=0.000, desc='Provide detailed, precise information or…'\n",
            "     • 'contextualize': last_score=0.000, desc='Place information or findings in an appr…'\n",
            "     • 'test': last_score=0.000, desc='Evaluate accuracy, performance, or valid…'\n",
            "     • 'track': last_score=0.000, desc='Follow the progression or change of elem…'\n",
            "     • 'monitor': last_score=0.000, desc='Continuously observe data, systems, or a…'\n",
            "     • 'translate': last_score=0.000, desc='Convert information between languages, f…'\n",
            "     • 'clarify': last_score=0.000, desc='Make complex, ambiguous, or vague inform…'\n",
            "     • 'restructure': last_score=0.000, desc='Reorganize the format, structure, or seq…'\n",
            " • Node_1.task_store:\n",
            "     • 'analyze': last_score=0.000, desc='Perform deep analysis of the content or …'\n",
            "     • 'retrieve': last_score=0.000, desc='Fetch information from memory or externa…'\n",
            "     • 'synthesize': last_score=0.000, desc='Combine multiple inputs into a coherent …'\n",
            "     • 'summarize': last_score=0.000, desc='Produce a concise summary of the key poi…'\n",
            "     • 'compute': last_score=0.000, desc='Perform numerical or logical calculation…'\n",
            "     • 'deduct': last_score=0.000, desc='Draw logical conclusions from given fact…'\n",
            "     • 'infer': last_score=0.000, desc='Make inferences based on context and evi…'\n",
            "     • 'plan': last_score=0.000, desc='Lay out a step-by-step strategy or roadm…'\n",
            "     • 'model': last_score=0.000, desc='Build a structured model or simulation.…'\n",
            "     • 'code': last_score=0.000, desc='Write or generate executable code.…'\n",
            "     • 'verify': last_score=0.000, desc='Check facts or validate data against tru…'\n",
            "     • 'substantiate': last_score=0.000, desc='Provide evidence or references to suppor…'\n",
            "     • 'ground': last_score=0.000, desc='Anchor abstract reasoning in concrete ex…'\n",
            "     • 'design': last_score=0.000, desc='Architect a system, workflow, or high-le…'\n",
            "     • 'debug': last_score=0.000, desc='Identify and fix errors in code or logic…'\n",
            "     • 'optimize': last_score=0.000, desc='Improve performance or efficiency of a s…'\n",
            "     • 'critique': last_score=0.000, desc='Evaluate an argument or design and sugge…'\n",
            "     • 'visualize': last_score=0.000, desc='Generate or describe a chart, diagram, o…'\n",
            "     • 'reverse-engineer': last_score=0.000, desc='Deconstruct a system or process to under…'\n",
            "     • 'forecast': last_score=0.000, desc='Predict future trends or outcomes based …'\n",
            "     • 'mathematicize': last_score=0.000, desc='Formalize an idea in mathematical terms …'\n",
            "     • 'logicalize': last_score=0.000, desc='Apply formal logic to structure argument…'\n",
            "     • 'argument': last_score=0.000, desc='Construct or analyze a persuasive argume…'\n",
            "     • 'factualize': last_score=0.000, desc='Convert a statement into factual, eviden…'\n",
            "     • 'rationalize': last_score=0.000, desc='Explain the reasoning or logic behind a …'\n",
            "     • 'create': last_score=0.000, desc='Generate new content or ideas from scrat…'\n",
            "     • 'idealize': last_score=0.000, desc='Envision the perfect or optimal version …'\n",
            "     • 'abstractize': last_score=0.000, desc='Extract the underlying essence or abstra…'\n",
            "     • 'generalize': last_score=0.000, desc='Form broad principles or patterns from s…'\n",
            "     • 'specify': last_score=0.000, desc='Provide detailed, precise information or…'\n",
            "     • 'contextualize': last_score=0.000, desc='Place information or findings in an appr…'\n",
            "     • 'test': last_score=0.000, desc='Evaluate accuracy, performance, or valid…'\n",
            "     • 'track': last_score=0.000, desc='Follow the progression or change of elem…'\n",
            "     • 'monitor': last_score=0.000, desc='Continuously observe data, systems, or a…'\n",
            "     • 'translate': last_score=0.000, desc='Convert information between languages, f…'\n",
            "     • 'clarify': last_score=0.000, desc='Make complex, ambiguous, or vague inform…'\n",
            "     • 'restructure': last_score=0.000, desc='Reorganize the format, structure, or seq…'\n",
            " • Node_2.task_store:\n",
            "     • 'analyze': last_score=0.000, desc='Perform deep analysis of the content or …'\n",
            "     • 'retrieve': last_score=0.000, desc='Fetch information from memory or externa…'\n",
            "     • 'synthesize': last_score=0.000, desc='Combine multiple inputs into a coherent …'\n",
            "     • 'summarize': last_score=0.000, desc='Produce a concise summary of the key poi…'\n",
            "     • 'compute': last_score=0.000, desc='Perform numerical or logical calculation…'\n",
            "     • 'deduct': last_score=0.000, desc='Draw logical conclusions from given fact…'\n",
            "     • 'infer': last_score=0.000, desc='Make inferences based on context and evi…'\n",
            "     • 'plan': last_score=0.000, desc='Lay out a step-by-step strategy or roadm…'\n",
            "     • 'model': last_score=0.000, desc='Build a structured model or simulation.…'\n",
            "     • 'code': last_score=0.000, desc='Write or generate executable code.…'\n",
            "     • 'verify': last_score=0.000, desc='Check facts or validate data against tru…'\n",
            "     • 'substantiate': last_score=0.000, desc='Provide evidence or references to suppor…'\n",
            "     • 'ground': last_score=0.000, desc='Anchor abstract reasoning in concrete ex…'\n",
            "     • 'design': last_score=0.000, desc='Architect a system, workflow, or high-le…'\n",
            "     • 'debug': last_score=0.000, desc='Identify and fix errors in code or logic…'\n",
            "     • 'optimize': last_score=0.000, desc='Improve performance or efficiency of a s…'\n",
            "     • 'critique': last_score=0.000, desc='Evaluate an argument or design and sugge…'\n",
            "     • 'visualize': last_score=0.000, desc='Generate or describe a chart, diagram, o…'\n",
            "     • 'reverse-engineer': last_score=0.000, desc='Deconstruct a system or process to under…'\n",
            "     • 'forecast': last_score=0.000, desc='Predict future trends or outcomes based …'\n",
            "     • 'mathematicize': last_score=0.000, desc='Formalize an idea in mathematical terms …'\n",
            "     • 'logicalize': last_score=0.000, desc='Apply formal logic to structure argument…'\n",
            "     • 'argument': last_score=0.000, desc='Construct or analyze a persuasive argume…'\n",
            "     • 'factualize': last_score=0.000, desc='Convert a statement into factual, eviden…'\n",
            "     • 'rationalize': last_score=0.000, desc='Explain the reasoning or logic behind a …'\n",
            "     • 'create': last_score=0.000, desc='Generate new content or ideas from scrat…'\n",
            "     • 'idealize': last_score=0.000, desc='Envision the perfect or optimal version …'\n",
            "     • 'abstractize': last_score=0.000, desc='Extract the underlying essence or abstra…'\n",
            "     • 'generalize': last_score=0.000, desc='Form broad principles or patterns from s…'\n",
            "     • 'specify': last_score=0.000, desc='Provide detailed, precise information or…'\n",
            "     • 'contextualize': last_score=0.000, desc='Place information or findings in an appr…'\n",
            "     • 'test': last_score=0.000, desc='Evaluate accuracy, performance, or valid…'\n",
            "     • 'track': last_score=0.000, desc='Follow the progression or change of elem…'\n",
            "     • 'monitor': last_score=0.000, desc='Continuously observe data, systems, or a…'\n",
            "     • 'translate': last_score=0.000, desc='Convert information between languages, f…'\n",
            "     • 'clarify': last_score=0.000, desc='Make complex, ambiguous, or vague inform…'\n",
            "     • 'restructure': last_score=0.000, desc='Reorganize the format, structure, or seq…'\n",
            " • Node_1: roles=['Mathematician'], tasks=['compute', 'mathematicize']\n",
            "\n",
            "📦 Dynamic task_store contents after initial seed (per agent):\n",
            " • Node_0.task_store:\n",
            "     • 'analyze': last_score=0.000, desc='Perform deep analysis of the content or …'\n",
            "     • 'retrieve': last_score=0.000, desc='Fetch information from memory or externa…'\n",
            "     • 'synthesize': last_score=0.000, desc='Combine multiple inputs into a coherent …'\n",
            "     • 'summarize': last_score=0.000, desc='Produce a concise summary of the key poi…'\n",
            "     • 'compute': last_score=0.000, desc='Perform numerical or logical calculation…'\n",
            "     • 'deduct': last_score=0.000, desc='Draw logical conclusions from given fact…'\n",
            "     • 'infer': last_score=0.000, desc='Make inferences based on context and evi…'\n",
            "     • 'plan': last_score=0.000, desc='Lay out a step-by-step strategy or roadm…'\n",
            "     • 'model': last_score=0.000, desc='Build a structured model or simulation.…'\n",
            "     • 'code': last_score=0.000, desc='Write or generate executable code.…'\n",
            "     • 'verify': last_score=0.000, desc='Check facts or validate data against tru…'\n",
            "     • 'substantiate': last_score=0.000, desc='Provide evidence or references to suppor…'\n",
            "     • 'ground': last_score=0.000, desc='Anchor abstract reasoning in concrete ex…'\n",
            "     • 'design': last_score=0.000, desc='Architect a system, workflow, or high-le…'\n",
            "     • 'debug': last_score=0.000, desc='Identify and fix errors in code or logic…'\n",
            "     • 'optimize': last_score=0.000, desc='Improve performance or efficiency of a s…'\n",
            "     • 'critique': last_score=0.000, desc='Evaluate an argument or design and sugge…'\n",
            "     • 'visualize': last_score=0.000, desc='Generate or describe a chart, diagram, o…'\n",
            "     • 'reverse-engineer': last_score=0.000, desc='Deconstruct a system or process to under…'\n",
            "     • 'forecast': last_score=0.000, desc='Predict future trends or outcomes based …'\n",
            "     • 'mathematicize': last_score=0.000, desc='Formalize an idea in mathematical terms …'\n",
            "     • 'logicalize': last_score=0.000, desc='Apply formal logic to structure argument…'\n",
            "     • 'argument': last_score=0.000, desc='Construct or analyze a persuasive argume…'\n",
            "     • 'factualize': last_score=0.000, desc='Convert a statement into factual, eviden…'\n",
            "     • 'rationalize': last_score=0.000, desc='Explain the reasoning or logic behind a …'\n",
            "     • 'create': last_score=0.000, desc='Generate new content or ideas from scrat…'\n",
            "     • 'idealize': last_score=0.000, desc='Envision the perfect or optimal version …'\n",
            "     • 'abstractize': last_score=0.000, desc='Extract the underlying essence or abstra…'\n",
            "     • 'generalize': last_score=0.000, desc='Form broad principles or patterns from s…'\n",
            "     • 'specify': last_score=0.000, desc='Provide detailed, precise information or…'\n",
            "     • 'contextualize': last_score=0.000, desc='Place information or findings in an appr…'\n",
            "     • 'test': last_score=0.000, desc='Evaluate accuracy, performance, or valid…'\n",
            "     • 'track': last_score=0.000, desc='Follow the progression or change of elem…'\n",
            "     • 'monitor': last_score=0.000, desc='Continuously observe data, systems, or a…'\n",
            "     • 'translate': last_score=0.000, desc='Convert information between languages, f…'\n",
            "     • 'clarify': last_score=0.000, desc='Make complex, ambiguous, or vague inform…'\n",
            "     • 'restructure': last_score=0.000, desc='Reorganize the format, structure, or seq…'\n",
            " • Node_1.task_store:\n",
            "     • 'analyze': last_score=0.000, desc='Perform deep analysis of the content or …'\n",
            "     • 'retrieve': last_score=0.000, desc='Fetch information from memory or externa…'\n",
            "     • 'synthesize': last_score=0.000, desc='Combine multiple inputs into a coherent …'\n",
            "     • 'summarize': last_score=0.000, desc='Produce a concise summary of the key poi…'\n",
            "     • 'compute': last_score=0.000, desc='Perform numerical or logical calculation…'\n",
            "     • 'deduct': last_score=0.000, desc='Draw logical conclusions from given fact…'\n",
            "     • 'infer': last_score=0.000, desc='Make inferences based on context and evi…'\n",
            "     • 'plan': last_score=0.000, desc='Lay out a step-by-step strategy or roadm…'\n",
            "     • 'model': last_score=0.000, desc='Build a structured model or simulation.…'\n",
            "     • 'code': last_score=0.000, desc='Write or generate executable code.…'\n",
            "     • 'verify': last_score=0.000, desc='Check facts or validate data against tru…'\n",
            "     • 'substantiate': last_score=0.000, desc='Provide evidence or references to suppor…'\n",
            "     • 'ground': last_score=0.000, desc='Anchor abstract reasoning in concrete ex…'\n",
            "     • 'design': last_score=0.000, desc='Architect a system, workflow, or high-le…'\n",
            "     • 'debug': last_score=0.000, desc='Identify and fix errors in code or logic…'\n",
            "     • 'optimize': last_score=0.000, desc='Improve performance or efficiency of a s…'\n",
            "     • 'critique': last_score=0.000, desc='Evaluate an argument or design and sugge…'\n",
            "     • 'visualize': last_score=0.000, desc='Generate or describe a chart, diagram, o…'\n",
            "     • 'reverse-engineer': last_score=0.000, desc='Deconstruct a system or process to under…'\n",
            "     • 'forecast': last_score=0.000, desc='Predict future trends or outcomes based …'\n",
            "     • 'mathematicize': last_score=0.000, desc='Formalize an idea in mathematical terms …'\n",
            "     • 'logicalize': last_score=0.000, desc='Apply formal logic to structure argument…'\n",
            "     • 'argument': last_score=0.000, desc='Construct or analyze a persuasive argume…'\n",
            "     • 'factualize': last_score=0.000, desc='Convert a statement into factual, eviden…'\n",
            "     • 'rationalize': last_score=0.000, desc='Explain the reasoning or logic behind a …'\n",
            "     • 'create': last_score=0.000, desc='Generate new content or ideas from scrat…'\n",
            "     • 'idealize': last_score=0.000, desc='Envision the perfect or optimal version …'\n",
            "     • 'abstractize': last_score=0.000, desc='Extract the underlying essence or abstra…'\n",
            "     • 'generalize': last_score=0.000, desc='Form broad principles or patterns from s…'\n",
            "     • 'specify': last_score=0.000, desc='Provide detailed, precise information or…'\n",
            "     • 'contextualize': last_score=0.000, desc='Place information or findings in an appr…'\n",
            "     • 'test': last_score=0.000, desc='Evaluate accuracy, performance, or valid…'\n",
            "     • 'track': last_score=0.000, desc='Follow the progression or change of elem…'\n",
            "     • 'monitor': last_score=0.000, desc='Continuously observe data, systems, or a…'\n",
            "     • 'translate': last_score=0.000, desc='Convert information between languages, f…'\n",
            "     • 'clarify': last_score=0.000, desc='Make complex, ambiguous, or vague inform…'\n",
            "     • 'restructure': last_score=0.000, desc='Reorganize the format, structure, or seq…'\n",
            " • Node_2.task_store:\n",
            "     • 'analyze': last_score=0.000, desc='Perform deep analysis of the content or …'\n",
            "     • 'retrieve': last_score=0.000, desc='Fetch information from memory or externa…'\n",
            "     • 'synthesize': last_score=0.000, desc='Combine multiple inputs into a coherent …'\n",
            "     • 'summarize': last_score=0.000, desc='Produce a concise summary of the key poi…'\n",
            "     • 'compute': last_score=0.000, desc='Perform numerical or logical calculation…'\n",
            "     • 'deduct': last_score=0.000, desc='Draw logical conclusions from given fact…'\n",
            "     • 'infer': last_score=0.000, desc='Make inferences based on context and evi…'\n",
            "     • 'plan': last_score=0.000, desc='Lay out a step-by-step strategy or roadm…'\n",
            "     • 'model': last_score=0.000, desc='Build a structured model or simulation.…'\n",
            "     • 'code': last_score=0.000, desc='Write or generate executable code.…'\n",
            "     • 'verify': last_score=0.000, desc='Check facts or validate data against tru…'\n",
            "     • 'substantiate': last_score=0.000, desc='Provide evidence or references to suppor…'\n",
            "     • 'ground': last_score=0.000, desc='Anchor abstract reasoning in concrete ex…'\n",
            "     • 'design': last_score=0.000, desc='Architect a system, workflow, or high-le…'\n",
            "     • 'debug': last_score=0.000, desc='Identify and fix errors in code or logic…'\n",
            "     • 'optimize': last_score=0.000, desc='Improve performance or efficiency of a s…'\n",
            "     • 'critique': last_score=0.000, desc='Evaluate an argument or design and sugge…'\n",
            "     • 'visualize': last_score=0.000, desc='Generate or describe a chart, diagram, o…'\n",
            "     • 'reverse-engineer': last_score=0.000, desc='Deconstruct a system or process to under…'\n",
            "     • 'forecast': last_score=0.000, desc='Predict future trends or outcomes based …'\n",
            "     • 'mathematicize': last_score=0.000, desc='Formalize an idea in mathematical terms …'\n",
            "     • 'logicalize': last_score=0.000, desc='Apply formal logic to structure argument…'\n",
            "     • 'argument': last_score=0.000, desc='Construct or analyze a persuasive argume…'\n",
            "     • 'factualize': last_score=0.000, desc='Convert a statement into factual, eviden…'\n",
            "     • 'rationalize': last_score=0.000, desc='Explain the reasoning or logic behind a …'\n",
            "     • 'create': last_score=0.000, desc='Generate new content or ideas from scrat…'\n",
            "     • 'idealize': last_score=0.000, desc='Envision the perfect or optimal version …'\n",
            "     • 'abstractize': last_score=0.000, desc='Extract the underlying essence or abstra…'\n",
            "     • 'generalize': last_score=0.000, desc='Form broad principles or patterns from s…'\n",
            "     • 'specify': last_score=0.000, desc='Provide detailed, precise information or…'\n",
            "     • 'contextualize': last_score=0.000, desc='Place information or findings in an appr…'\n",
            "     • 'test': last_score=0.000, desc='Evaluate accuracy, performance, or valid…'\n",
            "     • 'track': last_score=0.000, desc='Follow the progression or change of elem…'\n",
            "     • 'monitor': last_score=0.000, desc='Continuously observe data, systems, or a…'\n",
            "     • 'translate': last_score=0.000, desc='Convert information between languages, f…'\n",
            "     • 'clarify': last_score=0.000, desc='Make complex, ambiguous, or vague inform…'\n",
            "     • 'restructure': last_score=0.000, desc='Reorganize the format, structure, or seq…'\n",
            " • Node_2: roles=['Mathematician'], tasks=['compute', 'mathematicize']\n",
            "\n",
            "📦 Dynamic task_store contents after initial seed (per agent):\n",
            " • Node_0.task_store:\n",
            "     • 'analyze': last_score=0.000, desc='Perform deep analysis of the content or …'\n",
            "     • 'retrieve': last_score=0.000, desc='Fetch information from memory or externa…'\n",
            "     • 'synthesize': last_score=0.000, desc='Combine multiple inputs into a coherent …'\n",
            "     • 'summarize': last_score=0.000, desc='Produce a concise summary of the key poi…'\n",
            "     • 'compute': last_score=0.000, desc='Perform numerical or logical calculation…'\n",
            "     • 'deduct': last_score=0.000, desc='Draw logical conclusions from given fact…'\n",
            "     • 'infer': last_score=0.000, desc='Make inferences based on context and evi…'\n",
            "     • 'plan': last_score=0.000, desc='Lay out a step-by-step strategy or roadm…'\n",
            "     • 'model': last_score=0.000, desc='Build a structured model or simulation.…'\n",
            "     • 'code': last_score=0.000, desc='Write or generate executable code.…'\n",
            "     • 'verify': last_score=0.000, desc='Check facts or validate data against tru…'\n",
            "     • 'substantiate': last_score=0.000, desc='Provide evidence or references to suppor…'\n",
            "     • 'ground': last_score=0.000, desc='Anchor abstract reasoning in concrete ex…'\n",
            "     • 'design': last_score=0.000, desc='Architect a system, workflow, or high-le…'\n",
            "     • 'debug': last_score=0.000, desc='Identify and fix errors in code or logic…'\n",
            "     • 'optimize': last_score=0.000, desc='Improve performance or efficiency of a s…'\n",
            "     • 'critique': last_score=0.000, desc='Evaluate an argument or design and sugge…'\n",
            "     • 'visualize': last_score=0.000, desc='Generate or describe a chart, diagram, o…'\n",
            "     • 'reverse-engineer': last_score=0.000, desc='Deconstruct a system or process to under…'\n",
            "     • 'forecast': last_score=0.000, desc='Predict future trends or outcomes based …'\n",
            "     • 'mathematicize': last_score=0.000, desc='Formalize an idea in mathematical terms …'\n",
            "     • 'logicalize': last_score=0.000, desc='Apply formal logic to structure argument…'\n",
            "     • 'argument': last_score=0.000, desc='Construct or analyze a persuasive argume…'\n",
            "     • 'factualize': last_score=0.000, desc='Convert a statement into factual, eviden…'\n",
            "     • 'rationalize': last_score=0.000, desc='Explain the reasoning or logic behind a …'\n",
            "     • 'create': last_score=0.000, desc='Generate new content or ideas from scrat…'\n",
            "     • 'idealize': last_score=0.000, desc='Envision the perfect or optimal version …'\n",
            "     • 'abstractize': last_score=0.000, desc='Extract the underlying essence or abstra…'\n",
            "     • 'generalize': last_score=0.000, desc='Form broad principles or patterns from s…'\n",
            "     • 'specify': last_score=0.000, desc='Provide detailed, precise information or…'\n",
            "     • 'contextualize': last_score=0.000, desc='Place information or findings in an appr…'\n",
            "     • 'test': last_score=0.000, desc='Evaluate accuracy, performance, or valid…'\n",
            "     • 'track': last_score=0.000, desc='Follow the progression or change of elem…'\n",
            "     • 'monitor': last_score=0.000, desc='Continuously observe data, systems, or a…'\n",
            "     • 'translate': last_score=0.000, desc='Convert information between languages, f…'\n",
            "     • 'clarify': last_score=0.000, desc='Make complex, ambiguous, or vague inform…'\n",
            "     • 'restructure': last_score=0.000, desc='Reorganize the format, structure, or seq…'\n",
            " • Node_1.task_store:\n",
            "     • 'analyze': last_score=0.000, desc='Perform deep analysis of the content or …'\n",
            "     • 'retrieve': last_score=0.000, desc='Fetch information from memory or externa…'\n",
            "     • 'synthesize': last_score=0.000, desc='Combine multiple inputs into a coherent …'\n",
            "     • 'summarize': last_score=0.000, desc='Produce a concise summary of the key poi…'\n",
            "     • 'compute': last_score=0.000, desc='Perform numerical or logical calculation…'\n",
            "     • 'deduct': last_score=0.000, desc='Draw logical conclusions from given fact…'\n",
            "     • 'infer': last_score=0.000, desc='Make inferences based on context and evi…'\n",
            "     • 'plan': last_score=0.000, desc='Lay out a step-by-step strategy or roadm…'\n",
            "     • 'model': last_score=0.000, desc='Build a structured model or simulation.…'\n",
            "     • 'code': last_score=0.000, desc='Write or generate executable code.…'\n",
            "     • 'verify': last_score=0.000, desc='Check facts or validate data against tru…'\n",
            "     • 'substantiate': last_score=0.000, desc='Provide evidence or references to suppor…'\n",
            "     • 'ground': last_score=0.000, desc='Anchor abstract reasoning in concrete ex…'\n",
            "     • 'design': last_score=0.000, desc='Architect a system, workflow, or high-le…'\n",
            "     • 'debug': last_score=0.000, desc='Identify and fix errors in code or logic…'\n",
            "     • 'optimize': last_score=0.000, desc='Improve performance or efficiency of a s…'\n",
            "     • 'critique': last_score=0.000, desc='Evaluate an argument or design and sugge…'\n",
            "     • 'visualize': last_score=0.000, desc='Generate or describe a chart, diagram, o…'\n",
            "     • 'reverse-engineer': last_score=0.000, desc='Deconstruct a system or process to under…'\n",
            "     • 'forecast': last_score=0.000, desc='Predict future trends or outcomes based …'\n",
            "     • 'mathematicize': last_score=0.000, desc='Formalize an idea in mathematical terms …'\n",
            "     • 'logicalize': last_score=0.000, desc='Apply formal logic to structure argument…'\n",
            "     • 'argument': last_score=0.000, desc='Construct or analyze a persuasive argume…'\n",
            "     • 'factualize': last_score=0.000, desc='Convert a statement into factual, eviden…'\n",
            "     • 'rationalize': last_score=0.000, desc='Explain the reasoning or logic behind a …'\n",
            "     • 'create': last_score=0.000, desc='Generate new content or ideas from scrat…'\n",
            "     • 'idealize': last_score=0.000, desc='Envision the perfect or optimal version …'\n",
            "     • 'abstractize': last_score=0.000, desc='Extract the underlying essence or abstra…'\n",
            "     • 'generalize': last_score=0.000, desc='Form broad principles or patterns from s…'\n",
            "     • 'specify': last_score=0.000, desc='Provide detailed, precise information or…'\n",
            "     • 'contextualize': last_score=0.000, desc='Place information or findings in an appr…'\n",
            "     • 'test': last_score=0.000, desc='Evaluate accuracy, performance, or valid…'\n",
            "     • 'track': last_score=0.000, desc='Follow the progression or change of elem…'\n",
            "     • 'monitor': last_score=0.000, desc='Continuously observe data, systems, or a…'\n",
            "     • 'translate': last_score=0.000, desc='Convert information between languages, f…'\n",
            "     • 'clarify': last_score=0.000, desc='Make complex, ambiguous, or vague inform…'\n",
            "     • 'restructure': last_score=0.000, desc='Reorganize the format, structure, or seq…'\n",
            " • Node_2.task_store:\n",
            "     • 'analyze': last_score=0.000, desc='Perform deep analysis of the content or …'\n",
            "     • 'retrieve': last_score=0.000, desc='Fetch information from memory or externa…'\n",
            "     • 'synthesize': last_score=0.000, desc='Combine multiple inputs into a coherent …'\n",
            "     • 'summarize': last_score=0.000, desc='Produce a concise summary of the key poi…'\n",
            "     • 'compute': last_score=0.000, desc='Perform numerical or logical calculation…'\n",
            "     • 'deduct': last_score=0.000, desc='Draw logical conclusions from given fact…'\n",
            "     • 'infer': last_score=0.000, desc='Make inferences based on context and evi…'\n",
            "     • 'plan': last_score=0.000, desc='Lay out a step-by-step strategy or roadm…'\n",
            "     • 'model': last_score=0.000, desc='Build a structured model or simulation.…'\n",
            "     • 'code': last_score=0.000, desc='Write or generate executable code.…'\n",
            "     • 'verify': last_score=0.000, desc='Check facts or validate data against tru…'\n",
            "     • 'substantiate': last_score=0.000, desc='Provide evidence or references to suppor…'\n",
            "     • 'ground': last_score=0.000, desc='Anchor abstract reasoning in concrete ex…'\n",
            "     • 'design': last_score=0.000, desc='Architect a system, workflow, or high-le…'\n",
            "     • 'debug': last_score=0.000, desc='Identify and fix errors in code or logic…'\n",
            "     • 'optimize': last_score=0.000, desc='Improve performance or efficiency of a s…'\n",
            "     • 'critique': last_score=0.000, desc='Evaluate an argument or design and sugge…'\n",
            "     • 'visualize': last_score=0.000, desc='Generate or describe a chart, diagram, o…'\n",
            "     • 'reverse-engineer': last_score=0.000, desc='Deconstruct a system or process to under…'\n",
            "     • 'forecast': last_score=0.000, desc='Predict future trends or outcomes based …'\n",
            "     • 'mathematicize': last_score=0.000, desc='Formalize an idea in mathematical terms …'\n",
            "     • 'logicalize': last_score=0.000, desc='Apply formal logic to structure argument…'\n",
            "     • 'argument': last_score=0.000, desc='Construct or analyze a persuasive argume…'\n",
            "     • 'factualize': last_score=0.000, desc='Convert a statement into factual, eviden…'\n",
            "     • 'rationalize': last_score=0.000, desc='Explain the reasoning or logic behind a …'\n",
            "     • 'create': last_score=0.000, desc='Generate new content or ideas from scrat…'\n",
            "     • 'idealize': last_score=0.000, desc='Envision the perfect or optimal version …'\n",
            "     • 'abstractize': last_score=0.000, desc='Extract the underlying essence or abstra…'\n",
            "     • 'generalize': last_score=0.000, desc='Form broad principles or patterns from s…'\n",
            "     • 'specify': last_score=0.000, desc='Provide detailed, precise information or…'\n",
            "     • 'contextualize': last_score=0.000, desc='Place information or findings in an appr…'\n",
            "     • 'test': last_score=0.000, desc='Evaluate accuracy, performance, or valid…'\n",
            "     • 'track': last_score=0.000, desc='Follow the progression or change of elem…'\n",
            "     • 'monitor': last_score=0.000, desc='Continuously observe data, systems, or a…'\n",
            "     • 'translate': last_score=0.000, desc='Convert information between languages, f…'\n",
            "     • 'clarify': last_score=0.000, desc='Make complex, ambiguous, or vague inform…'\n",
            "     • 'restructure': last_score=0.000, desc='Reorganize the format, structure, or seq…'\n",
            "\n",
            "==== Interval 1 ====\n",
            "🔒 Node_0 retains roles/tasks: ['Mathematician'] → ['compute', 'mathematicize']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'StrategicAgent' object has no attribute 'act'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0f012d54ffa7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 🚀 MAIN EXECUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# ─────────────────────────────────────────────────────────────\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpromoted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmission_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n🚦 Promoted this round?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpromoted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-a4fa6ef5374d>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, mission_prompt, verbose)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# A-5) ACT → get output, raw physio (x = k-1 start)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 output, score, attention, fatigue, hunger, *_ = agent.act(\n\u001b[0m\u001b[1;32m    288\u001b[0m                     \u001b[0mmission_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'StrategicAgent' object has no attribute 'act'"
          ]
        }
      ]
    }
  ]
}